<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <!-- originally created with BlueGriffon, starting in December 2019 -->
    <!-- switch to SeaMonkey May 2021, BlueGriffon incompatible with Big Sur -->
    <title>TexSyn</title>
    <style>
      body {
             background-color: gray;
             font-family: Arial, Helvetica, sans-serif;
           }
      h1 { padding: 0; border: 0; margin: 0; margin-top: 0.2em; }
      h2 { padding: 0; border: 0; margin: 0; margin-top: 0.2em; }
      p { color: black; }
      code { font-size: 120%; }
      pre {
            color: white;
	          border: 1px dashed rgb(65%,65%,65%);
	          padding: 10px;
           	margin-bottom: 1em;
          }
      .wrapping_code {
                       color: white;
                       border: 1px dashed rgb(65%,65%,65%);
                       padding: 10px;
                       margin-bottom: 1em;
                       font-family: Courier, "Courier New", monospace;
                       font-weight: normal;
                       font-size: 83%;
                     }
      .comment { color: rgb(80%,80%,80%); }
      <!-- for in-page date anchors -->
      a.date         {color: white; }
			a.date:link    {text-decoration: none; color: white;}
			a.date:visited {text-decoration: none; color: white;}
			a.date:hover   {text-decoration: underline; color: white;}
			a.date:active  {text-decoration: none; color: white;}
      .post {
              border-top: 0.3em solid rgb(25%,25%,25%);
              margin-top: 1em;
              padding-left: 2em;
              padding-right: 2em;
              padding-top: 1em;
              clear: left ;
            }
      .designnote { color: rgb(25%,25%,25%) }
      .novak_pad { margin-right: 105px; margin-left: 105px; }
      .smaller { font-size: 75%; }
      .img_flow { padding-right: 1em; padding-bottom: 1em; }
    table, th, td {
      border: 1px solid rgb(25%,25%,25%);

    }
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="yyyymmdd">
      <a href="#yyyymmdd" class="date">Month 0, 0000</a>
      <h1>Title</h1>
      <p>...</p>
      <pre>x</pre>
      <img src="images/xxx.png" alt="xxx" title="xxx" height="512" width="512" class="img_flow">
    </div>
    <!-- --------------------------------------------------------------------------- -->
    </style>
  </head>
  <body id="top">
    <p class="smaller"> <a href="https://cwreynolds.github.io/TexSyn/">This








        page on GitHub</a> </p>
    <h1>TexSyn</h1>
    <p style="font-size:125%"><br>
      <b>TexSyn</b> is a library for procedural texture synthesis.
      Textures are defined by code fragments, compositions of TexSyn
      functions. Generally, TexSyn programs will be automatically
      generated and optimized using a “genetic programming” system. As a
      result, TexSyn contains some quirky design choices, reflecting
      that it is not primarily intended for direct use by human
      programmers. TexSyn was built as a component of a larger project
      aiming to simulate the evolution of camouflage in nature.<br>
    </p>
    <ul>
      <li>This document is a blog (lab notebook? design diary?) about
        making and using TexSyn. It describes in “blog order” the steps
        and design decisions made in TexSyn’s construction, then later
        shifts focus to evolutionary texture optimization, then
        continues into camouflage simulation. Warning: this is just one
        huge web page. There is some background and introductory
        information about TexSyn, at the bottom of this page, in the
        entry for <a href="#20191215">December 15, 2019</a>.<br>
      </li>
      <li>While TexSyn is written in c++, many of the code samples below
        are shown in a simplified style: textures are often written as
        an isolated expression rather than a complete c++ statement,
        semicolons are often omitted, as are <code>auto</code>
        declarations when defining variables.<br>
      </li>
      <li><b>Note</b>: as of <a href="#20200506">May 6, 2020</a> an
        incompatible change was made to improve gamma handling. For
        entries dated before then, the given TexSyn code, if re-rendered
        anew, will produce a texture that looks different from the one
        shown in the doc. It will generally be brighter and less
        saturated.<br>
      </li>
      <li>Related documents:<br>
      </li>
      <ul>
        <li>TexSyn was intended for use with <a
            href="https://cwreynolds.github.io/LazyPredator/">LazyPredator</a>,
          an implementation of “genetic programming”, a type of
          evolutionary computation. LazyPredator maintains a population
          of “program fragments” and optimizes them according to a given
          fitness metric. In this work, those programs are in the domain
          specific language defined by TexSyn.<br>
        </li>
        <li>Some of the posts below describe “interactive evolution of
          camouflage” runs using a (very alpha) app called <a
            href="https://cwreynolds.github.io/TexSyn/evo_camo_game_doc.html">evo_camo_game</a>
          based on TexSyn and LazyPredator.</li>
      </ul>
    </ul>
    <br style="-------------------------------------------------------">
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20220926"> <a href="#20220926" class="date">September
        26, 2022</a>
      <h1>More code cleanup, another run<br>
      </h1>
      <p>I've been moving informal clumps of prototype code, originally
        from a Jupyter notebook on Colab, into more formal Python
        modules in <code>.py</code> files. This was my first project in
        Python, so I did just about everything wrong at first. I have
        been slowly working to “make code less wrong.” (I once worked
        with <a moz-do-not-send="true"
          href="https://www.linkedin.com/in/andrewstern/">Andrew Stern</a>
        at UCSC. He was especially amused when I made a <code>git
          commit</code> with that message.) After each big refactor I,
        do another test run. The one below (<font size="-1">ID
          tree_leaf_blossom_sky_20220925_1228</font>) uses photos of
        small trees in a parking lot with sky in the background. As a
        reminder, this model is <b>purely 2d</b> so these background
        are just a flat 2d color texture, that just happen to look like
        trees and leaves and blossoms. In this model they are as just
        flat disks on flat backgrounds. <br>
      </p>
      <p>It seems like the model struggles to handle all the constraints
        imposed by these backgrounds, such as the lumpy distributions of
        colors and spatial frequencies, while simultaneously being
        sufficiently “disruptive” at the border of each prey disk, so as
        to hide their edge.<br>
      </p>
      <img src="images/20220926_step_1444.png" alt="20220926_step_1444"
        title="20220926_step_1444" class="img_flow" height="512"
        width="512"> <img src="images/20220926_step_4864.png"
        alt="20220926_step_4864" title="20220926_step_4864"
        class="img_flow" height="512" width="512"> <img
        src="images/20220926_step_5567.png" alt="20220926_step_5567"
        title="20220926_step_5567" class="img_flow" height="512"
        width="512"> <img src="images/20220926_step_6077.png"
        alt="20220926_step_6077" title="20220926_step_6077"
        class="img_flow" height="512" width="512"> <img
        src="images/20220926_step_6130.png" alt="20220926_step_6130"
        title="20220926_step_6130" class="img_flow" height="512"
        width="512"> <img src="images/20220926_step_6143.png"
        alt="20220926_step_6143" title="20220926_step_6143"
        class="img_flow" height="512" width="512"> </div>
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20220923"> <a href="#20220923" class="date">September


        23, 2022</a>
      <h1>Adjusting predator population model<br>
      </h1>
      <p>Using the latest simulation architecture I tried a run with
        another background: smooth colored pebbles pressed into concrete
        tiles, bordering a neighbor's yard. The results were not as
        vivid as in the <a moz-do-not-send="true" href="#20220918">previous
          run</a>, so I tried some adjustments. I made three such runs
        making adjustments between. The results below are from the third
        run (<font size="-1">ID pebbles_in_concrete_20220922_1213</font>).
        For historical context, see an interactive <a
          moz-do-not-send="true"
          href="file:///Users/cwr/Documents/code/TexSyn/docs/index.html#20210628">June
          28, 2021</a> run on this same background. </p>
      <p>I changed how the fine-tuning dataset was curated. Originally
        it was a strict history of the previous <i>n</i> (=500)
        simulation steps as training examples. Now new data is inserted
        at a random index. This means that sometimes the overwritten
        entry may be relatively recent allowing an older entry to
        persist. So the sampling of history is stochastic and “smeared”
        further back in time. This provides more memory without adding
        fine-tuning cost.<br>
      </p>
      <p>One of the metrics I watch has to do with performance of the <code>Predator</code>'s



        deep learning model during fine-tuning. It is a custom metric
        called <code>in_disk()</code> I pass into the Keras/TF <code>model.fit()</code>
        training calls. It is similar to the standard widely used <code>accuracy()</code>
        metric but takes into account the finite size of prey disks: it
        is the fraction of training examples when the model predicts a
        position <b>inside</b> a prey disk. When I used a single <code>Predator</code>
        this value got well up into the range from 80% to 90%. With a
        population of predators — and “death by starvation” — typical
        values rarely reached 70%. My theory was that too much random
        noise was being injected into the model by creating “offspring”
        predators, after too frequent starvation. I reduced the <code>Predator.success_history_ratio</code>
        from 0.33 to 0.2. Now starvations happen on about 1% of
        simulation steps. (Down from the previous two runs which had
        rates of 6% and 9%.) Indeed this allowed the in_disk metric to
        peak up to 78% around simulation step 3000. It seemed to have
        fallen off (65%) by step 6000.<br>
      </p>
      <p>I see some disappointing results from the predators. In the
        first image below, the yellowish prey in the upper left looks to
        me like the most conspicuous, but none of the three predators
        chose it. The one at the bottom center looks especially good to
        me. Similarly in the third image, the prey in the lower left
        seems quite effective camouflage, but all three predators
        attacked it as most conspicuous. Note also that the last two
        images show “predator fails” where all three predators miss all
        three prey.<br>
      </p>
      <img src="images/20220923_step_2632.png" alt="20220923_step_2632"
        title="20220923_step_2632" class="img_flow" height="512"
        width="512"> <img src="images/20220923_step_3147.png"
        alt="20220923_step_3147" title="20220923_step_3147"
        class="img_flow" height="512" width="512"> <img
        src="images/20220923_step_4446.png" alt="20220923_step_4446"
        title="20220923_step_4446" class="img_flow" height="512"
        width="512"> <img src="images/20220923_step_5377.png"
        alt="20220923_step_5377" title="20220923_step_5377"
        class="img_flow" height="512" width="512"> <img
        src="images/20220923_step_5966.png" alt="20220923_step_5966"
        title="20220923_step_5966" class="img_flow" height="512"
        width="512"> </div>
    <div class="post" id="20220918"> <a href="#20220918" class="date">September





        18, 2022</a>
      <h1>Not a fluke<br>
      </h1>
      <p>I tried a new run, using the same parameters as in the <a
          moz-do-not-send="true" href="#20220916">September 16</a> post,
        but with a different set of background photos. I had been
        holding that background constant for some time while tweaking
        code and various parameters. The upshot is that the previous run
        was not just a lucky fluke, but rather the simulation model
        seems to be operating well and producing effective camouflage
        for a given set of background images. The photos used in this
        run are of a steeply sloped embankment in front of a neighbor's
        house, taken last December, showing fallen leaves from a
        Japanese maple, plus moss, sprouts, and bare soil.<br>
      </p>
      <p>In the first image below, very early at step 187, the prey are
        moving toward a rough match to the background: several colors
        with hard edges, although the black background is darker than
        the bare soil in the photos. By the second image, at step 2360,
        a background close to the deep moss green has been found.
        Thereafter these elements remix and refine to produce high
        quality camouflage. Again I let this run (<font size="-1">ID
          maple_leaf_litter_20220917_1121</font>) go overnight. It seems
        to have reached a “evolutionary stable state” — making only
        small improvements on an otherwise consistent phenotype — by the
        last image at step 7600.<br>
      </p>
      <img src="images/20220918_step_187.png" alt="20220918_step_187"
        title="20220918_step_187" class="img_flow" height="512"
        width="512"> <img src="images/20220918_step_2360.png"
        alt="20220918_step_2360" title="20220918_step_2360"
        class="img_flow" height="512" width="512"> <img
        src="images/20220918_step_3743.png" alt="20220918_step_3743"
        title="20220918_step_3743" class="img_flow" height="512"
        width="512"> <img src="images/20220918_step_3971.png"
        alt="20220918_step_3971" title="20220918_step_3971"
        class="img_flow" height="512" width="512"> <img
        src="images/20220918_step_5947.png" alt="20220918_step_5947"
        title="20220918_step_5947" class="img_flow" height="512"
        width="512"> <img src="images/20220918_step_6745.png"
        alt="20220918_step_6745" title="20220918_step_6745"
        class="img_flow" height="512" width="512"> <img
        src="images/20220918_step_7372.png" alt="20220918_step_7372"
        title="20220918_step_7372" class="img_flow" height="512"
        width="512"> <img src="images/20220918_step_7562.png"
        alt="20220918_step_7562" title="20220918_step_7562"
        class="img_flow" height="512" width="512"> <img
        src="images/20220918_step_7600.png" alt="20220918_step_7600"
        title="20220918_step_7600" class="img_flow" height="512"
        width="512"> </div>
    <div class="post" id="20220916"> <a href="#20220916" class="date">September





        16, 2022</a>
      <h1>It seems to work — for some values of “it” and “work”</h1>
      <p>I have been building toward a version of this camouflage
        simulation using a population of predators versus population of
        prey. Things fell into place a few days ago and I tried a test
        run. It <i>really</i> did not perform well. I decided that too
        much noise was being injected into the system by the new
        “predator starvation” aspect of the model. So I dialed back the
        likelihood of predator death, leading to less frequent
        replacement by naïve offspring, and better quality predation
        overall. The next test run (<font size="-1">ID
          tiger_eye_beans_20220915_1010</font>) did pretty well. <br>
      </p>
      <p>This first image (simulation step 1577) shows when prey
        evolution has begun to discover patterns with about the right
        complexity and frequency to match up with the background. More
        conspicuous colors, like the blue on on the right, are
        attracting the predators. This allows the more cryptic colored
        prey to survive:<br>
      </p>
      <img src="images/20220916_step_1577.png" alt="20220916_step_1577"
        title="20220916_step_1577" class="img_flow" height="512"
        width="512">
      <p>These six images, from step 2458 to 3338 exhibit apparently
        high quality camouflage. A metric I watch for in these
        experiments is the occurrence of steps(/tournaments/each of the
        images below) where all three of the prey seem well camouflaged.
        It is common for a given simulation step to have one or two well
        camouflaged prey, but usually there is one that is poorly
        camouflaged: conspicuous. (For example the blue one just above.)
        In each of the six images below, all three prey seem to have
        good camouflage quality. Quite a few of these were generated
        during run <font size="-1">tiger_eye_beans_20220915_1010</font>.
        This suggests to me that the simulation is “well tuned” and
        running as hoped.<br>
      </p>
      <img src="images/20220916_step_2458.png" alt="20220916_step_2458"
        title="20220916_step_2458" class="img_flow" height="512"
        width="512"> <img src="images/20220916_step_2475.png"
        alt="20220916_step_2475" title="20220916_step_2475"
        class="img_flow" height="512" width="512"> <img
        src="images/20220916_step_2717.png" alt="20220916_step_2717"
        title="20220916_step_2717" class="img_flow" height="512"
        width="512"> <img src="images/20220916_step_2835.png"
        alt="20220916_step_2835" title="20220916_step_2835"
        class="img_flow" height="512" width="512"> <img
        src="images/20220916_step_3230.png" alt="20220916_step_3230"
        title="20220916_step_3230" class="img_flow" height="512"
        width="512"> <img src="images/20220916_step_3338.png"
        alt="20220916_step_3338" title="20220916_step_3338"
        class="img_flow" height="512" width="512">
      <p>That period of effective simulation described above persisted
        from roughly step 2000 to step 3500. I had been using 2000 steps
        as a standard simulation length. With a population of predators
        (currently 20) additional training steps will likely be
        required. It may be that 3000 steps is a good simulation length
        for the near future.<br>
      </p>
      <p>Around step 3500 I began to see a new type of pattern appear on
        prey. It was simple and geometric, one or two layers of a square
        wave texture (from TexSyn's <i>Grating</i> operator). To my
        eye, these were clearly more conspicuous than the organic,
        disruptive examples shown above. But the predators seemed blind
        to these grid patterns and preferentially went after the older
        more organic patterns. I let the simulation continue to run
        overnight. The images below are from around step 6700.<br>
      </p>
      <img src="images/20220916_step_6764.png" alt="20220916_step_6764"
        title="20220916_step_6764" class="img_flow" height="512"
        width="512"> <img src="images/20220916_step_6783.png"
        alt="20220916_step_6783" title="20220916_step_6783"
        class="img_flow" height="512" width="512">
      <p><b>Update</b> to the September 12 post: I changed the criteria
        for “predator starvation” from two sequential predator fails, to
        ⅔ of a predator's last 10 tournaments being fails, to ⅔ of a
        predator's last 20 tournaments being fails. This leads to
        somewhere between 5% and 7% of simulation steps resulting in
        “predator starvation” and replacement in the population with a
        new offspring. </p>
    </div>
    <div class="post" id="20220912"> <a href="#20220912" class="date">September








        12, 2022</a>
      <h1>Inching closer to infrastructure for population of predators<br>
      </h1>
      <p>It has been a long series of “oh, one more thing” steps as I
        close in on the ability to co-evolve a predator population
        against the prey population. I am now running with a small
        population of 12 predators.<br>
      </p>
      <p>I randomly select tournaments of size 3. (Which correspond to
        the three predictions shown as crosshair annotation in the
        images below.) I added a new <code>Tournament</code> class to
        encapsulate these. This provides a home for the bloat that had
        been accumulating in the <code>write_response_file()</code>
        function of the “predator server”. <br>
      </p>
      <p>I gave each <code>Predator</code> a history of recent
        tournaments it participated in to keep track of its successes,
        sort of a win/loss record. This is the same concept (with the
        opposite sense) of “predator fails” mentioned previously. If a
        predator predicts a position which is <b>not</b> inside any
        prey disks, then it has failed to detect/hunt prey and “goes
        hungry.” This is what will drive “death by starvation” for a <code>Predator</code>
        who is unable to catch enough prey. My plan is to remove that
        Predator form the population and replace it with a new one. The
        current starvation criteria is two tournaments/hunts in a row
        without success. This criteria will probably be adjusted later
        on. However after yesterday's batch of changes, while everything
        else seems to be working as usual, the “starvation” counts are
        way too high. I suspect I broke something. Currently that
        success history is being logged and does not otherwise affect
        the simulation.<br>
      </p>
      <img src="images/20220912_step_5890.png" alt="20220912_step_5890"
        title="20220912_step_5890" class="img_flow" height="512"
        width="512"> <img src="images/20220912_step_6087.png"
        alt="20220912_step_6087" title="20220912_step_6087"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220908"> <a href="#20220908" class="date">September














        8, 2022</a>
      <h1>Properly jiggling Keras/TF models</h1>
      <p>My vague plan for using a population of predators assumed they
        would start out similar but with slight variations. I thought I
        would create a small number (10-20?) of <code>Predator</code>
        instances, initializing each of their Keras/TF models to my
        standard pre-trained model (<font size="-1">20220321_1711_FCD6_rc4</font>),














        and then “jiggle” them a bit.</p>
      <p>I recently started displaying the predictions of three <code>Predators</code>
        (a prototype tournament). It became obvious that despite the
        initial randomization, all three <code>Predators</code>
        produced the same initial prediction. That is, the three
        crosshairs would be drawn exactly on top of one another. (Which
        gets repeated for the first 50 simulation steps for “<a
          href="#20220530">reasons</a>”.) I tried debugging this, and
        accomplished some useful cleanup/refactoring, but the three
        randomized models stubbornly continued to produce exactly the
        same predictions. The randomization is performed using utilities
        from TensorFlow to add signed noise to all parameters of a deep
        neural net model. It closely follows this <a
href="https://stackoverflow.com/questions/64542231/efficiently-add-noise-to-all-trainable-weights-in-a-model/64542651#64542651">stackoverflow
















          answer</a>.<br>
      </p>
      <p>It turned out to be a quirk/feature of TensorFlow: eager mode
        and <a moz-do-not-send="true"
          href="https://www.tensorflow.org/guide/function#tracing">tracing</a><code></code>.
        Basically each time I called <code>tf.random.uniform()</code> I
        got the <b>same</b><b> set of </b><b>random weights</b>. That
        is explained in <a moz-do-not-send="true"
          href="https://stackoverflow.com/a/73162028/1991373">another
          stackoverflow answer</a>. (I am a heavy and unapologetic user
        of google and stackoverflow as coding resources.) The fix was to
        simply add a “<code>@tf.function</code>” decoration to my
        function for jiggling the weights of a model. Although now, TF
        warns me that I am doing too much re-tracing, which is
        computationally expensive. Since it happens only occasionally, I
        am not worried about the performance hit, but would like to
        eventually silence the warning.<br>
      </p>
      <p>Two images from near the end of a test run with three <code>Predators</code>
        now correctly randomized from the very beginning of the run:<br>
      </p>
      <img src="images/20220908_step_3724.png" alt="20220908_step_3724"
        title="20220908_step_3724" class="img_flow" height="512"
        width="512"> <img src="images/20220908_step_4104.png"
        alt="20220908_step_4104" title="20220908_step_4104"
        class="img_flow" height="512" width="512">
      <p>(Even without my intended initial randomization, I noticed
        multiple <b>identical</b> Predators would diverge once I
        started fine-tuning their Keras/TF models. They started out the
        same, then were all fine-tuned using exactly the same training
        set of images accumulated during a run. I am not sure where that
        divergence comes from, possibly the non-determinism of training
        Keras/TF models using hardware parallelism? This is an
        interesting question which I plan to ignore for now.)</p>
    </div>
    <div class="post" id="20220904"> <a href="#20220904" class="date">September

























        4, 2022</a>
      <h1>Predator response: now “best of three”<br>
      </h1>
      <p>After what felt like a very long dry spell of working on boring
        infrastructure issues — I have finally returned to work on the
        camouflage evolution model itself. I had previously abstracted
        the visual hunting CNN model into a Python class called <code>Predator</code>.
        That allowed me to instantiate several of them, and fine tune
        them in parallel. I used this to mock up a three-way <b>tournament


























          of predators</b>. I now rank the three predators on <i>accuracy</i>.
        This is based on the semi-self-supervised nature of this model:
        having generated each image procedurally, I know exactly where
        each prey texture is located. Each predator predicts an xy
        position where it thinks the most conspicuous prey is located.
        So I determine which of the three prey is nearest that location,
        then measure the <i>aim error</i>. That is: I assume the
        predator was “aiming” at that nearest prey, then measure the
        distance — the “aim error” — from the predator's estimate to the
        prey's centerpoint. I treat this aim error as an accuracy
        metric, with zero being best. I measure the accuracy of each
        predator and sort them, so the first listed predator is the
        “most accurate.” That “best” predator is used to drive
        (negative) selection in the evolution of prey camouflage.<br>
      </p>
      <p>Shown below are four images from late in an extended test run (<font
          size="-1">ID tiger_eye_beans_20220903_1401</font>). In these I
        draw crosshair annotation of the “prediction”/“estimate”
        produced by each of three <code>Predators</code>. The one in
        black-and-white is the one judged most accurate (least aim
        error), green is second best, and red-ish is third. Note that in
        three of the four images, all predators agree on (all crosshairs
        are within) a single prey. While in image number three, the
        “white predator” chose one prey, the “green predator” chose
        another, and the “red predator” was between them, missing all
        prey: a “predator fail.” In the last image (number four) the
        leftmost prey seems quite good.<br>
      </p>
      <img src="images/20220904_step_4750.png" alt="20220904_step_4750"
        title="20220904_step_4750" class="img_flow" height="512"
        width="512"> <img src="images/20220904_step_4427.png"
        alt="20220904_step_4427" title="20220904_step_4427"
        class="img_flow" height="512" width="512"> <img
        src="images/20220904_step_3800.png" alt="20220904_step_3800"
        title="20220904_step_3800" class="img_flow" height="512"
        width="512"> <img src="images/20220904_step_3705.png"
        alt="20220904_step_3705" title="20220904_step_3705"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220831"> <a href="#20220831" class="date">August





























        31, 2022</a>
      <h1>Working toward population of predators</h1>
      <p>While there is a lot wrong with the images below, I just wanted
        to note that I have been running tests with <b>two</b> active
        predators in parallel. This is in preparation for maintaining a
        population of predators. Both start as a copy of the pre-trained
        “find conspicuous disk” model (<font size="-1">20220321_1711_FCD6_rc4</font>).




























        Then each is fine-tuned on the collected results of the current
        camouflage run. Then each one generates a “prediction” — an
        estimate of where in each input image the most conspicuous prey
        is probably located.<br>
      </p>
      <p>Previous logging convinced me that they started out identical,
        and that they then diverged during fine-tuning. But it was hard
        to interpret this. Sometimes the distance between the two
        estimates was tiny, sometimes it indicated locations on opposite
        sides of the image. So I made the PredatorEye side send the
        estimates back to the TexSyn side for visualization. In the
        first image below, both predators have estimated positions
        inside one prey (the flat grayish one in the lower right). In
        the second image, one predator chose the orange striped prey on
        the left, while the other predator chose the flat black prey on
        the right. This is the learning-based predator equivalent of the
        idea that “reasonable people may have differing opinions.” Both
        predators has “good aim” — selecting a position well inside one
        of the three prey — they just had differing ideas about which is
        most conspicuous.<br>
      </p>
      <p>Note that by the time of these images, this run had badly
        converged, losing most of the prey population's diversity. Which
        is why all the prey patterns are just stripes (gratings). Also
        note that the green crosshair in the center is not yet used.<br>
      </p>
      <img src="images/20220831_step_5265.png" alt="20220831_step_5265"
        title="20220831_step_5265" class="img_flow" height="512"
        width="512"> <img src="images/20220831_step_5356.png"
        alt="20220831_step_5356" title="20220831_step_5356"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220824"> <a href="#20220824" class="date">August































        24, 2022</a>
      <h1>Goodbye to Rube Goldberg: running locally on M1, sans GPU<br>
      </h1>
      <p>I was working toward running my simulation on a remote Win10 at
        SFU graciously lent by Steve DiPaola. I wanted to test that my
        pre-trained “find conspicuous disk” model (<font size="-1">20220321_1711_FCD6_rc4</font>)
        would read into TensorFlow running on Windows. This had failed
        the first time I tried it on macOS on Apple Silicon, so I wanted
        to do a reality check by reproducing that error. <b>It
          stubbornly failed to fail.</b> I then tried using that
        pre-trained model with a handwritten test calling <code>model.predict()</code>
        and that worked too. Then I rebuilt TexSyn and PredatorEye to
        run in “local mode” which also worked! (“Local mode” is “non
        Rube Goldberg mode” with both predator and prey running on the
        same machine.) <br>
      </p>
      <p>My only theory about why it failed in the past, yet worked now,
        might involve the optional Rosetta facility in macOS. It “<a
          moz-do-not-send="true"
          href="https://support.apple.com/en-us/HT211861">enables a Mac
          with Apple Silicon to use apps built for a Mac with an Intel
          processor.</a>” The original error seemed to be complaining
        about x86_64 code embedded in the TF/Keras model. Perhaps that
        was before I installed Rosetta on my M1 laptop. Perhaps
        installing it allowed TensorFlow to run any pre-compiled x86
        code in my saved model.<br>
      </p>
      <p>Some bad news with the good. As described on <a
          moz-do-not-send="true" href="#20220717">July 17</a>, I ran
        into a <a moz-do-not-send="true"
          href="https://developer.apple.com/forums/thread/706920">known
          bug</a> in the TensorFlow-Metal plugin, essentially a memory
        leak (of <code> IOGPUResource</code>). I assume that is why my
        first attempt at a local-mode run, hung after about 90 minutes.
        So I wrapped: <code>with tf.device('/cpu:0')</code> around the
        <code>model.fit()</code> call for fine-tuning, and it ran fine.
        <b>Running locally is about 4x faster than “</b><b>Rube Goldberg
          mode</b><b>”</b> since even CPU-only TensorFlow is faster than
        the communication delay imposed by Rube Goldberg mode. The
        CPU-only mode seems to be very stable. Unlike time-boxed Colab,
        locally I could let it just keep running. In about 25 hours of
        computation, I ran for 8000 simulation steps. In Rube Goldberg
        mode, 2000 steps was about all I could get in a 24 hour Colab
        session. Running CPU-only, macOS's Activity Monitor app shows
        the Python process peaking up between 600% to 700% of a CPU.
        TexSyn averaged about 20% of a CPU. Some OK-but-not-great images
        from near the end of the 8000 step run are shown below. </p>
      <p>As I suggested in my <a moz-do-not-send="true"
          href="#20220604">June 4</a> talk, I next want to look at using
        a small <b>population of predators</b>. I want them to jointly
        co-evolve against the population of prey. I think there to needs
        to be more dire consequences when predators fail. They should
        survive only if they are successful, forcing them to compete
        with each other on “hunting” ability.<br>
      </p>
      <img src="images/20220824_step_7866.png" alt="20220824_step_7866"
        title="20220824_step_7866" class="img_flow" height="512"
        width="512"> <img src="images/20220824_step_7771.png"
        alt="20220824_step_7771" title="20220824_step_7771"
        class="img_flow" height="512" width="512"> <img
        src="images/20220824_step_7695.png" alt="20220824_step_7695"
        title="20220824_step_7695" class="img_flow" height="512"
        width="512"> </div>
    <div class="post" id="20220819"> <a href="#20220819" class="date">August































        19, 2022</a>
      <h1>CMake build, access to new hardware.<br>
      </h1>
      <p>Today I got a CMake build of TexSyn working on my laptop. I
        have not tested it cross-platform but hope to get to that soon.
        In the past, I have used CMake to build other projects, but not
        previously written a CMake build script from the ground up for
        my own code. I had been stuck for a while getting my build to
        correctly link to OpenCV. Today I discovered CMake's <code>find_package(OpenCV































          REQUIRED)</code> which somehow does all the magic. I had
        planned to eventually make TexSyn buildable with CMake, since it
        previously built only on macOS using Xcode. CMake offers the
        possibility of cross-platform builds. <br>
      </p>
      <p>This has become more urgent since, as described on <a
          moz-do-not-send="true" href="#20220802">August 2</a>, I am not
        able to run the predator model either locally on my laptop
        (because of a bug in the TensorFlow-Metal layer) or on Colab
        (because of a disconnect bug there). My old friend and former
        coworker <a moz-do-not-send="true"
          href="https://www.dipaola.org/">Steve DiPaola</a> generously
        offered remote access to a Windows 10 machine with an NVIDIA GPU
        in the <a moz-do-not-send="true" href="https://ivizlab.org">iVizLab</a>
        he directs at <a moz-do-not-send="true"
          href="https://www.sfu.ca">Simon Fraser University</a> in
        Vancouver.<br>
      </p>
      <p>So since I returned from SIGGRAPH 2022 a week ago, I have been
        working in parallel on provisioning that machine with tools and
        libraries I need, and working on the CMake build, which I will
        need to build TexSyn on the Win10 machine. This has been moving
        slow since I am a complete newbie on Windows generally.<br>
      </p>
    </div>
    <div class="post" id="20220802"> <a href="#20220802" class="date">
        August 2, 2022</a>
      <h1>Colab troubles<br>
      </h1>
      <p>On July 21 I said “Until [the TensorFlow-Metal plugin issue] is
        fixed I will need to return to go back to my inconvenient ‘Rube
        Goldberg’ contraption using Google Colab.” Oh — if <i>only</i>
        it had been that simple. For reasons I do not understand, the
        Colab side of “Rube Goldberg” which had been working well for
        months is now getting <i>very</i> frequent momentary
        disconnects, and long before completing a camouflage run it
        disconnects permanently. I reported the temporary disconnects
        here: <a
          href="https://github.com/googlecolab/colabtools/issues/2965">
          Frequent disconnects while busy running in Google Colab #2965</a>.
        I am now waiting for response to that, while looking at a third
        alternative, now that I have problems running both locally on my
        M1 laptop, <b>and</b> in “Rube Goldberg” mode using Colab in
        the cloud.<br>
      </p>
      <p><b>Update on September 5</b>: as mentioned on <a
          href="#20220824">August 24</a>, I moved to running this
        project locally on my laptop, an M1/“Apple Silicon” MacBook Pro.
        It is running CPU-only because of the <b>tensorflow-metal</b>
        bug mentioned on <a href="#20220717">July 17</a>. However even
        without GPU acceleration, the overall speed is faster because of
        the significant communication delay caused by “Rube Goldberg
        mode.” So yesterday I went back to <a
          href="https://github.com/googlecolab/colabtools/issues/2965">Google
























          Colab Issue #2965</a> on Github to post a “so long and thanks
        for all the fish” message. I decided to test one more time, and
        managed to diagnose the problem, or at least narrow it down. It
        works fine using Safari browser, but the “frequent disconnect”
        problem remains using Google Chrome browser under macOS Monterey
        on Apple Silicon as described in <a
href="https://github.com/googlecolab/colabtools/issues/2965#issuecomment-1237570020">this
























          comment on Issue #2965</a>.<br>
      </p>
    </div>
    <div class="post" id="20220725"> <a href="#20220725" class="date">July
























        25, 2022</a>
      <h1>TexSyn now “officially” version 2<br>
      </h1>
      <p>Nothing changed, but I realized it had been reporting the wrong
        version number (0.9.7) for a <i>very</i> long time. The slides
        for my GPTP talk referred to the 2008-2011 implementation as
        version 1, and the current one as version 2. I made this
        official today by changing <code>texsyn_version_string</code>.<br>
      </p>
      <p>Indeed basic TexSyn has not changed for quite a long time. On
        June 3, I added an optional render timeout, convenient for long
        evolutionary optimization runs. Around <a
          moz-do-not-send="true" href="#20220623">June 23</a> I made
        some changes to multi-threaded render for my M1 (Apple Silicon)
        laptop.<br>
      </p>
      <p>There have been lots of changes in the <code>EvoCamoGame</code>
        classes. They are essentially applications build on top of
        TexSyn, but currently live inside it. They should eventually be
        moved out into a separate project/repository.<br>
      </p>
    </div>
    <div class="post" id="20220717"> <a href="#20220717" class="date">July
























        17, 2022</a>
      <h1>TensorFlow on GPU on M1<br>
      </h1>
      <p>Today I got GPU accelerated deep learning running on my M1
        (Apple Silicon) laptop. My predator model is built on <a
          moz-do-not-send="true" href="https://keras.io">Keras</a> on <a
          moz-do-not-send="true" href="https://www.tensorflow.org">TensorFlow</a>.
        The <a moz-do-not-send="true"
href="https://blog.tensorflow.org/2021/06/pluggabledevice-device-plugins-for-TensorFlow.html">PluggableDevice</a>
        abstraction allows TensorFlow to run on arbitrary devices.
        Apple's <a moz-do-not-send="true"
          href="https://developer.apple.com/metal/">Metal</a> provides
        an abstraction of their compute devices, such as the GPU in my
        M1 laptop. A layer called <b>tensorflow-metal</b> provides the
        interface between these worlds.<br>
      </p>
      <p>I started by following these instructions <a
href="https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706">
          How To Install TensorFlow on M1 Mac (The Easy Way)</a> but
        something went awry. I switched to Apple's <a
          href="https://developer.apple.com/metal/tensorflow-plugin/">
          Getting Started with tensorflow-metal PluggableDevice</a> and
        tried again. That seemed to work and I was able to run the MNIST
        benchmark giving in the first doc. The execution time indicated
        that GPU acceleration was in use.<br>
      </p>
      <p><b>Update on July 18, 2022</b>: I modified my Jupyter notebook
        which builds the pre-trained FCD6 generalist predator. I
        expected it to run about 17 hours. After 16 minutes and 45
        seconds it hung. I tried it again and got the same result.
        Exactly the same as near as I could tell. It was on epoch 2/100
        and batch (sub-batch?) 2493/4000 of the training run. I rebooted
        the laptop and tried again. Same result. For the moment I am
        stuck and trying to decide how to proceed.<br>
      </p>
      <p><b>Update on July 21, 2022</b>: damn! I set aside all my
        Anaconda virtual environments, carefully followed the
        instructions from <a
          href="https://developer.apple.com/metal/tensorflow-plugin/">Getting











































          Started with tensorflow-metal PluggableDevice</a> and got
        exactly the same hang at exactly the same place (2/100,
        2493/4000). <strike>I guess I can try deactivating the “call
          back” code I use: my </strike><strike><code>in_disk()</code></strike><strike>
          metric and </strike><strike><code>Find3DisksGenerator</code></strike><strike>
          for dataset augmentation.</strike> No, wait! I finally read
        the last bit on the <a
          href="https://developer.apple.com/metal/tensorflow-plugin/">
          Getting Started...</a> doc which says “To ask questions and
        share feedback about the tensorflow-metal plugin, visit the <a
href="https://developer.apple.com/forums/tags/tensorflow-metal"> Apple
          Developer Forum</a>.” Aha! There was a <a
          href="https://developer.apple.com/forums/thread/706920">
          month-old report</a> of a very similar symptom, even
        mentioning “16 minutes.” That seems to correspond to “<code>leaking












































          IOGPUResource</code>.” So this seems to be a known bug. Until
        it is fixed I will need to return to go back to my inconvenient
        “Rube Goldberg” contraption using Google Colab.<br>
      </p>
    </div>
    <div class="post" id="20220714"> <a href="#20220714" class="date">
        July 14, 2022</a>
      <h1>Apple responds?!<br>
      </h1>
      <p>On July 2, I mentioned what I thought might be an Xcode bug,
        related to code signing. I did the Right Thing and reported via
        Apple's Feedback Assistant. And knock me over with a feather,
        they wrote back! So much of “bug reporting” feels like talking
        to a wall. So it is a nice surprise when the wall talks back! I
        answered their questions and am now waiting again. I assume that
        getting a response indicates they thought the symptom I
        described sounded wrong. Which is progress. Now we are working
        toward a repeatable test case.<br>
      </p>
      <p>To make the work-around easier to use, I temporarily added my
        Xcode project build directory to my search <code>PATH</code>
        using my new <code>.zshrc</code> file.<br>
      </p>
      <p>I have been working to set up my local Python/Keras/TensorFlow
        environment on my M1 laptop. I've also been doing some TexSyn
        code cleanup, mostly in the Texture class for multi-threaded
        rendering while listening for commands related to hiding/showing
        windows.<br>
      </p>
    </div>
    <div class="post" id="20220702"> <a href="#20220702" class="date">
        July 2, 2022</a>
      <h1>Shells and beans</h1>
      <p>On <a moz-do-not-send="true" href="#20220627">June 27</a> I
        said: “Some bad interaction between the now-recommended <code>zsh</code>
        shell and Homebrew (the package manager I use to install OpenCV)
        means that I can't run the <code>texsyn</code> command on the
        command line!” Now I think that description is completely wrong,
        and that the guilty party is actually Xcode (the macOS IDE) and
        its handling of <b>code signing</b>. I found a work-around and
        filed an official Feedback. The fact that the work-around works
        suggests to me that this is an Xcode issue. If I ever learn one
        way or the other I will report back here.<br>
      </p>
      <p>In any case, TexSyn is now running from the shell as intended.
        I did another run, identical to the previous one, except for a
        different random seed. This one was a bit more successful. Some
        tournament images from late in the run:<br>
      </p>
      <img src="images/20220702_step_1672.png" alt="20220702_step_1672"
        title="20220702_step_1672" class="img_flow" height="512"
        width="512"> <img src="images/20220702_step_1726.png"
        alt="20220702_step_1726" title="20220702_step_1726"
        class="img_flow" height="512" width="512"> <img
        src="images/20220702_step_1891.png" alt="20220702_step_1891"
        title="20220702_step_1891" class="img_flow" height="512"
        width="512"> <img src="images/20220702_step_1944.png"
        alt="20220702_step_1944" title="20220702_step_1944"
        class="img_flow" height="512" width="512"> <img
        src="images/20220702_step_1978.png" alt="20220702_step_1978"
        title="20220702_step_1978" class="img_flow" height="512"
        width="512"> <img src="images/20220702_step_2041.png"
        alt="20220702_step_2041" title="20220702_step_2041"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220627"> <a href="#20220627" class="date">June


















































        27, 2022</a>
      <h1>Shakedown test with Dana's beans<br>
      </h1>
      <p>At this point TexSyn seems to be working well in the “new
        world” (M1 laptop, macOS Monterey). But other problems remain.
        Some bad interaction between the now-recommended <code>zsh</code>
        shell and Homebrew (the package manager I use to install OpenCV)
        means that I can't run the <code>texsyn</code> command on the
        command line! I have not even begun to switch over to using
        Jupyter/Keras/TensorFlow on my local machine, so at the moment
        they still run in the cloud on Google Colab. So I still use
        Google Drive as the communication channel. Google's “Drive for
        Desktop” now uses macOS's “File Provider” layer, so Drive is
        different in subtle ways (mount point pathnames, etc.).<br>
      </p>
      <p>Nonetheless I was able to get a simulation running (<font
          size="-1">ID tiger_eye_beans_20220626_1405</font>). My
        daughter Dana bought these dried beans at a local farmer's
        market. They are an heirloom variety called Tiger Eye, grown by
        Fifth Crow Farms in Pescadero, California. I prevailed on her to
        take some photos before cooking. The run only got to step 1700,
        85% of the typical duration. Here are four hand picked
        tournament images from between steps 1500 and 1700. In the first
        image the lower right prey has inappropriate reds and greens,
        but the spatial features seem just about right. In the second
        image the upper right prey looks very promising. In the fourth
        image the two prey at the bottom seem to me to be clearly better
        than the top prey, but the predator decided to eat one of them.
        Oh well. This is all just to say it seems to be working at the
        same “OK but not great” level of quality seen before.<br>
      </p>
      <img src="images/20220627_step_1520.png" alt="20220627_step_1520"
        title="20220627_step_1520" class="img_flow" height="512"
        width="512"> <img src="images/20220627_step_1623.png"
        alt="20220627_step_1623" title="20220627_step_1623"
        class="img_flow" height="512" width="512"> <img
        src="images/20220627_step_1634.png" alt="20220627_step_1634"
        title="20220627_step_1634" class="img_flow" height="512"
        width="512"> <img src="images/20220627_step_1653.png"
        alt="20220627_step_1653" title="20220627_step_1653"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220623"> <a href="#20220623" class="date">June


















































        23, 2022</a>
      <h1>Multi-threading issues on M1 processor<br>
      </h1>
      <p>It took a while to get my M1 set up to run TexSyn, then to
        adjust my build settings so that I could compile and run on both
        the old and new machines. I wanted to be able to verify
        consistent texture synthesis results, and to measure relative
        performance. What I saw initially was texture rendering was <b>slower</b>
        on the new machine. That was surprising and counterintuitive
        since the clock rate of the new M1 Max processor is 3.2 GHz and
        the older one is 2.5 GHz (2014 MacBook Pro, Quad-Core Intel Core
        i7).<br>
      </p>
      <p>However, by default, TexSyn uses multi-threading during render.
        Each pixel of a TexSyn render is completely unrelated to all
        other pixels — it is “embarrassingly parallel.” So each pixel
        could be its own thread, but with some vague concern about
        per-thread overhead, I made a thread for each <b>row</b> of
        pixels in the rendered image. This worked fine on the old
        processor, the multi-threaded rendering was obviously faster. I
        never bothered to look into it in more detail. On the M1
        processor, this approach did <b>not</b> render faster.<br>
      </p>
      <p>Two smart friends suggested I look closer at the effect of
        multi-threading. I tried increasing the size of the per-thread
        workload (hence an inverse relationship in the number of threads
        per render). Where the old approach had been one rendered row of
        pixels per thread, I changed that to <i>n</i> rows per thread.
        I then swept <i>n</i> from 1 up to the image height. That is
        from the previous multi-thread approach, to a single thread
        approach, and all values in between. For each value of <i>n</i>
        I measured the texture render time. For a given test texture, I
        did that for a 512×512 render, then for a 100×100 render which
        is about the size used in the camouflage simulation. In both
        cases the shape of the curve — while mysterious — appears to
        have a minimum at about 16% of the total texture height.<br>
      </p>
      <img src="images/20220623_512x512.png" alt="512x512 render time
        vs. per-thread workload size" title="512x512 render time vs.
        per-thread workload size" class="img_flow" height="460"
        width="559"><br>
      <img src="images/20220623_100x100.png" alt="100x100 render time
        vs. per-thread workload size" title="100x100 render time vs.
        per-thread workload size" class="img_flow" height="460"
        width="559">
      <p>For comparison, here is the same experiment (100×100 render
        size) run on the old laptop (2014 MacBook Pro, 2.5 GHz Quad-Core
        Intel Core i7):<br>
      </p>
      <img src="images/20220623_100x100_Intel.png" alt="Intel 100x100
        render time vs. per-thread workload size" title="Intel 100x100
        render time vs. per-thread workload size" class="img_flow"
        height="460" width="559">
      <p>This looks <b>very</b> different at the low end. On the M1,
        the render time was highest at the beginning <b>and</b> end of
        the range (100 threads with one row each, versus 1 thread with
        100 rows). As mentioned above the minimum (fastest render) was
        between them, at about 6 threads with 16 rows each. On the Intel
        laptop, the minimum is clearly at the beginning of the range
        (100 threads with one row each), and the maximum is at the end
        of the range (1 thread with 100 rows). This is easier to see by
        ignoring the red plot for run “e” — perhaps another process ran
        during the end of that run. This data shows that render time is
        definitely not monotone increasing. It has that dip around 50.
        But this does clearly show how 1 row per thread worked well on
        the Intel processor. I don't have the background to know, but I
        assume the shape of these curves, and the significant
        differences between old and new, must be related to the two
        processor architectures and how they interact with the operating
        system's threading mechanisms.<br>
      </p>
      <p>Finally here are some time comparisons. The test case here is
        rendering 20 100×100 textures. As above, on the old processor,
        fastest (smallest) render times was for the 1 row per thread
        case at 4.7 seconds. For the new processor, fastest times were
        for the 16 rows per thread case, at 3.5 seconds. So with recent
        changes the new machine seems to be roughly 33% faster. For the
        single-threaded case (right column) the new machine is about 40%
        faster.<br>
      </p>
      <table cellspacing="0" cellpadding="2" border="1" width="900">
        <caption><b>Render time for 20 textures (in seconds):
            architecture versus per-thread workload size</b><br>
        </caption><tbody>
          <tr>
            <td align="center" valign="center"><br>
            </td>
            <td align="center" valign="center">1 row per thread<br>
            </td>
            <td align="center" valign="center">16 rows per thread<br>
            </td>
            <td align="center" valign="center">all 100 rows by 1 thread<br>
            </td>
          </tr>
          <tr>
            <td align="center" valign="center">old / Intel / x86-64<br>
            </td>
            <td align="center" valign="center"><b>4.7</b><br>
            </td>
            <td align="center" valign="center">6.8<br>
            </td>
            <td align="center" valign="center">18.1<br>
            </td>
          </tr>
          <tr>
            <td align="center" valign="center">new / ARM / M1<br>
            </td>
            <td align="center" valign="center">6.7<br>
            </td>
            <td align="center" valign="center"><b>3.5</b><br>
            </td>
            <td align="center" valign="center">6.9<br>
            </td>
          </tr>
        </tbody>
      </table>
      <p><br>
        Just because I like pictures, the comparisons above used this
        test texture:</p>
      <pre>color_noise = ColorNoise(Vec2(1, 2), Vec2(3, 0.1), 0.95)
warped_color_noise = NoiseWarp(5, 0.5, 0.5, color_noise)
blurred = Blur(0.05, warped_color_noise)
spot = Spot(Vec2(), 0.5, blurred, 0.7, warped_color_noise)</pre>
      <img src="images/20220623_test_texture.jpg" alt="" title=""
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220607"> <a href="#20220607" class="date">June

























































        7, 2022</a>
      <h1>New laptop</h1>
      <p>After eight years of faithful service I finally have a new
        laptop. Photo below of old and new in the process of doing a
        brain (well, SSD) transplant. I anticipate that this M1 laptop
        will render TexSyn textures faster. I also expect I will be able
        to run TensorFlow locally, so I can say goodby to the current
        “Rube Goldberg” scheme where I run camouflage evolution on my
        laptop, but predator learning in the cloud with Colab. However
        in the meantime I am working through a series of issues — new
        processor, new operating system, new version of OpenCV — before
        I can even run TexSyn locally. <br>
      </p>
      <img src="images/20220607_new_laptop.jpg" alt="new laptop"
        title="new laptop" class="img_flow" moz-do-not-send="true"
        height="600" width="800"> </div>
    <div class="post" id="20220604"> <a href="#20220604" class="date">June

























































        4, 2022</a>
      <h1>Gave work-in-progress talk at GPTP XIX<br>
      </h1>
      <p>I was invited to speak at the <a
          href="http://gptp-workshop.com/">Genetic Programming Theory
          &amp; Practice XIX</a> workshop, held at the University of
        Michigan. I gave a summary of the experiments described
        previously in this blog. The talk seemed well received.
        Preparing for it gave me a useful opportunity to organize my
        thoughts about the project. I also got to experiment with how
        best to communicate the ideas that underlie this simulation.<br>
      </p>
      <img src="images/20220604_GPTP_talk.jpg" alt="GPTP XIX talk"
        title="GPTP XIX talk" class="img_flow" height="441" width="786">
      <p> Photo (by Linda Wood? — <a moz-do-not-send="true"
          href="https://twitter.com/UMICHCS/status/1533158498078441473">via</a>)
        of me in mid-bloviation:</p>
      <img src="images/20220604_GPTP_photo.jpg" alt="GPTP XIX photo"
        title="GPTP XIX photo" class="img_flow" height="417" width="521">
    </div>
    <div class="post" id="20220531"> <a href="#20220531" class="date">May


























































        31, 2022</a>
      <h1>Another run, making slides<br>
      </h1>
      <p>Using the same set up described on <a moz-do-not-send="true"
          href="#20220530">May 30</a>, I made another run (<font
          size="-1">ID oxalis_sprouts_20220530_1212</font>) for some
        presentation slides I am preparing. As can be seen in these four
        tournament images from near the end of the run, there are many
        prey with high quality camouflage, but enough lower quality prey
        that it was hard to find three good ones in one image. For
        example the upper left prey in the first image, and the upper
        right prey in the last image, both seem quite well suited for
        this environment. It is also encouraging that the prey nearest
        the predator's chosen location, seem to be convincingly of lower
        quality.<br>
      </p>
      <img src="images/20220531_step_1888.png" alt="20220531_step_1888"
        title="20220531_step_1888" class="img_flow" height="512"
        width="512"> <img src="images/20220531_step_1987.png"
        alt="20220531_step_1987" title="20220531_step_1987"
        class="img_flow" height="512" width="512"> <img
        src="images/20220531_step_1994.png" alt="20220531_step_1994"
        title="20220531_step_1994" class="img_flow" height="512"
        width="512"> <img src="images/20220531_step_2002.png"
        alt="20220531_step_2002" title="20220531_step_2002"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220530"> <a href="#20220530" class="date">May

























































































        30, 2022</a>
      <h1>Postpone fine-tuning until dataset is big enough<br>
      </h1>
      <p>I started out intending to compare two runs differing only by
        the “background scale” (hyper)parameter on the TexSyn side. I
        eventually got that but only after several poor quality runs at
        the smaller scale. I decided to bring back an aspect of the “B”
        experiment described on <a moz-do-not-send="true"
          href="#20220521">May 21</a>: make fine-tuning conditional on
        the size of the dataset which collects previous <i>tournament
          images</i>. So now, the simulation proceeds for the first 50
        steps (10% of <code>max_training_set_size</code>) with <b>no</b>
        fine-tuning. This means that the predator is still the
        pre-trained generalist (<i>FCD</i>) model. After 50 tournament
        images are collected, the fine tuning begins and run as before.</p>
      <p>These are four images from late in the run (<font size="-1">ID
          oak_leaf_litter_20220525_1648</font>) with background scale of
        0.4. I think the upper right prey, in the second image (step
        1931), is my favorite of this run. It contains leaf-green edging
        over a pattern quite reminiscent of the leafy background:</p>
      <img src="images/20220530_04_step_1714.png"
        alt="20220530_04_step_1714" title="20220530_04_step_1714"
        class="img_flow" height="512" width="512"> <img
        src="images/20220530_04_step_1931.png"
        alt="20220530_04_step_1931" title="20220530_04_step_1931"
        class="img_flow" height="512" width="512"> <img
        src="images/20220530_04_step_1972.png"
        alt="20220530_04_step_1972" title="20220530_04_step_1972"
        class="img_flow" height="512" width="512"> <img
        src="images/20220530_04_step_1979.png"
        alt="20220530_04_step_1979" title="20220530_04_step_1979"
        class="img_flow" height="512" width="512">
      <p>These are four images from late in the run (<font size="-1">ID
          oak_leaf_litter_20220529_1354</font>) with background scale of
        0.2, after implementing the “postpone fine-tuning until dataset
        is big enough” approach:</p>
      <img src="images/20220530_02_step_1681.png"
        alt="20220530_02_step_1681" title="20220530_02_step_1681"
        class="img_flow" height="512" width="512"> <img
        src="images/20220530_02_step_1793.png"
        alt="20220530_02_step_1793" title="20220530_02_step_1793"
        class="img_flow" height="512" width="512"> <img
        src="images/20220530_02_step_1907.png"
        alt="20220530_02_step_1907" title="20220530_02_step_1907"
        class="img_flow" height="512" width="512"> <img
        src="images/20220530_02_step_1918.png"
        alt="20220530_02_step_1918" title="20220530_02_step_1918"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220521"> <a href="#20220521" class="date">May

























































































        21, 2022</a>
      <h1><i>Ad hoc</i> solution to “inappropriate center fondness”<br>
      </h1>
      <p>As described on <a moz-do-not-send="true" href="#20220518">May
          18</a>, I determined that it was <i>not</i> just my
        imagination: the current predator model <i>does</i> have an
        inappropriate preference for predicting the most conspicuous
        prey is located at the center of its input image. (A “tournament
        image” with three prey over a random background crop.) This is
        true even when there is no prey there, and without regard for
        the quality of prey that is there.<br>
      </p>
      <p>I have only vague theories about why this happens and hope to
        investigate more fully later. But in the short term, I was able
        to find a work-around. The <a moz-do-not-send="true"
          href="#20220518">May 18</a> plot suggests the pre-trained
        generalist FCD predator model (see e.g. <a
          moz-do-not-send="true" href="#20220323">March 23</a>) does <i>not</i>
        have this “center fondness.” Instead it appears to develop while
        during fine-turning the predator model during a camouflage
        simulation run. So I tried avoiding using tournament images with
        any prey located in a region at the center. The good news is
        that this seems to fix the problem. But it feels <i>ad hoc</i>,
        poorly motivated, and overly complicated. I will use it until I
        find a more principled solution to “center fondness.” I tried
        three variations on this work-around:<br>
      </p>
      <ul>
      </ul>
      <ol type="A">
        <li>(<font size="-1">kitchen_granite_20220518_1410</font>)
          Change TexSyn's class EvoCamoVsLearningPredator to place the
          three prey (over a random background crop) so as to avoid a
          circular region at the center. It simply uses the previous
          uniform placement scheme (avoiding the image's margin and
          avoiding prey overlap) then loops, rejecting a placement if
          any prey touch the empty zone. I required that each prey's
          center was at least <b>3</b> × radius away from image center.<br>
        </li>
        <li>(<font size="-1">kitchen_granite_20220519_1717</font>)
          Turned off the approach of <b>A</b>, and instead put a
          constraint on the predator (Keras model in Python). New input
          images were only added to the fine-tuning dataset if they
          avoided the central empty zone. This seemed like a good idea,
          being local to the predator fine-tuning code, but it ended up
          having a lot of corner cases. I was not happy with the
          results. Used min distance of <b>2</b> × radius.</li>
        <li>(<font size="-1">kitchen_granite_20220520_1633</font>)
          Revert code for B. Go back to approach in <b>A</b>. Use <b>2</b>
          × radius.</li>
      </ol>
      <p>Four images from near end of run <b>A</b>:</p>
      <img src="images/20220521_a_step_1938.png"
        alt="20220521_a_step_1938" title="20220521_a_step_1938"
        class="img_flow" height="512" width="512"> <img
        src="images/20220521_a_step_1919.png" alt="20220521_a_step_1919"
        title="20220521_a_step_1919" class="img_flow" height="512"
        width="512"> <img src="images/20220521_a_step_1786.png"
        alt="20220521_a_step_1786" title="20220521_a_step_1786"
        class="img_flow" height="512" width="512"> <img
        src="images/20220521_a_step_1997.png" alt="20220521_a_step_1997"
        title="20220521_a_step_1997" class="img_flow" height="512"
        width="512">
      <p>Four images from second half of run <b>B</b>:</p>
      <img src="images/20220521_b_step_1260.png"
        alt="20220521_b_step_1260" title="20220521_b_step_1260"
        class="img_flow" height="512" width="512"> <img
        src="images/20220521_b_step_1660.png" alt="20220521_b_step_1660"
        title="20220521_b_step_1660" class="img_flow" height="512"
        width="512"> <img src="images/20220521_b_step_1862.png"
        alt="20220521_b_step_1862" title="20220521_b_step_1862"
        class="img_flow" height="512" width="512"> <img
        src="images/20220521_b_step_1890.png" alt="20220521_b_step_1890"
        title="20220521_b_step_1890" class="img_flow" height="512"
        width="512">
      <p>Four images from run <b>C</b>:</p>
      <img src="images/20220521_c_step_620.png"
        alt="20220521_c_step_620" title="20220521_c_step_620"
        class="img_flow" height="512" width="512"> <img
        src="images/20220521_c_step_1539.png" alt="20220521_c_step_1539"
        title="20220521_c_step_1539" class="img_flow" height="512"
        width="512"> <img src="images/20220521_c_step_1767.png"
        alt="20220521_c_step_1767" title="20220521_c_step_1767"
        class="img_flow" height="512" width="512"> <img
        src="images/20220521_c_step_1824.png" alt="20220521_c_step_1824"
        title="20220521_c_step_1824" class="img_flow" height="512"
        width="512">
      <p>This is the metric shown on <a moz-do-not-send="true"
          href="#20220518">May 18</a>: cumulative percentage over
        evolutionary time, how many prey picked by predator model “just
        happen” to be the prey which is located nearest the image's
        center? This was meant to characterize how “center fondness”
        developed during fine tuning. Before the workarounds described
        in this post, that number climbed up to <b>53%</b> by the end
        of the run. With these new changes, all three runs ended with <b>36%</b>
        or lower. It seems the ideal value is 33% (⅓) but apparently
        other factors intrude. Run <b>B</b>, where the constraint was
        applied in the fine-tuning code, the metric gets down to 28%.
        Runs <b>A</b> and <b>C</b>, where the constraint was applied
        in tournament layout (in TexSyn), both ended at 36%, suggesting
        that results are not sensitive to radius of empty zone.<br>
      </p>
      <img src="images/20220521_near_center_plot.png"
        alt="20220521_near_center_plot"
        title="20220521_near_center_plot" class="img_flow" height="330"
        width="518"> </div>
    <div class="post" id="20220518"> <a href="#20220518" class="date">May
































































































        18, 2022</a>
      <h1>Inappropriate fondness for the center?</h1>
      <p>Watching these simulations as they play out, I have made some
        informal subjective observations:</p>
      <ol>
        <li>Many — perhaps 60-70% — are of “disappointing quality.”</li>
        <li>In those runs, I often see the predator make poor choices,
          where the prey it picks, which I intend/hope to be the most
          conspicuous, are often the opposite, appearing to be the best
          camouflaged of the three prey. </li>
        <li>It slowly dawned on me that these poor choices often happen
          to be prey located close to the center of the “tournament”
          image presented as input to the predator model.</li>
      </ol>
      <p>A central difficulty of this work is that I lack an objective
        metric for camouflage quality. That is what has brought me to
        this rather elaborate <i>semi-self-supervised model</i>. So I
        do not have a way to quantify the number of “disappointing
        runs.” It is impossible to quantify when the predator makes a
        poor choice. Indeed, subjectively, I think <a
          moz-do-not-send="true"
          href="https://en.wikipedia.org/wiki/I_know_it_when_I_see_it">“I













































































































          know it when I see it”</a> (to quote SCOTUS Justice Potter
        Stewart in 1964). But that cannot be implemented in code, nor
        objectively measured in experiments.<br>
      </p>
      <p>However “distance from the center” of an image is an objective
        metric which can easily be measured. I tried that in run <font
          size="-1">kitchen_granite_20220517_0845</font> via ad hoc
        printing a metric during the run. See plot below:<br>
      </p>
      <img src="images/20220518_near_center_plot.png"
        alt="20220518_near_center_plot"
        title="20220518_near_center_plot" class="img_flow" height="344"
        width="501">
      <p> As described on <a href="#20220502">May 2</a>, the
        fine-tuning dataset is created in a self-referential loop. A
        tournament image (three prey over background) is presented to
        the predator's CNN model, it predicts (selects) an xy location
        in the image where it thinks a prey is likely to be. This
        location is used to drive texture evolution. It is also used to
        add an example to the dataset used to fine-tune the predator.
        The raw prediction is mapped to the nearest prey's centerpoint
        for use as a label. This label is paired with the image to form
        a training example which is then added to the dataset for
        subsequent fine-tuning. <br>
      </p>
      <p>The plot above shows during 2000 simulation steps, the
        percentage of steps when the prey nearest predator's prediction
        “just happens” to be the prey nearest the center of the image.
        In a perfect world, I would expect this to hover around 33% —
        assuming positions and quality are uniformly distributed —
        picking a prey will happen to be the one (of 3) nearest the
        center about ⅓ of the time. Instead it starts low then keeps
        increasing during predator fine-tuning reaching a max of 53% at
        the end of the run. Way too high and clearly inappropriate.<br>
        <br>
        A cynical perspective on this is that the predator model has
        just given up, stopped trying, and simply returns the constant
        value corresponding to the image center. (Which is (0.5, 0.5) in
        image-normalized coordinates.) Consider these four images (from
        near the end of the run, steps 1767, 1843, 1957, and 1995). The
        predator's response in all cases is very close to the center of
        the image.&nbsp; It chooses the center, even though in 3 of 4
        cases there is no prey there, leading to a <i>predator fail</i>.
        In the last example, there <b>is</b> a prey there, but (in my
        subjective view) it is of pretty high quality (if too green).
        This seems like an example of “poor choices” by the predator as
        mentioned in observation 2 above:</p>
      <img src="images/20220518_step_1767.png" alt="20220518_step_1767"
        title="20220518_step_1767" class="img_flow" height="512"
        width="512"> <img src="images/20220518_step_1843.png"
        alt="20220518_step_1843" title="20220518_step_1843"
        class="img_flow" height="512" width="512"> <img
        src="images/20220518_step_1957.png" alt="20220518_step_1957"
        title="20220518_step_1957" class="img_flow" height="512"
        width="512"> <img src="images/20220518_step_1995.png"
        alt="20220518_step_1995" title="20220518_step_1995"
        class="img_flow" height="512" width="512">
      <p>Here are two more examples from late in this run, grabbed by
        hand. The first image (like the last one in the previous four
        images) shows a prey with very high quality camouflage (edgy
        pattern with muted pinks and yellows) being “eaten” by the
        predator. I suspect this may be accidental targeting because it
        “just&nbsp; happens” to be near the center of the image. The
        second image below is just to show that the predator model is
        not <i>always</i> choosing the image center. Here it appears to
        be making an excellent choice, targeting lower quality (too
        green) prey, toward the right, so <b>not</b> at the center of
        the image. The other two prey in that second image both have
        high quality camouflage. (One is below and slightly to the left
        of the predator's pick. The other (really good!) one is about
        two diameters to the left and slightly above.)<br>
      </p>
      <img src="images/20220518_grab_a.png" alt="20220518_grab_a"
        title="20220518_grab_a" class="img_flow" height="512"
        width="512"> <img src="images/20220518_grab_b.png"
        alt="20220518_grab_b" title="20220518_grab_b" class="img_flow"
        height="512" width="512">
      <p>Next on my agenda is to dig into this “inappropriate fondness
        for the center” issue. I do not know what causes the predator
        model to get lazy and default to a constant response. Perhaps
        merely that it can. And what is to stop it? Probably a large
        basin of attraction for this behavior which is easy to fall
        into.</p>
      <p>Perhaps I will force the random placement of prey on the
        background (by the TexSyn side of the simulation) to push prey
        away from the center of the image. That seems too <i>ad hoc</i>
        to be a permanent solution, but might help understand what is
        going on.<br>
      </p>
    </div>
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20220515"> <a href="#20220515" class="date">May



















































































































        15, 2022</a>
      <h1>Another run, high res, high variance<br>
      </h1>
      <p>In this run (<font size="-1">ID huntington_hedge_20220514_1451</font>),




















































































































        on “Huntington hedge” background (see <a moz-do-not-send="true"
          href="#20210729">July 29, 2021</a>), rendering 512x512 images
        on the TexSyn side produced “pretty good” results. A solid B
        grade, but troubling that this was the best of three runs,
        identical except for random number seed. As discussed on <a
          moz-do-not-send="true" href="#20220508">May 8</a>, these
        “could have been better” runs seem to suffer from bad choices by
        the predator. <br>
      </p>
      <p>Consider the fifth image below. (Hovering over it should
        indicate “step_1254.”) The predator has chosen a green prey with
        an effective <i>disruptive</i> pattern. To me that one seems
        the best of the three prey. It “feels” like the predator is
        using the wrong criteria. Because of the semi-self-supervised
        nature of the predator fine-tuning, I worry about a feedback
        loop where the predator makes a mistake early, then effectively
        amplifies its mistake over time. On the other hand, the
        evolutionary texture synthesis seems to keep proposing high
        quality patterns, such as the upper right prey in the final
        image (“step_1976”).<br>
      </p>
      <img src="images/20220515_step_95.png" alt="20220515_step_95"
        title="20220515_step_95" class="img_flow" height="512"
        width="512"> <img src="images/20220515_step_399.png"
        alt="20220515_step_399" title="20220515_step_399"
        class="img_flow" height="512" width="512"> <img
        src="images/20220515_step_665.png" alt="20220515_step_665"
        title="20220515_step_665" class="img_flow" height="512"
        width="512"> <img src="images/20220515_step_855.png"
        alt="20220515_step_855" title="20220515_step_855"
        class="img_flow" height="512" width="512"> <img
        src="images/20220515_step_1254.png" alt="20220515_step_1254"
        title="20220515_step_1254" class="img_flow" height="512"
        width="512"> <img src="images/20220515_step_1482.png"
        alt="20220515_step_1482" title="20220515_step_1482"
        class="img_flow" height="512" width="512"> <img
        src="images/20220515_step_1615.png" alt="20220515_step_1615"
        title="20220515_step_1615" class="img_flow" height="512"
        width="512"> <img src="images/20220515_step_1729.png"
        alt="20220515_step_1729" title="20220515_step_1729"
        class="img_flow" height="512" width="512"> <img
        src="images/20220515_step_1881.png" alt="20220515_step_1881"
        title="20220515_step_1881" class="img_flow" height="512"
        width="512"> <img src="images/20220515_step_1976.png"
        alt="20220515_step_1976" title="20220515_step_1976"
        class="img_flow" height="512" width="512"> </div>
    <div class="post" id="20220511"> <a href="#20220511" class="date">May



















































































































        11, 2022</a>
      <h1>Higher resolution on the TexSyn side</h1>
      <p>I finally got around to replacing some slap-dash
        just-for-a-quick-test code from two months ago, so can adjust
        resolution of camouflage textures and “tournament images.” (Such
        as below, with three disks of evolved camouflage texture over a
        random crop of photographic background texture.) My general goal
        is to decouple the human readable/viewable output of these
        simulations from the tiny images used thus far in the computer
        vision part.<br>
      </p>
      <p>Shown below is a 512×512 image from yesterday's run (<font
          size="-1">ID michaels_gravel_20220510_1255</font>), a 256×256
        image (the size I had been using for a while), and just for
        comparison, a 128×128 image as still used on the predator vision
        side of the simulation. All three images are step 1577 of their
        respective simulations. The smaller two where from the runs
        described on <a moz-do-not-send="true" href="#20220508">May 8</a>.
        The larger two were both run using the same random seed, while
        other hyperparameters differ (background scale and output size)
        so they were not identical. Perhaps fine-tuning the predator
        model is non-deterministic due to asynchronous parallelism, but
        in any case I will not sweat the
        not-identical-but-still-quite-similar nature of the first two
        images below.<br>
      </p>
      <p>Lisa had an epic battle with our internet service, she sat
        through several endless support calls, hosted a site visit, and
        finally swapped out the cable modem. Seems solid now. But
        yesterday's run was interrupted at step 1652 by some unrelated
        problem on this elderly researcher's elderly laptop. I decided
        it was not worth running yet again, so just accepted this
        partial run as proof of concept.<br>
      </p>
      <img src="images/20220511_step_1577_big.png" alt="step_1577 big"
        title="step_1577 big" class="img_flow" height="512" width="512">
      <br>
      <img src="images/20220511_step_1577_lil.png" alt="step_1577
        little" title="step_1577 little" class="img_flow" height="256"
        width="256"> <br>
      <img src="images/20220511_step_1577_tiny.png" alt="step_1577 tiny"
        title="step_1577 tiny" class="img_flow" height="128" width="128">
    </div>
    <div class="post" id="20220508"> <a href="#20220508" class="date">May


























































































































        8, 2022</a>
      <h1>Variance in adversarial camouflage runs<br>
      </h1>
      <p>A tale of two adversarial camouflage runs. Maybe not the best
        of runs and the worst of runs, but a darn good one, and one that
        was disappointing.<br>
      </p>
      <p>In the disappointing run (<font size="-1">michaels_gravel_20220505_2112)</font>
        prey camouflage improved but remained mediocre. The prey were
        easy to spot against the background. (Which is photos of gravel
        in our neighbor's yard.) Several times I saw the predator in
        that run target/eat what looked to me like more promising
        camouflage, slowing progress, and allowing the “only OK”
        camouflage to survive. <br>
      </p>
      <p>For the other run (<font size="-1">michaels_gravel_20220507_0839</font>)
        I kept all hyperparameters unchanged except the random seed for
        the evolutionary texture synthesis side. This run performed
        significantly better. <br>
      </p>
      <p>So two samples from this distribution—camouflage simulation
        runs differing only in random seed—can yield <b>very</b>
        different results. I need to dig in to this concept. I need to
        make a series of runs to collect enough data to characterize
        this random distribution. I also have to take this into account
        when adjusting hyperparameter values, like the recent drama
        about “predator fine-tuning dataset size.” Trying a single test,
        then changing a hyperparameter, and making another single test,
        is subject to the large variance shown below. Is run B better
        than run A because of the changed hyperparameter value, or
        because of the expected natural variation in the random
        simulation process?<br>
      </p>
      <p>Some of the “least bad” tournament images from near the end of
        the disappointing run (<font size="-1">ID
          michaels_gravel_20220505_2112</font>). These camouflage
        patterns appear to be primarily derived from the “phasor noise”
        texture generators, see <a moz-do-not-send="true"
          href="#20210508">May 8, 2021</a>:<br>
      </p>
      <img src="images/20220508a_step_1672.png"
        alt="20220508a_step_1672" title="20220508a_step_1672"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508a_step_1691.png" alt="20220508a_step_1691"
        title="20220508a_step_1691" class="img_flow" height="256"
        width="256"> <img src="images/20220508a_step_1919.png"
        alt="20220508a_step_1919" title="20220508a_step_1919"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508a_step_1976.png" alt="20220508a_step_1976"
        title="20220508a_step_1976" class="img_flow" height="256"
        width="256">
      <p>Here are 20 images from throughout the better run (<font
          size="-1">ID michaels_gravel_20220507_0839</font>) which
        differed only in initial random seed value. Over evolutionary
        time, these camouflage patterns increasingly match the colors
        and frequencies of the background photos, and develop a
        disruptive quality where the boundary between background and
        prey camouflage becomes harder to discern:<br>
      </p>
      <img src="images/20220508_step_57.png" alt="20220508_step_57"
        title="20220508_step_57" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_266.png"
        alt="20220508_step_266" title="20220508_step_266"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_380.png" alt="20220508_step_380"
        title="20220508_step_380" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_494.png"
        alt="20220508_step_494" title="20220508_step_494"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_589.png" alt="20220508_step_589"
        title="20220508_step_589" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_722.png"
        alt="20220508_step_722" title="20220508_step_722"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_931.png" alt="20220508_step_931"
        title="20220508_step_931" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_1064.png"
        alt="20220508_step_1064" title="20220508_step_1064"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_1178.png" alt="20220508_step_1178"
        title="20220508_step_1178" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_1235.png"
        alt="20220508_step_1235" title="20220508_step_1235"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_1254.png" alt="20220508_step_1254"
        title="20220508_step_1254" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_1425.png"
        alt="20220508_step_1425" title="20220508_step_1425"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_1539.png" alt="20220508_step_1539"
        title="20220508_step_1539" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_1653.png"
        alt="20220508_step_1653" title="20220508_step_1653"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_1672.png" alt="20220508_step_1672"
        title="20220508_step_1672" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_1767.png"
        alt="20220508_step_1767" title="20220508_step_1767"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_1805.png" alt="20220508_step_1805"
        title="20220508_step_1805" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_1881.png"
        alt="20220508_step_1881" title="20220508_step_1881"
        class="img_flow" height="256" width="256"> <img
        src="images/20220508_step_1900.png" alt="20220508_step_1900"
        title="20220508_step_1900" class="img_flow" height="256"
        width="256"> <img src="images/20220508_step_1976.png"
        alt="20220508_step_1976" title="20220508_step_1976"
        class="img_flow" height="256" width="256">
      <p>Two charts for the second () run (“better”: <font size="-1">ID
          michaels_gravel_20220507_0839</font>. Predator failures
        (targeting a position not inside any prey) reach a level of 31%
        (616 of 2000). The in_disk metric reaches 0.9 around step 600,
        then actually dips below around step 1300, before peaking to
        about 0.94 near the end of the run.<br>
      </p>
      <img src="images/20220508_pred_fail_plot.png"
        alt="20220508_pred_fail_plot" title="20220508_pred_fail_plot"
        class="img_flow" height="330" width="520"> <img
        src="images/20220508_in_disk_plot.png"
        alt="20220508_in_disk_plot" title="20220508_in_disk_plot"
        class="img_flow" height="330" width="520"> </div>
    <div class="post" id="20220505"> <a href="#20220505" class="date">May


























































































































        5, 2022</a>
      <h1>Forget predator’s oldest memories, don’t update dataset, wait
        for cable guy<br>
      </h1>
      <p>Following up after the <a moz-do-not-send="true"
          href="#20220502">May 2</a> post: I have gone back to using the
        most recent <i>n</i> simulation steps as the dataset for
        predator fine-tuning. Previous values for <i>n</i> I tried
        were: 100, 200, ∞, and now 500. A typical run has 2000
        simulation steps, so the fine-tuning dataset size is now ¼ of
        all steps.<br>
      </p>
      <p>The idea of “updating” this dataset, as described on <a
          moz-do-not-send="true" href="#20220502">May 2</a>, was
        intended to avoid using obsolete/naïve data for fine tuning.
        Some experiments suggested it was not helping. Fortunately, it
        can be ignored, since “the most recent <i>n</i> simulation
        steps” inherently erases older training data, so there is no
        reason to update it. And that is good since it seemed overly
        complicated and <i>ad hoc</i>.<br>
      </p>
      <p>I think this 5 day detour started because I tried changing two
        things at the same time: adding the “update” mechanism and using
        a new background set. It looks like the <font size="-1">yellow_flower_on_green</font>
        background set is somewhat more challenging for the simulation
        than the previous example <font size="-1">maple_leaf_litter</font>.
        My guess is that using the full history of simulation steps for
        fine-tuning worked OK on <font size="-1">maple_leaf_litter</font>
        but not on <font size="-1">yellow_flower_on_green</font>. I
        will now use <b>500</b> (25% of total) for this hyperparameter.
        That produced acceptable results in this run (<font size="-1">ID
          yellow_flower_on_green_20220504_2109</font>).<br>
      </p>
      <p>I might have resolved this quicker but for the frequent network
        outages we have been experiencing here at our house. I am using
        <a moz-do-not-send="true"
          href="https://colab.research.google.com">Colab</a>, and flaky
        network is the natural enemy of hours long simulations run on a
        remote cloud server. My techie wife Lisa bravely sat on the
        phone with the cable company to convince them there was a
        problem, then dealt with the service tech who paid us a visit.<br>
      </p>
      <img src="images/20220505_step_190.png" alt="20220505_step_190"
        title="20220505_step_190" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_323.png"
        alt="20220505_step_323" title="20220505_step_323"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_437.png" alt="20220505_step_437"
        title="20220505_step_437" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_551.png"
        alt="20220505_step_551" title="20220505_step_551"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_779.png" alt="20220505_step_779"
        title="20220505_step_779" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_893.png"
        alt="20220505_step_893" title="20220505_step_893"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_988.png" alt="20220505_step_988"
        title="20220505_step_988" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_1216.png"
        alt="20220505_step_1216" title="20220505_step_1216"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_1330.png" alt="20220505_step_1330"
        title="20220505_step_1330" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_1368.png"
        alt="20220505_step_1368" title="20220505_step_1368"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_1444.png" alt="20220505_step_1444"
        title="20220505_step_1444" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_1501.png"
        alt="20220505_step_1501" title="20220505_step_1501"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_1558.png" alt="20220505_step_1558"
        title="20220505_step_1558" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_1672.png"
        alt="20220505_step_1672" title="20220505_step_1672"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_1710.png" alt="20220505_step_1710"
        title="20220505_step_1710" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_1786.png"
        alt="20220505_step_1786" title="20220505_step_1786"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_1805.png" alt="20220505_step_1805"
        title="20220505_step_1805" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_1843.png"
        alt="20220505_step_1843" title="20220505_step_1843"
        class="img_flow" height="256" width="256"> <img
        src="images/20220505_step_1900.png" alt="20220505_step_1900"
        title="20220505_step_1900" class="img_flow" height="256"
        width="256"> <img src="images/20220505_step_1938.png"
        alt="20220505_step_1938" title="20220505_step_1938"
        class="img_flow" height="256" width="256">
      <p>Metric plots for run (<font size="-1">ID
          yellow_flower_on_green_20220504_2109</font>). At the end of
        the run, the predator failure rate (fraction of steps where
        predator target a location outside the boundary of any prey) was
        <b>30%</b> (592 out of 2000) which is quite low compared to
        recent runs. The in_disk metric reached 90% by around step 400
        and peaked at 96% toward the end of the 2000 step run.<br>
      </p>
      <img src="images/20220505_pred_fail_plot.png"
        alt="20220505_pred_fail_plot" title="20220505_pred_fail_plot"
        class="img_flow" height="330" width="520"> <img
        src="images/20220505_in_disk_plot.png"
        alt="20220505_in_disk_plot" title="20220505_in_disk_plot"
        class="img_flow" height="330" width="520">&nbsp; </div>
    <div class="post" id="20220502"> <a href="#20220502" class="date">May






































































































































        2, 2022</a>
      <h1>Hmm...part 2<br>
      </h1>
      <p>I continue to fiddle with details of fine-tuning the predator
        model during a single run of camouflage evolution. I
        decided—based on meager evidence—that a training set of 100
        examples was not as good as 200 example, and 200 was not as good
        as remembering <b>all</b> previous steps (so 2000 at the end of
        a typical run). Perhaps bigger is better, but I worry that older
        is worse. I was concerned that training examples saved at the
        beginning of a run — effectively the predator's “childhood
        memories” — might interfere with effective fine-tuning later on.<br>
      </p>
      <p>The issue is that while the simulation can provide <i>ground
          truth</i> for centerpoint positions of the three prey, it does
        not know which of the three prey <b>should</b> be selected by
        the predator. If we had a computable metric for “camouflage
        quality” or “conspicuousness” then this could be a much less
        complicated simulation!<br>
      </p>
      <p>Instead, to create an example for the fine-tuning dataset, I
        generate the label (an xy position) for an image by using the
        predator's current neural net model to predict (get its best
        guess) where a prey is. (This prediction is shown as black and
        white crosshair and circle in the images below.) Then using the
        ground truth prey position data, I select the prey <b>nearest</b>
        to the predicted position. This effectively assumes the predator
        “meant” to target that nearest prey, but missed by some small
        amount. These form a training example: the “tournament” image
        and the position of the prey nearest the predator's prediction.
        That fine-tuning training example, which will be remembered for
        the rest of the run, may be based on a very “naive” early
        predator model. (Similar to the FCD6 pre-trained model.) The
        point of fine-tuning is to improve the predator over time. Do we
        want to train based on its early mistakes? So I added some code
        to occasionally, randomly, update the label for examples in the
        training set. It recomputes the training example label based on
        the predator's <b>current</b> prediction and the saved ground
        truth data. This forgets the old label and substitutes a new
        “modern” one. It seemed like a good idea to me.<br>
      </p>
      <p>The result was pretty bad, as can be seen in the poorly
        camouflaged prey in sample images below from late in this run (<font
          size="-1">ID yellow_flower_on_green_20220501_1140</font>).
        Compare these with a human-in-loop run from <a
          moz-do-not-send="true" href="#20210531">May 31, 2021</a> on
        the same background. I'm running another simulation, identical
        but for random seed, to learn more.<br>
      </p>
      <img src="images/20220502_step_1710.png" alt="20220502_step_1710"
        title="20220502_step_1710" class="img_flow" height="256"
        width="256"> <img src="images/20220502_step_1748.png"
        alt="20220502_step_1748" title="20220502_step_1748"
        class="img_flow" height="256" width="256"> <img
        src="images/20220502_step_1805.png" alt="20220502_step_1805"
        title="20220502_step_1805" class="img_flow" height="256"
        width="256"> <img src="images/20220502_step_1824.png"
        alt="20220502_step_1824" title="20220502_step_1824"
        class="img_flow" height="256" width="256"> </div>
    <div class="post" id="20220425"> <a href="#20220425" class="date">April














































































































































        25, 2022</a>
      <h1>Total recall: use <i>entire</i> history for predator fine
        tuning<br>
      </h1>
      <p>Recent posts (<a href="#20220421">April 21</a>, <a
          href="#20220424">April 24</a>) have discussed the <b>size</b>
        of a training set used for fine-tuning the predator model on
        each step of a simulation. This fine-tuning adapts a predator to
        a specific environment and its evolved population of prey. Data
        in the training set comes from previous simulation steps —
        essentially the “memories” of the predator's experience during
        its lifetime. Originally I thought I would use just the current
        step for fine tuning, then I tried using the “last <i>n</i>”
        steps for training, trying different values for <i>n</i> (100
        vs. 200). Finally in this run I set <i>n</i> to infinity. (In
        the code, this variable is: <code>max_training_set_size</code>.)














































































































































        So here, the training set used for fine-turning consists of <b>all</b>
        previous steps. Otherwise, this run (<font size="-1">ID
          maple_leaf_litter_20220424_1144</font>) is identical to the
        runs described on <a href="#20220421">April 21</a> and <a
          href="#20220424">April 24</a>, except that this one got
        interrupted around step 1900. I think the late-in-run camouflage
        here is better than those previous runs with <code>max_training_set_size</code>
        set to 100 or 200. <br>
      </p>
      <p>Pictured below are a selection of 20 simulation steps out of
        1900. Some at the bottom are quite nice. See for example the
        prey in the lower right corner of the last image below.<br>
      </p>
      <img src="images/20220425_step_95.png" alt="20220425_step_95"
        title="20220425_step_95" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_133.png"
        alt="20220425_step_133" title="20220425_step_133"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_304.png" alt="20220425_step_304"
        title="20220425_step_304" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_418.png"
        alt="20220425_step_418" title="20220425_step_418"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_494.png" alt="20220425_step_494"
        title="20220425_step_494" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_646.png"
        alt="20220425_step_646" title="20220425_step_646"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_836.png" alt="20220425_step_836"
        title="20220425_step_836" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_950.png"
        alt="20220425_step_950" title="20220425_step_950"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_1216.png" alt="20220425_step_1216"
        title="20220425_step_1216" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_1311.png"
        alt="20220425_step_1311" title="20220425_step_1311"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_1330.png" alt="20220425_step_1330"
        title="20220425_step_1330" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_1425.png"
        alt="20220425_step_1425" title="20220425_step_1425"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_1520.png" alt="20220425_step_1520"
        title="20220425_step_1520" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_1558.png"
        alt="20220425_step_1558" title="20220425_step_1558"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_1672.png" alt="20220425_step_1672"
        title="20220425_step_1672" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_1786.png"
        alt="20220425_step_1786" title="20220425_step_1786"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_1805.png" alt="20220425_step_1805"
        title="20220425_step_1805" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_1862.png"
        alt="20220425_step_1862" title="20220425_step_1862"
        class="img_flow" height="256" width="256"> <img
        src="images/20220425_step_1881.png" alt="20220425_step_1881"
        title="20220425_step_1881" class="img_flow" height="256"
        width="256"> <img src="images/20220425_step_1900.png"
        alt="20220425_step_1900" title="20220425_step_1900"
        class="img_flow" height="256" width="256">
      <p>Some statistics from this run. The cumulative predator failure
        rate is especially low: <b>34%</b> (650 fails during 1900
        steps). During fine-tuning, the in_disk metric (rate of predator
        predictions that were <i>inside</i> some prey's disk) reached
        90% by about step 500, then stayed above that level for the rest
        of the run. These both suggest a well performing predator and
        that <code>max_training_set_size</code> should stay at ∞.<br>
      </p>
      <img src="images/20220425_pred_fail_plot.png"
        alt="20220425_pred_fail_plot" title="20220425_pred_fail_plot"
        class="img_flow" height="327" width="535"> <img
        src="images/20220425_in_disk_plot.png"
        alt="20220425_in_disk_plot" title="20220425_in_disk_plot"
        class="img_flow" height="327" width="535"> </div>
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20220424"> <a href="#20220424" class="date">April





















































































































































        24, 2022</a>
      <h1>Hmm...</h1>
      <p>I was happy that I seemed to have stumbled onto a good set of
        “hyperparameters” (as discussed on <a moz-do-not-send="true"
          href="#20220421">April 21</a>) and decided that I would try
        another run to convince myself that it was not just a lucky
        accident. This run (<font size="-1">ID
          maple_leaf_litter_20220422_2106</font>) left me unconvinced.
        There was some obvious convergence toward better camouflage but
        it was not as vivid as I hoped. Several plausible varieties of
        camouflage patterns arose during the run but failed to dominate
        the population. For example, the last (step 1938) of these 20
        examples (selected from a 2000 step run) has two “stripy”
        patterns, with almost an embossed look, in greens and blues. To
        me they look out of place and conspicuous on this background
        set. Similarly several late-in-run examples (such as next to
        last, step 1900) has a noisy “confetti” pattern which also seems
        to contrast with the background. Now I'm thinking about trying
        more alternative approaches.<br>
      </p>
      <img src="images/20220424_step_76.png" alt="20220424_step_76"
        title="20220424_step_76" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_247.png"
        alt="20220424_step_247" title="20220424_step_247"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_266.png" alt="20220424_step_266"
        title="20220424_step_266" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_361.png"
        alt="20220424_step_361" title="20220424_step_361"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_494.png" alt="20220424_step_494"
        title="20220424_step_494" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_646.png"
        alt="20220424_step_646" title="20220424_step_646"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_703.png" alt="20220424_step_703"
        title="20220424_step_703" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_760.png"
        alt="20220424_step_760" title="20220424_step_760"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_855.png" alt="20220424_step_855"
        title="20220424_step_855" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_912.png"
        alt="20220424_step_912" title="20220424_step_912"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_969.png" alt="20220424_step_969"
        title="20220424_step_969" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_1368.png"
        alt="20220424_step_1368" title="20220424_step_1368"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_1425.png" alt="20220424_step_1425"
        title="20220424_step_1425" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_1539.png"
        alt="20220424_step_1539" title="20220424_step_1539"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_1672.png" alt="20220424_step_1672"
        title="20220424_step_1672" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_1786.png"
        alt="20220424_step_1786" title="20220424_step_1786"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_1805.png" alt="20220424_step_1805"
        title="20220424_step_1805" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_1881.png"
        alt="20220424_step_1881" title="20220424_step_1881"
        class="img_flow" height="256" width="256"> <img
        src="images/20220424_step_1900.png" alt="20220424_step_1900"
        title="20220424_step_1900" class="img_flow" height="256"
        width="256"> <img src="images/20220424_step_1938.png"
        alt="20220424_step_1938" title="20220424_step_1938"
        class="img_flow" height="256" width="256">
      <p>Two “bonus” step images with interesting camouflage patterns,
        captured by hand from near end of run. In the first, all three
        prey seem well colored, the lower two have patterns which are a
        good middle ground between structure and confetti noise. In the
        second image, the prey just above and right of center, appears <i>very</i>
        well suited to this environment. It is disappointing that this
        was not more widespread in the population.<br>
      </p>
      <img src="images/20220424_extra_1.png" alt="20220424_extra_1"
        title="20220424_extra_1" class="img_flow" height="256"
        width="256"> <img src="images/20220424_extra_2.png"
        alt="20220424_extra_2" title="20220424_extra_2" class="img_flow"
        height="256" width="256"> </div>
    <div class="post" id="20220421"> <a href="#20220421" class="date">April

























































































































































        21, 2022</a>
      <h1>Remembrance of predations past: how much is best?</h1>
      <p>The current adaptive predator model does <i>fine-tuning</i>
        during a run based on earlier simulation steps. In a sense it is
        learning from its experience in the simulated world. Training
        examples used for fine-turning consist of a “tournament image”
        along with a label. Each image is a random crop of a background
        image, overlaid with three prey, rendered as disk-shaped samples
        of evolved camouflage texture. The simulation knows the “ground
        truth” position for each prey. The chosen label is the prey
        center position nearest the predator neural net model's current
        prediction for the image. So there is a free hyper-parameter:
        how many past simulation steps are remembered in this
        fine-tuning training set. <br>
      </p>
      <p>Typically I have been running simulations for 2000 steps. I had
        a good result on <a moz-do-not-send="true" href="#20220417">April
























































































































































          17</a> using the most recent 200 steps for a fine tuning
        training set. I wondered if this might be too much, causing the
        predator model to “worry” too much about examples from the
        “distant past.” I did two runs, trying to hold everything else
        constant, with 100 and 200 as the fine tuning training set size.<br>
      </p>
      <p>I used the whimsical “bean soup mix” background set (a mixture
        of dried beans from our local supermarket) and made two runs
        using the same random seeds as starting points. The short answer
        is that fine tuning with the most recent <b>200</b> steps (10%
        of total steps) seemed to produce better results. This is a
        pretty small experimental comparison, but for now I will use
        that value.<br>
      </p>
      <p>Here is an example of what seemed like poor choices being made
        by the predator when using the smaller training set size (100)
        for fine tuning. This image was captured by hand near the end of
        the run. Two of the three prey have large conspicuous stripes
        across them, two pink, one beige. The third prey (left of and
        below image center) seems to me to have more effective
        camouflage. Perhaps not saturated enough, but at least it has no
        bold stripes! Yet it was this apparently better one that the
        predator selected for lunch (b&amp;w crosshairs on right of
        prey):<br>
      </p>
      <img src="images/20220421_poor_choice.jpg" alt="poor choice"
        title="poor choice" class="img_flow" height="256" width="256">
      <p>Not sure if this shows cause or effect, but I have been keeping
        track of the <code>in_disk</code> metric during fine-turning. I
        used it previously during pre-training (e.g. for measuring the
        quality of FCD5 and FCD6). It measures how frequently the
        predator model predicts a location inside one of the prey. In
        the plot below there is a clear distinction between the runs
        with 100 and 200 fine-tuning samples. The 200 sample
        fine-turning training set definitely has higher scores, peaking
        at about 90%. The 100 sample run averages somewhere near 60%.<br>
      </p>
      <img src="images/20220421_in_disk_plot.png" alt="in_disk plot"
        title="in_disk plot" class="img_flow" height="322" width="523">
      <p>Along the same lines, the 100 sample fine-tuning set has
        generally higher (worse) cumulative predator failure count than
        the 200 sample approach. These measure the actual performance of
        the predator model when it “hunts” prey (predicts prey position)
        <b>during</b> a simulation step. Whereas the <code>in_disk</code>
        metric mentioned above is measured during fine turning,
        supervised learning, which effectively occurs <b>between</b>
        simulation steps. In these two runs, the final cumulative
        predator failure rate is <b>42% for 100 samples</b> and <b>40%
          for 200 samples</b> (847 and 738 fails during 2000 steps):<br>
      </p>
      <img src="images/20220421_predator_fails.png" alt="predator fails
        plot" title="predator fails plot" class="img_flow" height="308"
        width="532">
      <p>Here are 10 steps hand selected from the 2000 steps of the run
        with a fine-tuning training set derived from the <b>100</b>
        most recent simulation steps (<font size="-1">run ID
          bean_soup_mix_20220420_1143</font>). Camouflage quality
        clearly improves during the run but does not reach the level
        seen in the other run (further below).<br>
      </p>
      <img src="images/20220421_step_0.png" alt="20220421_step_0"
        title="20220421_step_0" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_209.png"
        alt="20220421_step_209" title="20220421_step_209"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_418.png" alt="20220421_step_418"
        title="20220421_step_418" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_684.png"
        alt="20220421_step_684" title="20220421_step_684"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_760.png" alt="20220421_step_760"
        title="20220421_step_760" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_988.png"
        alt="20220421_step_988" title="20220421_step_988"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_1140.png" alt="20220421_step_1140"
        title="20220421_step_1140" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_1634.png"
        alt="20220421_step_1634" title="20220421_step_1634"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_1843.png" alt="20220421_step_1843"
        title="20220421_step_1843" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_1957.png"
        alt="20220421_step_1957" title="20220421_step_1957"
        class="img_flow" height="256" width="256">
      <p>Here are 10 steps hand selected from the 2000 steps of the run
        with a fine-tuning training set derived from the <b>200</b>
        most recent simulation steps (<font size="-1">run ID
          bean_soup_mix_20220418_1642</font>). I suggest that these seem
        to exhibit subjectively <b>better camouflage</b>. <br>
      </p>
      <img src="images/20220421_step_0.png" alt="20220421_step_0"
        title="20220421_step_0" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_285b.png"
        alt="20220421_step_285b" title="20220421_step_285b"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_494b.png" alt="20220421_step_494b"
        title="20220421_step_494b" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_855b.png"
        alt="20220421_step_855b" title="20220421_step_855b"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_1121b.png" alt="20220421_step_1121b"
        title="20220421_step_1121b" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_1159b.png"
        alt="20220421_step_1159b" title="20220421_step_1159b"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_1501b.png" alt="20220421_step_1501b"
        title="20220421_step_1501b" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_1520b.png"
        alt="20220421_step_1520b" title="20220421_step_1520b"
        class="img_flow" height="256" width="256"> <img
        src="images/20220421_step_1843b.png" alt="20220421_step_1843b"
        title="20220421_step_1843b" class="img_flow" height="256"
        width="256"> <img src="images/20220421_step_1957b.png"
        alt="20220421_step_1957b" title="20220421_step_1957b"
        class="img_flow" height="256" width="256">
      <p>(<b>Note:</b> previously when I have said “20 hand selected
        steps out of 2000” — first of all, the simulation automatically
        saves every 20ᵗʰ “tournament” image — so the hand selection is
        made from this randomly culled set of 5% of all tournament
        images. Often I make an aesthetic choice for
        interesting/cool/effective camouflage patterns. I also sample
        non-uniformly to create a more linear time progression in this
        document from random textures to effective camouflage. In this
        case, to allow a better comparison between the 100 versus 200
        training set size, I tried to maintain more “objective” sampling
        by breaking the saved images into 10 groups according to
        evolution order, and selecting one image from each group for the
        two histories above. Note that both histories start from the
        same image. Further, a given image in one history, say the
        third, corresponds to the third image image in the other
        history, and (for this example) both represent the range of
        simulation steps from 400 to 600.)<br>
        <br>
      </p>
    </div>
    <div class="post" id="20220417"> <a href="#20220417" class="date">April
































































































































































        17, 2022</a>
      <h1>Evolving camouflage versus adapting predator (<i>huzzah!</i>)</h1>
      <p>As near as I can tell—I think this is working! Camouflage
        evolves as before, with relative fitness determined in a (three
        way) tournament decided by a predator. This time however, the
        predator is adapting (“fine tuning”) to the specific conditions
        of each simulation run, such as choice of background images, and
        history of camouflage evolution in this population of prey.<br>
      </p>
      <p>The first attempt, on <a moz-do-not-send="true"
          href="#20220411">April 11</a>, failed to converge to effective
        camouflage. In this second attempt, I maintain a training set
        consisting of the most recent 200 steps. I do one epoch of
        training for each simulation step. (Previously, I fine-tuned
        from a single step's data, then all steps, then 100.) The
        quality of this run strikes me as “just OK” but good enough that
        I am convinced the simulation is working as intended. <i>Finally!</i><br>
      </p>
      <p>To recap, this adversarial model has gone through several
        architectures. Initially, predator behavior was provided by a
        human-in-the-loop, in an interactive game-like setting. Then a
        computational predator based on a convolutional neural net was
        pre-trained to locate “prey sized” areas which visually
        contrasted somehow with their background. And now that
        pre-trained predator model is <i>fine tuned</i> during the
        simulation based on “ground truth” position data about the prey.
        This allows the predator model to learn the specifics of its
        simulated world. It learns what the background environment is
        like in its world. It also learns what kind of camouflage is
        being evolved by the prey in its world.<br>
      </p>
      <p>Shown below are 20 hand-selected step images from this 2000
        step run (<font size="-1">ID tree_leaf_blossom_sky_20220416_1148</font>).




































































































































































        In each case, a random crop of a background is overlaid with
        three prey shown as a disk of evolved camouflage texture. A
        black and white circle and crosshair indicate where the predator
        has “decided” or “predicted” that one of its prey is located.<br>
        <br>
      </p>
      <img src="images/20220417_step_19.png" alt="20220417_step_19"
        title="20220417_step_19" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_38.png"
        alt="20220417_step_38" title="20220417_step_38" class="img_flow"
        height="256" width="256"> <img
        src="images/20220417_step_285.png" alt="20220417_step_285"
        title="20220417_step_285" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_323.png"
        alt="20220417_step_323" title="20220417_step_323"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_589.png" alt="20220417_step_589"
        title="20220417_step_589" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_665.png"
        alt="20220417_step_665" title="20220417_step_665"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_703.png" alt="20220417_step_703"
        title="20220417_step_703" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_779.png"
        alt="20220417_step_779" title="20220417_step_779"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_817.png" alt="20220417_step_817"
        title="20220417_step_817" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_1007.png"
        alt="20220417_step_1007" title="20220417_step_1007"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_1102.png" alt="20220417_step_1102"
        title="20220417_step_1102" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_1121.png"
        alt="20220417_step_1121" title="20220417_step_1121"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_1273.png" alt="20220417_step_1273"
        title="20220417_step_1273" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_1330.png"
        alt="20220417_step_1330" title="20220417_step_1330"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_1425.png" alt="20220417_step_1425"
        title="20220417_step_1425" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_1577.png"
        alt="20220417_step_1577" title="20220417_step_1577"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_1767.png" alt="20220417_step_1767"
        title="20220417_step_1767" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_1805.png"
        alt="20220417_step_1805" title="20220417_step_1805"
        class="img_flow" height="256" width="256"> <img
        src="images/20220417_step_1919.png" alt="20220417_step_1919"
        title="20220417_step_1919" class="img_flow" height="256"
        width="256"> <img src="images/20220417_step_1995.png"
        alt="20220417_step_1995" title="20220417_step_1995"
        class="img_flow" height="256" width="256">
      <p>In this 2000 step run, there were 816 steps where the predator
        chooses a location which is <b>not</b> inside any prey disk.
        When these predator failures occur all three prey survive the
        tournament. Normally the predator's chosen prey is “eaten“,
        removed from the population, and replaced by an offspring of the
        other two prey in the tournament. In this run, the rate of
        predator failure is <b>41%</b> which is the lowest found so far
        on this background set. This may suggest, as hoped, that the
        adaptive predator is functioning better that the static
        pre-trained predator. Plotted below are the cumulative total of
        predator failures over 2000 simulation steps.<br>
        <br>
      </p>
      <img src="images/20220417_fail_plot.png" alt="predator failure
        plot" title="predator failure plot" class="img_flow"
        height="275" width="536"> </div>
    <div class="post" id="20220411"> <a href="#20220411" class="date">April










































































































































































        11, 2022</a>
      <h1>On the road to a
        dynamic/learning/specializing/fine-tuning/adapting predator<br>
      </h1>
      <p>Since the <a moz-do-not-send="true" href="#20220325">March 25</a>
        run I have been working to move from a “static” to “dynamic”
        predator vision model. In the previous approach, I pre-trained a
        convolutional neural net (CNN) for the generalized task of
        “finding conspicuous disks” — of camouflaged prey — overlaid on
        background images. This was encouragingly successful as an
        adversary (fitness function) to camouflage evolution. However
        since it had only a general ability to pick out prey, once the
        prey began evolving moderately effective camouflage, the
        predator's effectiveness began to wane. In nature, this would
        correspond to a predator getting less and less to eat, reducing
        its fitness to survive and reproduce.<br>
      </p>
      <p>The next step in this series of models is a dynamic predator: a
        predator which can dynamically <i>adapt</i> to its environment
        and ongoing prey camouflage evolution. The pre-trained predator
        has a <i>generalist</i> ability to find prey-like objects on
        arbitrary backgrounds. Ideally it should <i>specialize</i> for
        a given background set, and for the current population of
        camouflaged prey. My approach is to start from a pre-trained
        model (like <a moz-do-not-send="true" href="#20220322" style="">FCD6</a>)
        then allow the neural network to continue learning during
        simulation, based on <i>supervision</i> from the simulation's
        “ground truth.” This kind of additional training is often called
        <i>fine-tuning</i>, a type of <i>transfer learning</i>.<br>
      </p>
      <p>I have been building the plumbing and infrastructure for this.
        Last evening I got to a version that could at least run. The
        very first test quick test seemed promising, but to provide a
        better comparison I decided to use the same background set as
        the previous run. Almost immediately I could see it was
        performing poorly. By this morning it was clear my additions
        were (as Shrek said) “the opposite of helping.” At simulation
        step 1340, the count of predator failures was 982, for a failure
        rate of 73%. This is much worse than the static FCD6 which got
        fail rates around 47%.<br>
      </p>
      <p>So definitely not working as intended yet. Back to the drawing
        board!<br>
      </p>
    </div>
    <div class="post" id="20220325"> <a href="#20220325" class="date">March











































































































































































        25, 2022</a>
      <h1>FCD6 static predator, second camouflage run<br>
      </h1>
      <p>Another adversarial run using the FCD6 static predator versus
        camouflage evolved to be cryptic against the “tree leaf blossom
        sky” background set (<font size="-1">ID
          tree_leaf_blossom_sky_20220323_1457</font>). See <a
          moz-do-not-send="true" href="#20210315">March 15, 2021</a> for
        camouflage interactively evolved on this same background. <br>
      </p>
      <p> (As before: shown below, in chronological order, are 20 steps
        out of the 4000 total steps in this simulation. They were chosen
        by hand from 200 images saved every 20 steps. Each image
        contains a random portion of one background photo and three disk
        shaped “prey” each with its evolved texture. A black and white
        crosshair, surrounded by a dashed black and white circle, shows
        where the FCD6 static predator (see <a moz-do-not-send="true"
          href="#20220322">March 22</a>) decided a prey was probably
        located. Recall this model is purely 2d—there is no tree or
        sky—just color patterns on a 2d surface.)<br>
      </p>
      <img src="images/20220325_step_60.png" alt="20220325_step_60"
        title="20220325_step_60" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_180.png"
        alt="20220325_step_180" title="20220325_step_180"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_440.png" alt="20220325_step_440"
        title="20220325_step_440" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_560.png"
        alt="20220325_step_560" title="20220325_step_560"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_880.png" alt="20220325_step_880"
        title="20220325_step_880" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_1040.png"
        alt="20220325_step_1040" title="20220325_step_1040"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_1140.png" alt="20220325_step_1140"
        title="20220325_step_1140" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_1440.png"
        alt="20220325_step_1440" title="20220325_step_1440"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_1860.png" alt="20220325_step_1860"
        title="20220325_step_1860" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_1900.png"
        alt="20220325_step_1900" title="20220325_step_1900"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_2020.png" alt="20220325_step_2020"
        title="20220325_step_2020" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_2220.png"
        alt="20220325_step_2220" title="20220325_step_2220"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_2380.png" alt="20220325_step_2380"
        title="20220325_step_2380" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_2580.png"
        alt="20220325_step_2580" title="20220325_step_2580"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_2940.png" alt="20220325_step_2940"
        title="20220325_step_2940" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_3000.png"
        alt="20220325_step_3000" title="20220325_step_3000"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_3060.png" alt="20220325_step_3060"
        title="20220325_step_3060" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_3480.png"
        alt="20220325_step_3480" title="20220325_step_3480"
        class="img_flow" height="256" width="256"> <img
        src="images/20220325_step_3900.png" alt="20220325_step_3900"
        title="20220325_step_3900" class="img_flow" height="256"
        width="256"> <img src="images/20220325_step_4000.png"
        alt="20220325_step_4000" title="20220325_step_4000"
        class="img_flow" height="256" width="256">
      <p>Here are two more steps (before predator response) that I
        happened to notice and manually screen grab. They are near steps
        2700 and 3700. Five of the six prey seem well camouflaged. There
        is one poorly camouflaged prey near the top of the first image
        with greenish “confetti noise.” These static predators (like
        FCD6) seem surprisingly blind to this sort of high frequency
        noise pattern.<br>
      </p>
      <img src="images/20220325_step~2700.png" alt="20220325_step~2700"
        title="20220325_step~2700" class="img_flow" height="400"
        width="400"> <img src="images/20220325_step~3700.png"
        alt="20220325_step~3700" title="20220325_step~3700"
        class="img_flow" height="400" width="400">
      <p>Plot of cumulative “predator fails” versus steps of evolution
        time. Compare to similar chart, also for the FCD6 predator, in
        the <a moz-do-not-send="true" href="#20220323">March 23</a>
        entry. This run is 4000 steps, twice as long as that previous
        one. At 2000 steps the ratio of fails to steps is <b>47%</b>,
        at 4000 steps it is 60%. The gray dashed line is drawn just to
        indicate that the evolving population has settled into fooling
        the predator a roughly constant fraction of the steps. <br>
      </p>
      <p>This run can be seen as 4000 steps of which 2387 were skipped
        due to predator failure. Alternately it can be seen as a run of
        1613 steps with valid predation.<br>
      </p>
      <img src="images/20220325_predator_fails.png" alt="20220325
        predator_fails" title="20220325 predator_fails" class="img_flow"
        moz-do-not-send="true" height="399" width="748"> </div>
    <div class="post" id="20220323"> <a href="#20220323" class="date">March











































































































































































        23, 2022</a>
      <h1>FCD6 static predator with <i>oxalis sprouts</i> backgrounds</h1>
      <p>An adversarial camouflage evolution run (<font size="-1">ID
          oxalis_sprouts_20220322_1010</font>) using the new slightly
        improved (?) FCD6 static predator. There is some effective
        camouflage near the end of the ~2000 step run. I actually
        intended to continue it longer, but my machine had a “rapid
        unscheduled disassembly.” I'll do another.<br>
      </p>
      <img src="images/20220323_step_20.png" alt="20220323_step_20"
        title="20220323_step_20" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_60.png"
        alt="20220323_step_60" title="20220323_step_60" class="img_flow"
        height="256" width="256"> <img
        src="images/20220323_step_200.png" alt="20220323_step_200"
        title="20220323_step_200" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_420.png"
        alt="20220323_step_420" title="20220323_step_420"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_440.png" alt="20220323_step_440"
        title="20220323_step_440" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_580.png"
        alt="20220323_step_580" title="20220323_step_580"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_660.png" alt="20220323_step_660"
        title="20220323_step_660" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_800.png"
        alt="20220323_step_800" title="20220323_step_800"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_900.png" alt="20220323_step_900"
        title="20220323_step_900" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_1100.png"
        alt="20220323_step_1100" title="20220323_step_1100"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_1320.png" alt="20220323_step_1320"
        title="20220323_step_1320" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_1480.png"
        alt="20220323_step_1480" title="20220323_step_1480"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_1500.png" alt="20220323_step_1500"
        title="20220323_step_1500" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_1520.png"
        alt="20220323_step_1520" title="20220323_step_1520"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_1560.png" alt="20220323_step_1560"
        title="20220323_step_1560" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_1740.png"
        alt="20220323_step_1740" title="20220323_step_1740"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_1960.png" alt="20220323_step_1960"
        title="20220323_step_1960" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_1980.png"
        alt="20220323_step_1980" title="20220323_step_1980"
        class="img_flow" height="256" width="256"> <img
        src="images/20220323_step_2020.png" alt="20220323_step_2020"
        title="20220323_step_2020" class="img_flow" height="256"
        width="256"> <img src="images/20220323_step_2060.png"
        alt="20220323_step_2060" title="20220323_step_2060"
        class="img_flow" moz-do-not-send="true" height="256" width="256">
      <p>This is the predator-fails per step metric I've been tracking.
        At 2000 steps the ratio is <b>44%</b>. At 1000 steps it is 36%.
        In this run it looks like the rate of fails started high, slowed
        down for a while, then (around step 1100) sped up again. Another
        way to think about these fails is that, since they leave the
        evolving population unchanged, we could not count them as steps.
        Or we could define the number of “valid steps” as total steps
        attemped minus steps failed. So this is either a 2000 step run
        with 874 fails, or effectively a 1126 (2000 - 874) step run with
        no fails.</p>
      <img src="images/20220323_predator_fails.png" alt="20220323
        predator_fails" title="20220323 predator_fails" class="img_flow"
        moz-do-not-send="true" height="399" width="435"> </div>
    <div class="post" id="20220322"> <a href="#20220322" class="date">March











































































































































































        22, 2022</a>
      <h1>Training one more static predator: FCD6<br>
      </h1>
      <p>I wanted to try another go-around, training one more static
        predator for the “find conspicuous disk” task. (A reminder that,
        in this context, <i>static</i> means the predator's neural net
        does not change during camouflage evolution.) After four tries
        (below) I got well-behaved training with slightly better
        performance, but I suspect there is not much more to squeeze out
        of this model.<br>
      </p>
      <p>I used the same FCD5 dataset (see <a moz-do-not-send="true"
          href="#20220303">March 3</a>) but increased the level of <i>augmentation</i>
        applied during training. That dataset has 20,000 labeled
        examples, of which 16,000 (80%) are used as the training set.
        The training set is augmented during training by creating
        variations on the base data. As mentioned before, the random
        variations potentially include mirroring the images, rotating
        them by some multiple of 90°, and inverting the image brightness
        (e.g. black ↔︎ white). This time I added two other classes of
        variation: adjusting the <i>gamma</i> (alters
        contrast/exposure) and rotating the <i>hue</i> of colors in the
        image (e.g. red parts of the image become green, etc.). Both of
        these are more costly in terms of compute time, so I made they
        happen only occasionally: 5% and 10% respectively. Overall, the
        intent of augmentation is to promote generalization of the
        neural net model, by providing more variations for training.<br>
      </p>
      <p>This FCD6_rc4 model looks good to me in the first 3 plots
        below. The <i>loss</i> (error) metric, which actually drives
        learning, for the training set (blue) declines smoothly over 100
        training epochs. Measuring loss on the validation dataset
        (orange) has a nosier decline. But importantly, it does not
        later increase, which suggests <i>overfitting</i> the training
        dataset. Similarly the fraction-inside-disk metric (most
        relevant to FCD) increases smoothly for training data and
        noisily for validation. As seen in earlier runs, the model
        performs <b>better</b> (higher on fraction-inside-disk, lower
        on loss) on validation data than training data. My assumption is
        that strong anti-overfitting measures (via dropout, and
        augmentation, causing training data to change each epoch) means
        that the training data may be “harder” than the untouched
        validation. At the end of training, the fraction-inside-disk on
        validation dataset was ~<b>0.85</b>. In FCD6_rc4 the
        augmentation factor was increased from 16 to 32, meaning that
        each example is augmented with 31 variations on it. The
        augmented training set size was <b>512,000</b>.<br>
      </p>
      <img src="images/20220322_FCD6_rc4.png" alt="FCD6_rc4"
        title="FCD6_rc4" class="img_flow" height="224" width="903">
      <p>For rc3 I added another dropout layer in the output funnel of <i>dense</i>
        layers. I also reduce the dropout fraction from 0.5 to 0.2. It
        was no better than rc2.<br>
      </p>
      <img src="images/20220322_FCD6_rc3.png" alt="FCD6_rc3"
        title="FCD6_rc3" class="img_flow" height="224" width="903">
      <p>For rc2 I removed the final CNN layer in rc1 going back to CNN
        output being: 8 x 8 x 256. Removed the (months old) first 512
        size Dense layer so now the first is 128 size, move dropout
        layer after that. Trainable parameters now: 3,191,386.<br>
      </p>
      <img src="images/20220322_FCD6_rc2.png" alt="FCD6_rc2"
        title="FCD6_rc2" class="img_flow" height="224" width="903">
      <p>For this (rc1) model I added two more CNN layers at the end of
        the input funnel, with output size 8×8×256 and 4x4x512. This
        model has 9,548,890 trainable parameters, versus 17,118,042 in
        FCD5, and 16,912,858 before that. I have no idea what caused
        those huge drops in performance around epochs 75 and 90.<br>
      </p>
      <img src="images/20220322_FCD6_rc1.png" alt="FCD6_rc1"
        title="FCD6_rc1" class="img_flow" height="224" width="903"> </div>
    <div class="post" id="20220312"> <a href="#20220312" class="date">March











































































































































































        12, 2022</a>
      <h1>FCD5 static predator with “Huntington hedge” backgrounds<br>
      </h1>
      <p>Having reached a plateau of “sort of working” — using the FCD5
        static predator versus evolving camouflage — I have been trying
        different background sets. This uses the “Huntington hedge” set.
        Some details about that, and a human-in-the-loop camouflage run
        using it, are described in the entry for <a
          moz-do-not-send="true" href="#20210729">July 29, 2021</a>. The
        run below (ID <font size="-1">huntington_hedge_20220310_1231</font>)
        seemed to work pretty well, although I do not have an objective
        way to measure that. I let it run for 4000 steps, twice as long
        as normal. As before: each image shows a random portion of
        randomly selected background photo, overlaid with three
        disk-shaped “prey” each with an evolved texture. There is a
        black and white circle (with a b&amp;w cross in the center)
        indicating where the FCD5 predator thinks a prey is located.<br>
        <br>
      </p>
      <img src="images/20220312_step_20.png" alt="20220312_step_20"
        title="20220312_step_20" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_100.png"
        alt="20220312_step_100" title="20220312_step_100"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_200.png" alt="20220312_step_200"
        title="20220312_step_200" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_280.png"
        alt="20220312_step_280" title="20220312_step_280"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_400.png" alt="20220312_step_400"
        title="20220312_step_400" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_640.png"
        alt="20220312_step_640" title="20220312_step_640"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_920.png" alt="20220312_step_920"
        title="20220312_step_920" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_1040.png"
        alt="20220312_step_1040" title="20220312_step_1040"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_1140.png" alt="20220312_step_1140"
        title="20220312_step_1140" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_1480.png"
        alt="20220312_step_1480" title="20220312_step_1480"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_2300.png" alt="20220312_step_2300"
        title="20220312_step_2300" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_2520.png"
        alt="20220312_step_2520" title="20220312_step_2520"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_2700.png" alt="20220312_step_2700"
        title="20220312_step_2700" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_2860.png"
        alt="20220312_step_2860" title="20220312_step_2860"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_3240.png" alt="20220312_step_3240"
        title="20220312_step_3240" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_3400.png"
        alt="20220312_step_3400" title="20220312_step_3400"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_3660.png" alt="20220312_step_3660"
        title="20220312_step_3660" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_3840.png"
        alt="20220312_step_3840" title="20220312_step_3840"
        class="img_flow" height="256" width="256"> <img
        src="images/20220312_step_3920.png" alt="20220312_step_3920"
        title="20220312_step_3920" class="img_flow" height="256"
        width="256"> <img src="images/20220312_step_3960.png"
        alt="20220312_step_3960" title="20220312_step_3960"
        class="img_flow" height="256" width="256">
      <p>This is a plot of cumulative predator failures (vertical)
        versus steps (horizontal, effectively “evolution time”). The
        ratio of fails to steps is shown in red at the middle and end of
        the run, steps 2000 and 4000. As mentioned earlier, for a “human
        predator” in the interactive game-like simulation, this number
        is 0 or very close to it. But this <i>FCD5 static predator</i>
        (a deep neural net) regularly fails to find any prey. Over the
        entire run this ratio is <b>41%</b>. More typically I run these
        simulations for 2000 steps. At that point, the fail/step ratio
        was <b>32%</b>. In previous runs with this predator, the lowest
        value seen had been <b>47%</b>. I am hopeful that this new low
        value (32%) corresponds to the subjectively good quality of this
        run. That is, perhaps this ratio of failures to steps, for a
        benchmark predator model (here FCD5 static) can be used as a <i>metric</i>
        to access the quality of future simulation runs.<br>
        <br>
      </p>
      <img src="images/20220312_plot.png" alt="plot: predator failures
        over time" title="plot: predator failures over time"
        class="img_flow" moz-do-not-send="true" height="324" width="584">
    </div>
    <div class="post" id="20220308"> <a href="#20220308" class="date">March
























































































































































































        8, 2022</a>
      <h1>Partial run, new “predator fail” log</h1>
      <p>The previous run was long, this one was short. I wanted to make
        another adversarial camouflage evolution run using this new FCD5
        static predator. I chose another background image set that has
        not yet been used for camouflage. This one is called
        “bean_soup_mix” photographed from a package of mixed dried beans
        sold by our local grocery store. (Note <font size="-1">bean_soup_mix</font>,
        and <font size="-1">orange_pyracantha</font> used in the <a
          moz-do-not-send="true" href="#20220307">March 7</a> run, had
        been incorporated into the FCD5 dataset.) The run (ID: <font
          size="-1">bean_soup_mix_20220307_1826</font>) seemed to be
        going fine, until it got a transient filesystem error, which
        went unhandled, causing the simulation to crash. I hope I've
        learned a valuable lesson here. Maybe I'll put a condition
        handling retry loop in my code. The upshot is, this run is only
        1763 steps long. While 2000 is more typical, these simulations
        presumably continue to improve, but at slower and slower rates.<br>
      </p>
      <p>Shown below are 20 hand-selected steps recorded during the
        simulation. Each has a background image, three disks of evolved
        camouflage, representing prey. Each image also shows black and
        white crosshairs within a b&amp;w circle, indicating where the
        predator has “predicted” a prey is located.<br>
        <br>
      </p>
      <img src="images/20220308_step_0.png" alt="20220308_step_0"
        title="20220308_step_0" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_20.png"
        alt="20220308_step_20" title="20220308_step_20" class="img_flow"
        height="256" width="256"> <img
        src="images/20220308_step_60.png" alt="20220308_step_60"
        title="20220308_step_60" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_120.png"
        alt="20220308_step_120" title="20220308_step_120"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_200.png" alt="20220308_step_200"
        title="20220308_step_200" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_260.png"
        alt="20220308_step_260" title="20220308_step_260"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_280.png" alt="20220308_step_280"
        title="20220308_step_280" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_420.png"
        alt="20220308_step_420" title="20220308_step_420"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_480.png" alt="20220308_step_480"
        title="20220308_step_480" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_540.png"
        alt="20220308_step_540" title="20220308_step_540"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_700.png" alt="20220308_step_700"
        title="20220308_step_700" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_940.png"
        alt="20220308_step_940" title="20220308_step_940"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_1020.png" alt="20220308_step_1020"
        title="20220308_step_1020" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_1060.png"
        alt="20220308_step_1060" title="20220308_step_1060"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_1240.png" alt="20220308_step_1240"
        title="20220308_step_1240" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_1320.png"
        alt="20220308_step_1320" title="20220308_step_1320"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_1480.png" alt="20220308_step_1480"
        title="20220308_step_1480" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_1560.png"
        alt="20220308_step_1560" title="20220308_step_1560"
        class="img_flow" height="256" width="256"> <img
        src="images/20220308_step_1620.png" alt="20220308_step_1620"
        title="20220308_step_1620" class="img_flow" height="256"
        width="256"> <img src="images/20220308_step_1740.png"
        alt="20220308_step_1740" title="20220308_step_1740"
        class="img_flow" height="256" width="256">
      <p>I added a facility to keep track of <i>predator failures</i>
        as a time series, writing the total number of fails to a file
        every 10 simulation steps. A <i>predator failure</i> is the
        situation where the neural net model (representing the
        predator's vision) predicts that a prey is at a certain location
        in the image (black and white crosshair, above) but that
        location is <b>not</b> inside any of the three camouflaged prey
        disks. For example, the last two images above (steps 1620 and
        1740) are both predator fails, with the crosshairs near but
        outside a dark green prey. Before I was only measuring the
        predator fail rate at the end, as a percentage of total
        simulation steps. This run was cut short but the ratio was 48%
        (849 ÷ 1760). This is close to the 47% in the previous run. I
        suspect the shape of the plot is typical and represents the way
        a <i>static</i> predator model is moderately effective against
        the “random camouflage” at the beginning of a run, then becomes
        less and less effective as the prey population evolves better
        camouflage. Note that this static predator model was trained on
        the FCD5 dataset which is effectively the sort of “random
        camouflage” that is the initial state of a camouflage evolution
        run.<br>
        <br>
      </p>
      <img src="images/20220308_predator_fails_plot.png"
        alt="20220308_predator_fails_plot"
        title="20220308_predator_fails_plot" class="img_flow"
        height="512" width="512"> </div>
    <div class="post" id="20220307"> <a href="#20220307" class="date">March
























































































































































































        7, 2022</a>
      <h1>Camouflage evolution with FCD5 static predator<br>
      </h1>
      <p>I ran camouflage evolution against the <a
          moz-do-not-send="true" href="#20220304">March 4</a> static
        predator trained on dataset FCD5. I let it run for 4000 steps,
        twice the normal length of these simulations. This background
        image set is called “orange_pyracantha”. This is first time I've
        used it for camouflage. Seems a difficult environment, since a
        prey may find itself on bright orange berries, or green leaves,
        or in deep shadow. Evolving a pattern which is cryptic on all of
        those is a bit of a challenge. This run seems to have mostly
        converged toward deep red/orange patterns, broken up with
        “veins” of green, black, or white. The ID for this run is <font
          size="-1">orange_pyracantha_20220305_1146</font>.</p>
      <p>As with other runs in this series using a <i>static predator</i>
        — the prey population quickly evolves toward a “moderately”
        cryptic set of camouflage patterns — but then stops improving
        once it is able to frequently “fool” the predator. These cases
        of predator failure are indicated below where the black and
        white cross falls outside all of the prey disks. Shown below are
        20 hand curated examples selected from the 200 automatically
        recorded images out of 4000 total steps. (The whole set is
        available from me should anyone want to see them.) Additional
        notes:</p>
      <ul>
        <li>Recall that this is a purely a two dimensional model of
          camouflage. That pyracantha bush pictured in the background
          images is a highly convoluted 3d surface, which interacts with
          sunlight to produce highlights and shadows, then recorded in
          perspective. But for this discussion, we ignore that 3d
          structure and treat it as a flat, two dimensional color
          texture.<br>
        </li>
        <li>I still have not added code to record this information in
          detail, but out of 4000 simulation steps, the predator failed
          on 1874 steps (
          <meta http-equiv="content-type" content="text/html;
            charset=UTF-8">
          “invalid tournament: no prey selected”). In a sense this seems
          like and improvement when using the FCD5 version. In this run
          the predator fail rate is 47% (1874÷4000). In contrast, the
          February 28 run got 53% fail rate, and the February 24 run got
          a 57% fail rate. So 47% is the lowest predator fail rate so
          far. </li>
      </ul>
      <img src="images/20220307_step_20.png" alt="20220307_step_20"
        title="20220307_step_20" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_160.png"
        alt="20220307_step_160" title="20220307_step_160"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_240.png" alt="20220307_step_240"
        title="20220307_step_240" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_540.png"
        alt="20220307_step_540" title="20220307_step_540"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_820.png" alt="20220307_step_820"
        title="20220307_step_820" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_980.png"
        alt="20220307_step_980" title="20220307_step_980"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_1160.png" alt="20220307_step_1160"
        title="20220307_step_1160" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_1220.png"
        alt="20220307_step_1220" title="20220307_step_1220"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_1500.png" alt="20220307_step_1500"
        title="20220307_step_1500" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_1580.png"
        alt="20220307_step_1580" title="20220307_step_1580"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_2060.png" alt="20220307_step_2060"
        title="20220307_step_2060" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_2320.png"
        alt="20220307_step_2320" title="20220307_step_2320"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_2700.png" alt="20220307_step_2700"
        title="20220307_step_2700" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_2940.png"
        alt="20220307_step_2940" title="20220307_step_2940"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_3400.png" alt="20220307_step_3400"
        title="20220307_step_3400" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_3640.png"
        alt="20220307_step_3640" title="20220307_step_3640"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_3720.png" alt="20220307_step_3720"
        title="20220307_step_3720" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_3800.png"
        alt="20220307_step_3800" title="20220307_step_3800"
        class="img_flow" height="256" width="256"> <img
        src="images/20220307_step_3840.png" alt="20220307_step_3840"
        title="20220307_step_3840" class="img_flow" height="256"
        width="256"> <img src="images/20220307_step_3920.png"
        alt="20220307_step_3920" title="20220307_step_3920"
        class="img_flow" height="256" width="256"> </div>
    <div class="post" id="20220304"> <a href="#20220304" class="date">March









































































































































































































        4, 2022</a>
      <h1>Static predator trained on FCD5<br>
      </h1>
      <p>Using <a moz-do-not-send="true" href="#20220303">yesterday</a>'s











































































































































































































        FCD5 dataset with other “hyper-parameters” as on <a
          moz-do-not-send="true" href="#20220228">February 28</a>, I
        trained a new deep neural net model, named <font size="-1">20220304_1135_FCD5_a</font>.
        I was encouraged by the progress of this training run. It looks
        like FCD5 was a step in the right direction The validation loss
        closely tracks the training set loss until around epoch 40,
        after which validation loss stays near 0.01 while training loss
        continues to overfit. The “fraction inside disk” metric is most
        relevant to finding disks in the camouflage simulation. As seen
        in some earlier runs, early in training, this metric is actually
        <i>higher</i> for the validation set than the training set. That
        seems the opposite of what I expect, but I do use lots of
        dropout in my neural net architecture, and a lot of randomized
        augmentation in the training set. (The augmentation factor
        remains at 16, and FCD5 has 20,000 examples, split 80/20,
        leading to an effective training set size of
        <meta charset="utf-8">
        256,000.) At the end of the training, this “fraction inside
        disk” metric was at 84% on the validation set (middle chart,
        orange plot). I am looking forward to using this in an
        adversarial camouflage run tomorrow.<br>
      </p>
      <img src="images/20220304_plots.png" alt="20220304_plots.png"
        title="20220304_plots.png" class="img_flow"
        moz-do-not-send="true" height="224" width="891"> </div>
    <div class="post" id="20220303"> <a href="#20220303" class="date">March














































































































































































































        3, 2022</a>
      <h1>FCD5 dataset</h1>
      <p>My new hobby seems to be making synthetic datasets. As
        suggested on <a moz-do-not-send="true" href="#20220228">February















































































































































































































          28</a>, I generated yet another dataset—the fifth—related to
        my “find conspicuous disk” task. I am trying to engineer a
        dataset so that a neural net trained on becomes good at finding
        conspicuous disks. That is: I want a net that can find features
        of a certain size, which seem to contrast with the background
        texture, or otherwise seem out of place.) As mentioned before,
        this is a type of <i>saliency</i> detection. My longer term
        goal is to start from this sort of pre-trained DNN, and
        fine-tune (or specialize) it for a given background.<br>
      </p>
      <p>This fifth “find conspicuous disk” dataset is called FCD5. It
        is composed of 20,000 examples, each a 128×128 PNG image file.
        Each image is labeled (in its filename) with an <i>xy</i>
        position in pixel coordinates of the most conspicuous disk drawn
        in the image. FCD5 is evenly divided into three types of
        examples. My intuition is each of these corresponds to a “skill”
        I want the FCD static predator model to have. We will see. Some
        hand-selected examples from the dataset are shown below.</p>
      <p><b>Type 1:</b> images with a <b>single</b> random “prey” disk
        over a random background, as described on <a
          moz-do-not-send="true" href="#20211114">November 14, 2021</a>
        (“FCD”).<br>
        <br>
      </p>
      <img src="images/20220303_UVSfqCzewt_38_20.png"
        alt="20220303_UVSfqCzewt_38_20"
        title="20220303_UVSfqCzewt_38_20" class="img_flow" height="128"
        width="128"> <img src="images/20220303_SBWaLRHOzk_56_33.png"
        alt="20220303_SBWaLRHOzk_56_33"
        title="20220303_SBWaLRHOzk_56_33" class="img_flow" height="128"
        width="128"> <img src="images/20220303_bUMqcbutgJ_25_78.png"
        alt="20220303_bUMqcbutgJ_25_78"
        title="20220303_bUMqcbutgJ_25_78" class="img_flow" height="128"
        width="128"> <img src="images/20220303_HZzUzWWqcC_54_28.png"
        alt="20220303_HZzUzWWqcC_54_28"
        title="20220303_HZzUzWWqcC_54_28" class="img_flow" height="128"
        width="128"> <img src="images/20220303_inuPKUxnHQ_72_71.png"
        alt="20220303_inuPKUxnHQ_72_71"
        title="20220303_inuPKUxnHQ_72_71" class="img_flow" height="128"
        width="128"> <img src="images/20220303_RRGCwhmcJc_101_84.png"
        alt="20220303_RRGCwhmcJc_101_84"
        title="20220303_RRGCwhmcJc_101_84" class="img_flow" height="128"
        width="128"> <img src="images/20220303_PYinyJAWaj_61_60.png"
        alt="20220303_PYinyJAWaj_61_60"
        title="20220303_PYinyJAWaj_61_60" class="img_flow" height="128"
        width="128"> <img src="images/20220303_TNXfhQtzYa_92_91.png"
        alt="20220303_TNXfhQtzYa_92_91"
        title="20220303_TNXfhQtzYa_92_91" class="img_flow" height="128"
        width="128"> <img src="images/20220303_cDMtFaTYKk_63_54.png"
        alt="20220303_cDMtFaTYKk_63_54"
        title="20220303_cDMtFaTYKk_63_54" class="img_flow" height="128"
        width="128">
      <p><b>Type 2:</b> images like the “find 3 disks” (F3D) dataset
        described on <a moz-do-not-send="true" href="#20220129">January
          29</a>. These images each have <b>three</b> random disks over
        a random background. One corresponds to the label, and the other
        two are visually <i>muted</i> by blending into the background
        image.<br>
        <br>
      </p>
      <img src="images/20220303_wIRPERwSCh_49_63.png"
        alt="20220303_wIRPERwSCh_49_63"
        title="20220303_wIRPERwSCh_49_63" class="img_flow" height="128"
        width="128"> <img src="images/20220303_edDsCjbHdf_61_92.png"
        alt="20220303_edDsCjbHdf_61_92"
        title="20220303_edDsCjbHdf_61_92" class="img_flow" height="128"
        width="128"> <img src="images/20220303_fGMFBgMQDX_93_86.png"
        alt="20220303_fGMFBgMQDX_93_86"
        title="20220303_fGMFBgMQDX_93_86" class="img_flow" height="128"
        width="128"> <img src="images/20220303_jQREPLQyuL_33_39.png"
        alt="20220303_jQREPLQyuL_33_39"
        title="20220303_jQREPLQyuL_33_39" class="img_flow" height="128"
        width="128"> <img src="images/20220303_ijBOHTccYX_104_101.png"
        alt="20220303_ijBOHTccYX_104_101"
        title="20220303_ijBOHTccYX_104_101" class="img_flow"
        height="128" width="128"> <img
        src="images/20220303_KAoOFAqFyU_80_58.png"
        alt="20220303_KAoOFAqFyU_80_58"
        title="20220303_KAoOFAqFyU_80_58" class="img_flow" height="128"
        width="128"> <img src="images/20220303_NExMwxEbzU_85_92.png"
        alt="20220303_NExMwxEbzU_85_92"
        title="20220303_NExMwxEbzU_85_92" class="img_flow" height="128"
        width="128"> <img src="images/20220303_kpcUyhHXOh_91_98.png"
        alt="20220303_kpcUyhHXOh_91_98"
        title="20220303_kpcUyhHXOh_91_98" class="img_flow" height="128"
        width="128"> <img src="images/20220303_oWPwPGkcSb_82_22.png"
        alt="20220303_oWPwPGkcSb_82_22"
        title="20220303_oWPwPGkcSb_82_22" class="img_flow" height="128"
        width="128">
      <p><b>Type 3:</b> Images with <b>three copies</b> of a single
        random disk texture. They are drawn with three levels of
        opacity. The one at the labeled position is fully opaque, the
        other two are less and less opaque, making them more muted, less
        conspicuous.<br>
        <br>
      </p>
      <img src="images/20220303_uAEPxMZbeo_83_45.png"
        alt="20220303_uAEPxMZbeo_83_45"
        title="20220303_uAEPxMZbeo_83_45" class="img_flow" height="128"
        width="128"> <img src="images/20220303_cADfBauZUV_47_32.png"
        alt="20220303_cADfBauZUV_47_32"
        title="20220303_cADfBauZUV_47_32" class="img_flow" height="128"
        width="128"> <img src="images/20220303_YAMfudJxeH_30_84.png"
        alt="20220303_YAMfudJxeH_30_84"
        title="20220303_YAMfudJxeH_30_84" class="img_flow" height="128"
        width="128"> <img src="images/20220303_JeyBgDfMcN_40_82.png"
        alt="20220303_JeyBgDfMcN_40_82"
        title="20220303_JeyBgDfMcN_40_82" class="img_flow" height="128"
        width="128"> <img src="images/20220303_OaOJaByhbU_90_55.png"
        alt="20220303_OaOJaByhbU_90_55"
        title="20220303_OaOJaByhbU_90_55" class="img_flow" height="128"
        width="128"> <img src="images/20220303_mhYpDjxaKf_78_57.png"
        alt="20220303_mhYpDjxaKf_78_57"
        title="20220303_mhYpDjxaKf_78_57" class="img_flow" height="128"
        width="128"> <img src="images/20220303_ASsEgFUlly_23_60.png"
        alt="20220303_ASsEgFUlly_23_60"
        title="20220303_ASsEgFUlly_23_60" class="img_flow" height="128"
        width="128"> <img src="images/20220303_nzgItDrYqT_71_99.png"
        alt="20220303_nzgItDrYqT_71_99"
        title="20220303_nzgItDrYqT_71_99" class="img_flow" height="128"
        width="128"> <img src="images/20220303_QuHYtnPora_72_73.png"
        alt="20220303_QuHYtnPora_72_73"
        title="20220303_QuHYtnPora_72_73" class="img_flow" height="128"
        width="128"> </div>
    <div class="post" id="20220228"> <a href="#20220228" class="date">February


















































































































































































































        28, 2022</a>
      <h1>New dataset, new augmentations, new camo run (“F3D2”)<br>
      </h1>
      <p>Still fiddling with the various “hyper-parameters” of this
        static predator model based on finding the most conspicuous prey
        (textured disk) on a background. I first tried a synthetic
        training set with one disk over a background texture (called
        “FCD” for find conspicuous disk). I worried that it was confused
        by the camouflage images with three disks. Then I tried a couple
        of training sets where each example had three disks (“F3D”) with
        an effort to make two of them less conspicuous (by blending them
        into the background). This newest one is an effort to make these
        two “distractor” images somewhat more conspicuous. The
        difficulty is that only one of the three disks is labeled as the
        correct answer, so if the other two are <i>nearly</i> as
        conspicuous, and the DNN selects one, should that really count
        as a failure? (I named this newest set “F3D2” which feels too
        fussy. I think I should just call the whole range of variations
        “FCD” with a date.)<br>
      </p>
      <p>As before, the black and white dashed circle indicates the
        output of this new static predator model. It is the predator's
        best guess for where a prey is located—the <i>prediction</i> of
        the deep neural net. Ideally it would be choosing the most
        conspicuous prey, but any of them will do. I added a small black
        and white cross in the center of the circle to make clear that
        it is actually a <i>position</i> that the model is predicting.
        If that cross falls inside one of the camouflaged prey, then the
        predator succeeded, the prey is removed from its population, and
        replaced with a new prey, an offspring of the other two
        surviving prey. If the predator's prediction is not inside any
        prey (see for example the last two images below) then all three
        prey survive and the population is unchanged.<br>
      </p>
      <p>This newest dataset has 10,000 unique examples, the previous
        one had 20,000. I added new types of augmentation, variations
        created “on the fly” during training. Previously I used
        mirroring, rotation, and inverting the image brightness (like a
        photographic “negative”). In this run I add occasional
        adjustment of gamma and hue. I also made more of these modified
        training example, increasing from a factor of 8 to 16 in the
        number of variations on each precomputed example in the dataset.
        The new dataset used .png image format instead of .jpeg for the
        128×128 training images. At that resolution the file is only
        slightly bigger, and avoids JPEG compression artifacts, which
        became much more obvious when fiddling with hue and gamma.<br>
      </p>
      <p>Training a DNN static predator model from this new dataset less
        successful according to my metrics. Compare the training history
        plots at the bottom of this post with the “RC4” model on <a
          moz-do-not-send="true" href="#20220222">February 22</a>. This
        model got only 51% for its “in disk” rate on the validation set,
        whereas the previous model was 78%. (“In disk” on training set
        for new model was 82% before versus 88% previously.) But looking
        qualitatively at this run and the <a moz-do-not-send="true"
          href="#20220224">February 24</a> run, they seem pretty
        similar. In both cases the predator picked off many of the
        highly conspicuous prey (for example, that solid bright green
        one in step 0 below). Then as the prey population began to
        evolve, it got to a stage where many were camouflaged well
        enough to “fool” the predator. Of the 2000 steps in this
        simulation, 1054 of them got the “invalid tournament: no prey
        selected” flag. This suggests slightly <i>better</i> predator
        performance than in the previous run.<br>
      </p>
      <p>Perhaps I should next try another dataset with some mixture of
        training examples — varying the number of disks, and varying
        levels of “conspicuousness,” in each example.</p>
      <p>I started recording an image every 20 steps, producing 100
        images during this 2000 step run (<font size="-1">id:
          michaels_gravel_20220227_1654</font>). Of those I hand
        selected the 20 images below. I particularly like the leftmost
        prey in step 1120, and topmost in step 1960. Both are a bit too
        saturated, but to my eye, they seem to “melt” into the gravel
        background image. (In my browser if I hover over an image it
        displays floating text with the step number. They are: 0, 60,
        80, 180, 280, 320, 460, 560, 700, 800, 1000, 1120, 1460, 1560,
        1680, 1780, 1880, 1900, 1960, 2000.) </p>
      <img src="images/20220228_step_0.png" alt="20220228_step_0"
        title="20220228_step_0" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_60.png"
        alt="20220228_step_60" title="20220228_step_60" class="img_flow"
        height="256" width="256"> <img
        src="images/20220228_step_80.png" alt="20220228_step_80"
        title="20220228_step_80" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_180.png"
        alt="20220228_step_180" title="20220228_step_180"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_280.png" alt="20220228_step_280"
        title="20220228_step_280" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_320.png"
        alt="20220228_step_320" title="20220228_step_320"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_460.png" alt="20220228_step_460"
        title="20220228_step_460" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_560.png"
        alt="20220228_step_560" title="20220228_step_560"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_700.png" alt="20220228_step_700"
        title="20220228_step_700" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_800.png"
        alt="20220228_step_800" title="20220228_step_800"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_1000.png" alt="20220228_step_1000"
        title="20220228_step_1000" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_1120.png"
        alt="20220228_step_1120" title="20220228_step_1120"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_1460.png" alt="20220228_step_1460"
        title="20220228_step_1460" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_1560.png"
        alt="20220228_step_1560" title="20220228_step_1560"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_1680.png" alt="20220228_step_1680"
        title="20220228_step_1680" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_1780.png"
        alt="20220228_step_1780" title="20220228_step_1780"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_1880.png" alt="20220228_step_1880"
        title="20220228_step_1880" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_1900.png"
        alt="20220228_step_1900" title="20220228_step_1900"
        class="img_flow" height="256" width="256"> <img
        src="images/20220228_step_1960.png" alt="20220228_step_1960"
        title="20220228_step_1960" class="img_flow" height="256"
        width="256"> <img src="images/20220228_step_2000.png"
        alt="20220228_step_2000" title="20220228_step_2000"
        class="img_flow" height="256" width="256">
      <p>Training history for model <font size="-1">20220227_0746_F3D2_a</font>:<br>
      </p>
      <img src="images/20220228_plots.png" alt="20220228_step_2000"
        title="20220228_step_2000" class="img_flow" height=" 224"
        width="891"> </div>
    <div class="post" id="20220224"> <a href="#20220224" class="date">February


























































































































































































































        24, 2022</a>
      <h1>Another adversarial run with static/pre-trained predator<br>
      </h1>
      <p>Similar to other recent runs (<a moz-do-not-send="true"
          href="#20220209">February 9</a>, <a moz-do-not-send="true"
          href="#20220204">February 4</a>) where prey camouflage evolves
        against predation by a simple visual predator model which was
        pre-trained on the “find conspicuous disk” task. (This was the
        “RC4” model trained on <a moz-do-not-send="true"
          href="#20220222">February 22</a>.) The background set is
        oak_leaf_litter. Here I automatically captured images every 100
        steps out of total of 2000 steps. I may want to increase that
        rate in the future. It looks like by step 400, most of the
        evolutionary change has already happened. By that point, the
        average evolved camouflage seems to be good enough to “fool” the
        predator, so prey can survive without further change.<br>
      </p>
      <p>These images are rendered by TexSyn at 256×256 for better
        visual quality, although they get down-scaled for use by the
        predator DNN (deep neural net) which continues to expect input
        at 128×128. As before, the circle drawn as a black and white
        dashed line indicates the output of the DNN. This is its guess
        (“prediction”) for where a prey is most likely located. If the
        center of that circle is inside one of the three camouflaged
        prey disks, then the predator has successfully found and “eats”
        the prey. (Which is then replaced in the evolving prey
        population based on crossover between the other two prey seen in
        that tournament group.) If the center of the b&amp;w circle is
        outside all prey, then all three prey survive. (And the
        “predator goes hungry” although that has no impact in this
        version of the simulation.)<br>
      </p>
      <img src="images/20220224_step_0.png" alt="20220224_step_0.png"
        title="20220224_step_0.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_100.png"
        alt="20220224_step_100.png" title="20220224_step_100.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_200.png" alt="20220224_step_200.png"
        title="20220224_step_200.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_300.png"
        alt="20220224_step_300.png" title="20220224_step_300.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_400.png" alt="20220224_step_400.png"
        title="20220224_step_400.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_500.png"
        alt="20220224_step_500.png" title="20220224_step_500.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_600.png" alt="20220224_step_600.png"
        title="20220224_step_600.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_700.png"
        alt="20220224_step_700.png" title="20220224_step_700.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_800.png" alt="20220224_step_800.png"
        title="20220224_step_800.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_900.png"
        alt="20220224_step_900.png" title="20220224_step_900.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_1000.png" alt="20220224_step_1000.png"
        title="20220224_step_1000.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_1100.png"
        alt="20220224_step_1100.png" title="20220224_step_1100.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_1200.png" alt="20220224_step_1200.png"
        title="20220224_step_1200.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_1300.png"
        alt="20220224_step_1300.png" title="20220224_step_1300.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_1400.png" alt="20220224_step_1400.png"
        title="20220224_step_1400.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_1500.png"
        alt="20220224_step_1500.png" title="20220224_step_1500.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_1600.png" alt="20220224_step_1600.png"
        title="20220224_step_1600.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_1700.png"
        alt="20220224_step_1700.png" title="20220224_step_1700.png"
        class="img_flow" height="256" width="256"> <img
        src="images/20220224_step_1800.png" alt="20220224_step_1800.png"
        title="20220224_step_1800.png" class="img_flow" height="256"
        width="256"> <img src="images/20220224_step_1900.png"
        alt="20220224_step_1900.png" title="20220224_step_1900.png"
        class="img_flow" height="256" width="256">
      <p>Here is an interesting factoid: of the 2000 steps in this
        simulation, 1143 were marked in the log as “invalid tournament:
        no prey selected” meaning the result from the deep neural net
        completely missed all three prey 57% of the time. I will see
        about storing this as time series information for easier
        plotting, but generally there were fewer at the beginning of the
        run and more at the end of the run.<br>
        <br>
      </p>
    </div>
    <div class="post" id="20220222"> <a href="#20220222" class="date">February


























































































































































































































        22, 2022</a>
      <h1>Oy, RC4</h1>
      <p>I discovered that yesterday's (“F3D augmented”) training run
        had a bug which made the on-the-fly augmentation have less
        variation than intended. I fixed that, improved the
        visualization of the training set so I would have seen that
        problem, and cleaned up the Jupyter notebook
        (Find_3_Disks.ipynb). I trained a new predator disk-finder model
        (<font size="-2">id: 20220222_1747_F3D_augmented_rc4</font>).
        This model is again a slight improvement with the “in disk” rate
        for the validation set. It ended at 76-78% versus yesterday’s
        ~70%.</p>
      <img src="images/20220222_plots.png" alt="20220221_1113 RC4
        training history" title="20220221_1113 RC4 training history"
        class="img_flow" height="211" width="828"> </div>
    <div class="post" id="20220221"> <a href="#20220221" class="date">February









































































































































































































































        21, 2022</a>
      <h1>Pre-trained predator “conspicuous disk finder” RC3</h1>
      <p>I fixed some issues with the pre-trained predator “conspicuous
        disk finder.” This made a relatively small improvement on its
        ability to find conspicuous disks. The “RC3” tag is just a joke
        about commercial software release procedures where there are
        several “release candidates” before one meets all the release
        tests.<br>
      </p>
      <p>This deep neural net model (<font size="-2">id:
          20220221_1113_F3D_augmented_rc3</font>) was trained with four
        times more <i>augmentation</i> of the training set. This run
        was trained on the 20,000 example F3D dataset, split into 16,000
        training and 4,000 validation examples. The training set was
        augmented on-the-fly by a factor of 8, for an effective size of
        128,000. This is done so that each training batch contains some
        of the original image/location training examples, and some of
        the variations made by augmentation: rotations, mirroring, and
        white-for-black brightness flipping.<br>
      </p>
      <p>This uses a new deep neural net architecture with different CNN
        layers at the front. For a couple of months I’ve been using a
        model with 2 “units” each of 3 CNNs, and down-sampling by stride
        of 2. Now there are four CNN layers, each using 5x5 filters,
        with the number of filters doubling each layer (16, 32, 64,
        128), and all but the first layer is down-sampling by stride of
        2. The dense layers at the end (that boil the data down to an xy
        position) are unchanged from before. The previous architecture
        had 16,912,858 trainable parameters, this one is slightly larger
        at 17,118,042. Yesterday’s training took about 114 seconds per
        epoch. This one took about 70 seconds per epoch.<br>
      </p>
      <p>In the images below, as before, the predator <i> succeeds</i>
        if the black and white circle (the <i>prediction</i> of the
        neural net) surrounds the crosshair center (the <i>label</i>
        for that training example).<br>
      </p>
      <img src="images/20220221_a1.png" alt="20220221_1113 RC3
        validation" title="20220221_1113 RC3 validation"
        class="img_flow" height="250" width="250"> <img
        src="images/20220221_a2.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a3.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a4.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a5.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a6.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a7.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a8.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a9.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a10.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a11.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a12.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a13.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a14.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a15.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a16.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a17.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a18.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250"> <img src="images/20220221_a19.png"
        alt="20220221_1113 RC3 validation" title="20220221_1113 RC3
        validation" class="img_flow" height="250" width="250"> <img
        src="images/20220221_a20.png" alt="20220221_1113 RC3 validation"
        title="20220221_1113 RC3 validation" class="img_flow"
        height="250" width="250">
      <p>In the random selection above, the predator has succeeded only
        11 of 20 times. While the validation “fraction inside disk”
        metric crawled above 70%, it is only slightly better than
        previous runs. The&nbsp; “fraction inside disk” metric for the
        training set reached 95% presumably due to overfitting. I may
        still need to tweak the function that makes random variations on
        a given image.<br>
        <br>
      </p>
      <img src="images/20220221_plots.png" alt="20220221_1113 RC3
        training history" title="20220221_1113 RC3 training history"
        class="img_flow" height="211" width="828"> </div>
    <div class="post" id="20220209"> <a href="#20220209" class="date">February










































































































































































































































        9, 2022</a>
      <h1>Another adversarial run, this time with fewer bugs</h1>
      <p>This is essentially the same as the previous run: camouflage
        evolution (using TexSyn and LazyPredator) for prey versus a
        pre-trained F3D-complex model for predator vision.The two are
        combining with my prototype adversarial framework. I fixed the
        background scaling issue mentioned on <a moz-do-not-send="true"
          href="#20220204">February 4</a>, and another bug where the
        image-normalized xy coordinates from the DNN model were not
        being scaled back to pixels. (I <i>was</i> drawing those white
        circles in the correct place, just not passing on that scaled
        value.) As a result, every “predation” attempt was a miss, so it
        was effectively evolution with random selection.<br>
      </p>
      <p>To recap: in this model, the “predator” is presented with a
        background image overlaid with three camouflaged disk-shaped
        “prey.” The predator's job is to locate any of the three prey by
        selecting a position inside one of them. Think of this as a bird
        pecking at a spot on a tree trunk where it suspects a tasty but
        well-camouflaged beetle may be located. In the human-in-loop
        version of this simulation, while it sometimes takes bit of
        visual search, I have never been so bamboozled by camouflage
        quality that I could not locate all three, then choose the most
        conspicuous one to click. On the other hand, my newly trained
        DNN (deep neural net) predators frequently “miss” and so select
        none of the prey. Previously in LazyPredator this would have led
        to one of the three prey arbitrarily being declared the loser,
        and replaced in the population. This seems unfair since all
        three prey successfully “fooled” the predator. Now when this
        happens all three prey survive. The details are described in
        this <a moz-do-not-send="true"
          href="https://cwreynolds.github.io/LazyPredator/#20220209">LazyPredator













































































































































































































































































          blog post</a>.<br>
      </p>
      <p>With those fixes, I made a new run using the <code>maple_leaf_litter</code>
        background set. I was aiming at a run of 2000 steps, but my
        “predator server” running on Colab timed out. Since the predator
        side is stateless, I added some simple code to allow restarting
        the Colab side mid-run. I stopped the run at step 2142. I am
        still recording these images by hand, at irregular intervals. <br>
      </p>
      <p>The primary observation is that the F3D-complex predator model
        does well at the beginning of the run when the camouflage is
        essentially random and of poor quality. (The model is quite
        accurate at picking out “uniform” (single color) prey against a
        complex background.) Before long, the camouflage begins to
        improve, and soon seems able to fool this simple predator model.
        By the end of the run, most of the prey were surviving while the
        hapless predator frequently chose locations which cleanly missed
        all three prey. As a result the prey live long happy lives and
        the population only slowly drifts. These images are in time
        sequence order, from the upper left, row by row, to lower right.
        To me it seems the earlier ones are more conspicuous and the
        later ones are more cryptic. But I suppose that is a subjective
        judgement.<br>
      </p>
      <img src="images/20220209_2nd_adversarial.jpg" alt="Second
        adversarial camouflage run" title="Second adversarial camouflage
        run" class="img_flow" height="747" width="931"> </div>
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20220204"> <a href="#20220204" class="date">February













































































































































































































































































        4, 2022</a>
      <h1>First “no human in loop” adversarial camouflage run</h1>
      <p>This is my first adversarial camouflage run without a
        human-in-the-loop. After training the F3D-complex model on <a
          moz-do-not-send="true" href="#20220202">February 2</a>, I used
        my prototype predator/prey framework to evolve camouflage with
        TexSyn and LazyPredator, using a fitness function based on
        “predator vision” with the F3D-complex model. Initially I was
        seeing the same failure mode seen December 28 (noted at end of <a
          moz-do-not-send="true" href="#20211215">December 15</a>'s
        post). I dug into the code and found that pixel data read from
        the training set was not being remapped from a range of [0, 255]
        to [0, 1] as expected by the neural net. Once that was fixed,
        the system began to work correctly.<br>
      </p>
      <p>It ran for 1664 steps before my Colab instance timed out after
        about 20 hours on this simulation. The images below were just
        screen-grabbed by hand, at the beginning and end of the run. (It
        ran overnight so I was asleep during the middle of the run.) In
        each case you can see a bit of the <code>oak_leaf_litter</code>
        background set. These are incorrectly zoomed in by a factor of
        about 8, another bug to be fixed soon. Each image is overlaid by
        three disks of synthetic camouflage texture. The white circle
        indicates the output of the “predator vision” neural net. Many
        of these are near the position of one of the three disks. Often
        the selected disk is arguably the most conspicuous disk. This
        prototype run has several problems which I hope to fix soon.<br>
      </p>
      <p>This simulation architecture has been a goal of mine since
        about 2008 (<a moz-do-not-send="true"
          href="https://www.red3d.com/cwr/coc/1sheet.pdf">description</a>).















































































































































































































































































        I had a vague conception of it in 1994 which lead to a “stepping
        stone” project on adversarial learning: <a
          moz-do-not-send="true"
          href="https://www.red3d.com/cwr/papers/1994/alife4.html">Competition,














































































































































































































































































          Coevolution and the Game of Tag</a>. In 2010 I built the
        camouflage evolution part with visual predation via a
        human-in-the-loop: <a moz-do-not-send="true"
          href="https://www.red3d.com/cwr/iec/">Interactive Evolution of
          Camouflage</a>. These prototype results are low quality, but I
        am glad after all this time to see it finally run.<br>
      </p>
      <img src="images/20220204_1st_adversarial.png" alt="First
        adversarial camouflage run" title="First adversarial camouflage
        run" class="img_flow" height="407" width="1007"> </div>
    <div class="post" id="20220202"> <a href="#20220202" class="date">February













































































































































































































































































        2, 2022</a>
      <h1>F3D “complex” model working moderately well<br>
      </h1>
      <p>I took the 20,000 “find 3 disks” training examples generated by
        TexSyn and <i>augmented</i> the dataset by a factor of two.
        That is, I made one modified version of each precomputed
        example. The modifications include rotations (by multiples of
        90°), mirroring, and inverting the brightness (white↔︎black). I
        can easily create more this same way, but I am beginning to bump
        up against the memory size of the Google Colab instance I'm
        running on. So for now, my dataset is size 40,000, or which 20%
        are held out for validation. Training this model took 49 minutes
        on a Colab GPU instance.<br>
      </p>
      <p>The annotation shown below is somewhat different. The black and
        white crosshair marks are now used specifically to indicate the
        <b>label</b> (relative xy coordinate within image) for a given
        training example (image). In this case (F3D) that means that
        (what I assert is) the most conspicuous disk's centerpoint is at
        the intersection of the two crosshair lines. A circle, drawn
        with a black and white dashed line, is used to indicate the xy <b>prediction</b>
        made by the deep neural net model after training. So the
        crosshairs show what I think is the correct answer, and the
        circle is what the model “guesses” is the answer.<br>
      </p>
      <p>A <i>successful detection of the most conspicuous disk</i> is
        indicated by the crosshair intersection falling <b>inside</b>
        the circle. In the 10 examples below, numbers 1, 2, 5, 6, and 10
        are successful.</p>
      <img src="images/20220202_1.png" alt="20220202_1.png"
        title="20220202_1.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_2.png" alt="20220202_2.png"
        title="20220202_2.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_3.png" alt="20220202_3.png"
        title="20220202_3.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_4.png" alt="20220202_4.png"
        title="20220202_4.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_5.png" alt="20220202_5.png"
        title="20220202_5.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_6.png" alt="20220202_6.png"
        title="20220202_6.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_7.png" alt="20220202_7.png"
        title="20220202_7.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_8.png" alt="20220202_8.png"
        title="20220202_8.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_9.png" alt="20220202_9.png"
        title="20220202_9.png" class="img_flow" height="250" width="250">
      <img src="images/20220202_10.png" alt="20220202_10.png"
        title="20220202_10.png" class="img_flow" height="250"
        width="250">
      <p>This success rate (5 out of 10) is slightly worse than the
        average performance of this model. It got 67% of its predictions
        “inside disk” on the validation set. (It got 82% on the training
        set, presumably due to overfitting.) The plots below show
        training metrics over 100 epochs.</p>
      <img src="images/20220202_training_plots.png"
        alt="20220202_training_plots" title="20220202_training_plots"
        class="img_flow" height="227" width="908"> </div>
    <div class="post" id="20220130"> <a href="#20220130" class="date">January
























































































































































































































































































        30, 2022</a>
      <h1>F3D shows promise</h1>
      <p>I hooked up the plumbing so I could try learning against the
        new “Find 3 Disks” training set. This is preliminary, does not
        include any training set “augmentation” (including rotations,
        mirrorings, etc.), nor any tuning. It is encouraging that it is
        not a complete bust. The middle “fraction inside disk” metric
        only gets to about 0.5 for the training data, even lower for the
        validation data.<br>
      </p>
      <p>I also fixed a problem with these plots. I was getting a very
        high loss value on the first epoch of training, about 20× the
        value on the next epoch. This caused the plot's auto-scaling to
        push all the interesting data to the bottom. (See e.g. the plot
        at the end of <a moz-do-not-send="true" href="#20220109">January

























































































































































































































































































          9</a>.) Here I simply skip the first epoch in my plots:<br>
      </p>
      <img src="images/20220130_1st_F3D_complex.png" alt="Preliminary
        Find_3_Disks complex run." title="Preliminary Find_3_Disks
        complex run." class="img_flow" height="250" width="919"> </div>
    <div class="post" id="20220129"> <a href="#20220129" class="date">January
























































































































































































































































































        29, 2022</a>
      <h1>“Find 3 disks” training set</h1>
      <p>My <a moz-do-not-send="true" href="#20220109">January 9</a>
        post suggested the next step would be to train a model to find
        the “most conspicuous <b>of 3</b> disks” — while using images
        of natural complexity. This is a peek at a prototype training
        set I generated. It is based on combinations of photographic
        textures and synthetic textures from TexSyn (as in the earlier
        “find the (single) conspicuous disk” task). It also uses a fixed
        strategy to reduce the “conspicuousness” for two of three disks
        in each image. The idea is that the <i>label</i> for each image
        (training example) is the centerpoint of the (intended) most
        conspicuous disk. In addition there are two other disks
        (“distractors”) which are modified to (I hope) make them less
        conspicuous. Here are eight images from the training set of 5000
        procedurally generated images, rendered at their “postage stamp”
        resolution of 128×128:<br>
      </p>
      <img src="images/20220129_AdDgxpNgsv_24_28.jpeg"
        alt="20220129_AdDgxpNgsv_24_28.jpeg"
        title="20220129_AdDgxpNgsv_24_28.jpeg" class="img_flow"
        height="128" width="128"> <img
        src="images/20220129_aaDdTrMueQ_57_107.jpeg"
        alt="20220129_aaDdTrMueQ_57_107.jpeg"
        title="20220129_aaDdTrMueQ_57_107.jpeg" class="img_flow"
        height="128" width="128"> <img
        src="images/20220129_AaDKnxXRZz_101_108.jpeg"
        alt="20220129_AaDKnxXRZz_101_108.jpeg"
        title="20220129_AaDKnxXRZz_101_108.jpeg" class="img_flow"
        height="128" width="128"> <img
        src="images/20220129_AAHdRCSQGw_101_74.jpeg"
        alt="20220129_AAHdRCSQGw_101_74.jpeg"
        title="20220129_AAHdRCSQGw_101_74.jpeg" class="img_flow"
        height="128" width="128"> <img
        src="images/20220129_AAQDuqBUbU_47_72.jpeg"
        alt="20220129_AAQDuqBUbU_47_72.jpeg"
        title="20220129_AAQDuqBUbU_47_72.jpeg" class="img_flow"
        height="128" width="128"> <img
        src="images/20220129_AaYpwOaiHS_42_41.jpeg"
        alt="20220129_AaYpwOaiHS_42_41.jpeg"
        title="20220129_AaYpwOaiHS_42_41.jpeg" class="img_flow"
        height="128" width="128"> <img
        src="images/20220129_abNrfjPncl_81_20.jpeg"
        alt="20220129_abNrfjPncl_81_20.jpeg"
        title="20220129_abNrfjPncl_81_20.jpeg" class="img_flow"
        height="128" width="128"> <img
        src="images/20220129_aBpiDUdcQn_83_82.jpeg"
        alt="20220129_aBpiDUdcQn_83_82.jpeg"
        title="20220129_aBpiDUdcQn_83_82.jpeg" class="img_flow"
        height="128" width="128">
      <p>Here are some generated by the same model, rendered at 512×512
        for the benefit of my elderly eyes. As before (see <a
          moz-do-not-send="true" href="#20211114">November 14</a>) 60%
        of the time the background image is a (randomly selected and
        cropped) photograph and 40% of the time a synthetic texture.
        Conversely 60% of the disk textures are synthetic and 40% are
        photographic. The “algorithmic conspicuousness reduction”
        techniques used here are (a) blending the disk texture into the
        background at 50% opacity, and (b) “confetti noise” blending:
        multiply the soft matte signal by a random number on [0, 1] at
        each pixel of texture output. The goal is to have one <i>most</i>
        conspicuous disk, and two others which are <i>less</i>
        conspicuous, each in their own way. Looking over the images, I
        think this “often” works but not always. I think this
        “conspicuousness reduction” technique will be the key to making
        this approach work, and I may well have to adjust it in
        subsequent versions of this training set.<br>
      </p>
      <img src="images/20220129_REvCiYkSHv_93_255.jpeg"
        alt="20220129_REvCiYkSHv_93_255.jpeg"
        title="20220129_REvCiYkSHv_93_255.jpeg" class="img_flow"
        height="512" width="512"> <img
        src="images/20220129_avAhvuQMeW_234_148.jpeg"
        alt="20220129_avAhvuQMeW_234_148.jpeg"
        title="20220129_avAhvuQMeW_234_148.jpeg" class="img_flow"
        height="512" width="512"> <img
        src="images/20220129_hzOgdbwNVN_77_290.jpeg"
        alt="20220129_hzOgdbwNVN_77_290.jpeg"
        title="20220129_hzOgdbwNVN_77_290.jpeg" class="img_flow"
        height="512" width="512"> <img
        src="images/20220129_lBASrXYpXc_249_352.jpeg"
        alt="20220129_lBASrXYpXc_249_352.jpeg"
        title="20220129_lBASrXYpXc_249_352.jpeg" class="img_flow"
        height="512" width="512"> <img
        src="images/20220129_LgCbDpyvSb_342_377.jpeg"
        alt="20220129_LgCbDpyvSb_342_377.jpeg"
        title="20220129_LgCbDpyvSb_342_377.jpeg" class="img_flow"
        height="512" width="512"> <img
        src="images/20220129_mRAohowqIP_379_291.jpeg"
        alt="20220129_mRAohowqIP_379_291.jpeg"
        title="20220129_mRAohowqIP_379_291.jpeg" class="img_flow"
        height="512" width="512"> </div>
    <div class="post" id="20220109"> <a href="#20220109" class="date">January




























































































































































































































































































        9, 2022</a>
      <h1>Simple control of “conspicuous-ness”<br>
      </h1>
      <p>At the end of <a moz-do-not-send="true" href="#20211215">December




























































































































































































































































































          15</a>'s post, I mentioned I wanted to train a vision model
        that would (a) find conspicuous disks, (b) be unfazed by the
        presence of additional such disks, and (c) correctly choose the
        most conspicuous one out of several. (For convenience, I set
        “several” to 3.) Starting with a super-simplified case, I took
        the approach of procedurally generating a training set with
        (what I hoped would be) a controllable degree of
        “conspicuous-ness.” Using solid uniform color textures, I choose
        a random foreground and background color. Over the background I
        draw three non-overlapping, soft-edged disks: one with the
        foreground color and the other two increasingly blended into the
        background color. The label for each image is the centerpoint of
        the full foreground color disk, as (x, y) relative to the image
        bounds.<br>
      </p>
      <p>Then I used the same deep convolution neural net architecture
        as in the <a moz-do-not-send="true" href="#20211215">December
          15</a> post, training it on 5000 randomly generated examples
        of this uniform texture dataset, in a notebook called <code>Find_3_Disks</code>.
        Below are 10 random examples selected from the test (validation)
        set, with the model's predictions shown as black and white
        crosshairs. Very often the model correctly identifies the
        intended “most conspicuous” disk (for this contrived dataset)
        predicting a centerpoint which is at least inside the intended
        disk.<br>
      </p>
      <img src="images/20220109_a.png" alt="20220109_a.png"
        title="20220109_a.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_b.png" alt="20220109_b.png"
        title="20220109_b.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_c.png" alt="20220109_c.png"
        title="20220109_c.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_d.png" alt="20220109_d.png"
        title="20220109_d.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_e.png" alt="20220109_e.png"
        title="20220109_e.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_f.png" alt="20220109_f.png"
        title="20220109_f.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_g.png" alt="20220109_g.png"
        title="20220109_g.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_h.png" alt="20220109_h.png"
        title="20220109_h.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_i.png" alt="20220109_i.png"
        title="20220109_i.png" class="img_flow" height="250" width="250">
      <img src="images/20220109_j.png" alt="20220109_j.png"
        title="20220109_j.png" class="img_flow" height="250" width="250">
      <p>An interesting aspect of this run, unlike my previous limited
        experience: it made no apparent progress for the first ~60 of
        100 epochs in the run. Then suddenly it had a “eureka!” moment
        and rapidly organizes into an effective network. Maybe this
        corresponds to aimlessly wondering around on a nearly flat
        plateau in “loss space”?<br>
      </p>
      <img src="images/20220109_plots.png" alt="20220109_plots.png"
        title="20220109_plots.png" class="img_flow" height="247"
        width="970">
      <p>My next goal will be to take more general textures—photographs
        and TexSyn textures—and come up with a way to procedurally
        reduce their “conspicuous-ness” to do a version of this <code>Find_3_Disks</code>
        with more realistic image complexity.<br>
        <br>
      </p>
    </div>
    <div class="post" id="20211215"> <a href="#20211215" class="date">December































































































































































































































































































        15, 2021</a>
      <h1>Conspicuous disks found, but model was brittle<br>
      </h1>
      <p>Starting from the dataset described on <a
          moz-do-not-send="true" href="#20211114">November 14</a> (plus
        my newly acquired beginner's knowledge of deep learning, the
        excellent <a moz-do-not-send="true"
          href="https://research.google.com/colaboratory/faq.html">Colab</a>
        facility provided by Google for prototyping deep learning in the
        cloud, plus <a moz-do-not-send="true"
          href="https://www.python.org">Python</a>, <a
          moz-do-not-send="true" href="https://jupyter.org">Jupyter</a>,
        <a moz-do-not-send="true" href="https://keras.io">Keras</a> and
        <a moz-do-not-send="true" href="https://www.tensorflow.org">TensorFlow</a>,
        all new to me) I manged to cobble together the prototype “find
        conspicuous disk” model I was aiming for. It started working
        around December 15. This blog post is updated with later
        insights, and graphics saved during a run made on January 5,
        2022.<br>
      </p>
      <p>Previously I procedurally generated a dataset of 2000 examples,
        each with a background image (either photographic or synthetic)
        and an overlaid “disk” of another texture. Each training example
        included a “ground truth” label indicating where the disk's
        centerpoint was placed within the larger image. Eventually I
        added another 3000 such labeled examples. I “augmented” this
        dataset (in memory) by rotating and mirroring for a total of
        40,000 images. (I also tried inverting the brightness of the
        images but that seemed to cause difficulties. I disabled that
        and will return to it later.) The original images were 1024×1024
        RGB pixels. For now, I scaled those down to 128×128. Using the
        Keras API, I created a <i>deep neural net</i> of about 20
        sequential layers. It takes a 128×128 RGB image as input. As
        output it produces two floats representing its <i>prediction</i>
        of the disk's centerpoint. I split the dataset into <i>training</i>
        and <i>validation</i> sets, then trained the model for 100
        epochs. These plots summarize the training, with epochs
        (training time) running horizontally, versus some metric shown
        vertically, for both the training and validation sets. The <i>loss</i>
        function was a typical <i>mean standard error</i> (MSE). <i>Accuracy</i>
        is a standard statistical measure of prediction quality.
        “Fraction inside disk” is how often the predicted point was
        within the disk's boundary. (All graphics in this post are cut
        and pasted as drawn by Python's <code>matplotlib.pyplot</code>
        in the Jupyter notebook on Colab.)<br>
      </p>
      <img src="images/20220105_plots.png" alt="20220105_plots.png"
        title="20220105_plots.png" class="img_flow" height="253"
        width="1010">
      <p>After training I visualized the performance by picking 20
        images at random from the validation set, used the model to
        predict disk position, then drew “crosshairs” at that position.
        Images for the <code>20220105_1136</code> run are shown below.
        In most cases, the model predicted a location at or near the
        disk center, or at worst, somewhere within the disk's boundary.
        The final three images are clearly wrong: the disk is located
        more than a radius away from the crosshairs. The fourth from the
        last is hard to call. It looks like the same photo was used for
        both the background and the disk. So it is accidentally
        “cryptic” — well camouflaged. So I am not sure if the crosshairs
        correctly indicate the disk, or if it is somewhere else. At
        128×128 resolution it is very hard to tell, even for my brain's
        neural net. Note that this error rate roughly lines up with the
        “fraction inside disk” metric.<br>
      </p>
      <img src="images/20220105_a.png" alt="20220105_a.png"
        title="20220105_a.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_b.png" alt="20220105_b.png"
        title="20220105_b.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_c.png" alt="20220105_c.png"
        title="20220105_c.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_d.png" alt="20220105_d.png"
        title="20220105_d.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_e.png" alt="20220105_e.png"
        title="20220105_e.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_f.png" alt="20220105_f.png"
        title="20220105_f.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_g.png" alt="20220105_g.png"
        title="20220105_g.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_h.png" alt="20220105_h.png"
        title="20220105_h.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_i.png" alt="20220105_i.png"
        title="20220105_i.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_j.png" alt="20220105_j.png"
        title="20220105_j.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_k.png" alt="20220105_k.png"
        title="20220105_k.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_l.png" alt="20220105_l.png"
        title="20220105_l.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_m.png" alt="20220105_m.png"
        title="20220105_m.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_n.png" alt="20220105_n.png"
        title="20220105_n.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_o.png" alt="20220105_o.png"
        title="20220105_o.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_p.png" alt="20220105_p.png"
        title="20220105_p.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_w.png" alt="20220105_w.png"
        title="20220105_w.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_x.png" alt="20220105_x.png"
        title="20220105_x.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_y.png" alt="20220105_y.png"
        title="20220105_y.png" class="img_flow" height="250" width="250">
      <img src="images/20220105_z.png" alt="20220105_z.png"
        title="20220105_z.png" class="img_flow" height="250" width="250">
      <p>I constructed a bit of a Rube Goldberg machine to connect
        TexSyn running in c++ on my laptop with this “find conspicuous
        disk” vision model running in Python on the Colab cloud. I got
        them talking to each other on December 28 by reading and writing
        files to a shared directory in the Google Drive cloud storage
        utility. On January 1 I tried running the combined simulation.</p>
      <p><b>It did not work.</b> The xy predictions from the model were
        often significantly outside the [0, 1] range it had been trained
        to produce. Effectively it wanted to “draw the crosshair”
        outside the image. My current hypothesis is this model—which
        seemed to do a good job locating one conspicuous disk—is
        unprepared to handle three disks, as used in EvoCamoGame. For
        example, see the first image on <a href="#20211019">October 19</a>.
        My next task is to generalize in that direction.<br>
        <br>
      </p>
    </div>
    <div class="post" id="20211114"> <a href="#20211114" class="date">November










































































































































































































































































































        14, 2021</a>
      <h1>Training set for learning to find conspicuous disks<br>
      </h1>
      <p>For a predator to find prey visually, it must perceive some
        contrast between prey and environment. Camouflage is about
        making this detection more difficult: prey may evolve coloring
        allowing them to blend in with the environment. Before trying to
        break camouflage—to detect camouflaged prey—an important first
        step is to detect <i>conspicuous</i> prey.<br>
      </p>
      <p>Concretely, in my very abstract and synthetic 2d model, I hope
        to train a vision system to recognize conspicuous “objects.”
        These will represent prey. In the abstraction used here, I am
        talking about disk-shaped regions of texture, overlaid on a
        background image. (That background image represents the
        environment in which the predator and prey both live.) I hope to
        start using <i>supervised deep learning</i> to pre-train a
        network to locate disks, of a given size, with a contrasting
        texture, within a larger image. (This is very similar to <i>saliency























































































































































































































































































































          detection</i>.) If/when that works, I will then try to refine
        these networks (via <i>transfer learning</i>) to “break”
        camouflage. The supervised pre-training will require a
        collection of images with “ground truth” labels to indicate the
        location on the image of the conspicuous disks. <br>
      </p>
      <p>Last night I got my generator working and I have created about
        2000 training examples. I may decide I need a different design,
        or just more of the same. They are easy to crank out averaging
        about 10 seconds per training examples on my old 2014 MacBook
        Pro. Each image consists of a “random” background texture with a
        randomly placed disk of another “random” texture. Each training
        example is just a JPEG image file, with the pixel coordinates of
        the disk center encoded in the file name. Currently, the
        background image is either one of the photographs previously
        used in this project, or a TexSyn texture rendered from a random
        LazyPredator tree (as would be done in the initialization of an
        evolutionary computation run). 60% of the backgrounds are photos
        and the other 40% are synthetic. Conversely 60% of the overlaid
        disks are synthetic, and 40% photographic. So the majority are
        sim-over-real, but all four permutations are included
        (real-over-sim, sim-over-sim, and real-over-real). The images
        are 1024×1024 pixels and the disk diameter is 200. Shown below
        are a few examples hand selected from the first 2000 training
        examples. Some of them are simple (first one below) others are
        complex. Some are quite conspicuous (orange disk over gray tree
        bark) while others are surprisingly cryptic (last one below, a
        real-over-real case: pavement disk over polished granite). <br>
      </p>
      <img src="images/20211114_pngeVHaCDF_230_820.jpeg"
        alt="20211114_pngeVHaCDF_230_820"
        title="20211114_pngeVHaCDF_230_820" class="img_flow"
        height="400" width="400"> <img
        src="images/20211114_hTqTdsFhNv_844_420.jpeg"
        alt="20211114_hTqTdsFhNv_844_420"
        title="20211114_hTqTdsFhNv_844_420" class="img_flow"
        height="400" width="400"> <img
        src="images/20211114_uNFOorIfEt_149_234.jpeg"
        alt="20211114_uNFOorIfEt_149_234"
        title="20211114_uNFOorIfEt_149_234" class="img_flow"
        height="400" width="400"> <img
        src="images/20211114_azXzjfBAVH_816_668.jpeg"
        alt="20211114_azXzjfBAVH_816_668"
        title="20211114_azXzjfBAVH_816_668" class="img_flow"
        height="400" width="400"> <img
        src="images/20211114_qrXKbdMILt_343_433.jpeg"
        alt="20211114_qrXKbdMILt_343_433"
        title="20211114_qrXKbdMILt_343_433" class="img_flow"
        height="400" width="400"> <img
        src="images/20211114_sXFdGeQwiF_777_456.jpeg"
        alt="20211114_sXFdGeQwiF_777_456"
        title="20211114_sXFdGeQwiF_777_456" class="img_flow"
        height="400" width="400">&nbsp; <img
        src="images/20211114_vkbrxedwxK_825_668.jpeg"
        alt="20211114_vkbrxedwxK_825_668"
        title="20211114_vkbrxedwxK_825_668" class="img_flow"
        height="400" width="400"> <img
        src="images/20211114_hSCjLPGMQT_483_314.jpeg"
        alt="20211114_hSCjLPGMQT_483_314"
        title="20211114_hSCjLPGMQT_483_314" class="img_flow"
        height="400" width="400"> <img
        src="images/20211114_QJvCUSOVWk_414_713.jpeg"
        alt="20211114_QJvCUSOVWk_414_713"
        title="20211114_QJvCUSOVWk_414_713" class="img_flow"
        height="400" width="400"> </div>
    <div class="post" id="20211102"> <a href="#20211102" class="date">November


































































































































































































































































































































        2, 2021</a>
      <h1>Finally, disks with even diameters</h1>
      <p>I finally fixed a very old bug in TexSyn. As you will have
        noticed, I like to display procedural textures as disks. This is
        mostly a side effect of the application to camouflage evolution,
        where the idea is that these disks represent some sort of
        creature (say a beetle) on some background (say tree bark) but
        in any case, not a rectangular region of texture.</p>
      <p>Fine, whatever floats my boat. But the code that generates the
        circular rasterization initially assumed that the diameter was
        an <i>odd</i> number, so that there would a pixel in the
        middle, at (0, 0) in texture space. Originally this was helpful
        for testing. Now the only requirement is that the disk be
        exactly centered in the rendering window. For an <i>even</i>
        diameter, that means the origin of texture space will fall
        between four pixels near the center.</p>
      <p>But until today, only the odd-diameter case was supported.
        Worse, you got the same error (failed the same <code>assert</code>)
        using an even size when rendering to a non-disk square target!
        It is now fixed, which would not be worth writing about here.
        But while refactoring, I decided to use a slightly different
        (“better”?) definition for the disk. Generally rendered disks
        will effectively be ½ pixel larger in radius. This is nearly
        imperceptible. If you look closely at previously rendered
        texture disks, they had exactly one pixel on their top, bottom,
        left, and right sides. Now the in/out test is made for the
        center of the pixel (instead of its outer boundary) so now there
        will be “several” pixels on the boundaries of the disk (a
        function of disk diameter). I made this post just in case I
        notice something does not exactly match a previous result and
        need to remember when I made this change.</p>
    </div>
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20211019"> <a href="#20211019" class="date">October




































































































































































































































































































































        19, 2021</a>
      <h1>Camouflage on redwood leaf litter, <i>again</i><br>
      </h1>
      <p>Last <a moz-do-not-send="true" href="#20210414">April 14</a>,
        I made an <code>evo_camo_game</code> run on this same set of
        background images. They show redwood leaf litter, blown off
        trees in my neighborhood, and against the curb on the other side
        of the street. The previous results were ok, these are better,
        but not what I consider “strong.” I use that term when I look at
        an image showing a “tournament” of three camouflage textures,
        and it takes a while to find them. Most runs have some strong
        individuals which happen to be randomly placed in positions
        where they “melt” into the background. For example the one near
        the bottom center in the first image below, from simulation step
        2886. In the best runs, there will be some tournaments where all
        three are very well camouflaged. This run falls short of that
        level.<br>
      </p>
      <p>(I'll also note that this run (<font size="-2"><code>redwood_leaf_litter_20211010_1014</code></font>)
        used a larger simulation. The population size was increased from
        120 to 200, and the crossover size min/max where increased from
        [60, 180] to [75, 225]. I ran it for 3300 steps, or about 16
        “generation equivalents” (3300 ÷ 200 = 16.5) so roughly the same
        as earlier runs, adjusted for population size. The extra tree
        depth seemed to make a lot of the texture renders noticeably
        slower. In the end, this large simulation did not significantly
        increase the quality of results.)<br>
      </p>
      <p>As I wrap up my experiments with interactive evolution of
        camouflage, I am writing a report. It is not for publication in
        a journal, but just meant to be a more readable summary than
        this blog/diary. I wanted better images to include in that
        report.<br>
      </p>
      <img src="images/20211019_step_2886.png" alt="20211019_step_2886"
        title="20211019_step_2886" class="img_flow" height="533"
        width="800"> <img src="images/20211019_step_2950.png"
        alt="20211019_step_2950" title="20211019_step_2950"
        class="img_flow" height="533" width="800"> <img
        src="images/20211019_step_3116.png" alt="20211019_step_3116"
        title="20211019_step_3116" class="img_flow" height="533"
        width="800"> <img src="images/20211019_step_3192.png"
        alt="20211019_step_3192" title="20211019_step_3192"
        class="img_flow" height="533" width="800"> <img
        src="images/20211019_step_3236.png" alt="20211019_step_3236"
        title="20211019_step_3236" class="img_flow" height="533"
        width="800">
      <p>Nine hand-selected “thumbnails” from this run, on steps 1438
        through 3252. Number seven is impressively cryptic:<br>
      </p>
      <img src="images/20211019_thumbnail_1438_c.png"
        alt="20211019_thumbnail_1438_c.png"
        title="20211019_thumbnail_1438_c.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_1771_a.png"
        alt="20211019_thumbnail_1771_a.png"
        title="20211019_thumbnail_1771_a.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_2137_c.png"
        alt="20211019_thumbnail_2137_c.png"
        title="20211019_thumbnail_2137_c.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_2393_b.png"
        alt="20211019_thumbnail_2393_b.png"
        title="20211019_thumbnail_2393_b.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_2629_b.png"
        alt="20211019_thumbnail_2629_b.png"
        title="20211019_thumbnail_2629_b.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_2823_a.png"
        alt="20211019_thumbnail_2823_a.png"
        title="20211019_thumbnail_2823_a.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_2886_a.png"
        alt="20211019_thumbnail_2886_a.png"
        title="20211019_thumbnail_2886_a.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_3088_a.png"
        alt="20211019_thumbnail_3088_a.png"
        title="20211019_thumbnail_3088_a.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20211019_thumbnail_3252_a.png"
        alt="20211019_thumbnail_3252_a.png"
        title="20211019_thumbnail_3252_a.png" class="img_flow"
        height="402" width="402"> </div>
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20211016"> <a href="#20211016" class="date">October





































































































































































































































































































































        16, 2021</a>
      <h1>Three months later...</h1>
      <p>At the beginning of this project I was posting here every few
        days. Now months go by without a peep. Here is a quick recap.</p>
      <p><b>SimpleImageMatch</b>: after the <a moz-do-not-send="true"
          href="#20210729" style="">“Huntington hedge” camouflage run</a>
        I went back to my ill-fated <a moz-do-not-send="true"
          href="#20210727">simple image match</a> idea. It seemed easy:
        evolve a population of textures to “match” a given target image.
        I hoped it would come up with many interesting “near misses” —
        textures similar to the target image, but different in various
        ways. But it didn't work. It <i><b>seriously</b></i> didn't
        work. By “not work” I mean that the population of textures would
        converge on a family of portrayals that were only barely similar
        to the target image. For quite a long time I would say “Well,
        that run didn't work. Oh! I know what went wrong!” And I would
        say it over and over, run after run. I still don't understand
        why it so stubbornly fails to be “easy” but I need to admit I'm
        beat. While I hope to return to this idea, for now I am stuck.<br>
      </p>
      <p><b>Deep learning</b>: the SimpleImageMatch runs were long,
        non-interactive computations. So in parallel I began once again
        to learn deep learning. My first try was 20 years ago. The field
        has progressed by leaps and bounds since then. Useful
        abstractions have been adopted. The tools have become powerful
        and standardized. I was especially helped by the timely
        publication of <a moz-do-not-send="true"
          href="https://glassner.com">Andrew Glassner</a>'s excellent
        book <a moz-do-not-send="true"
          href="https://nostarch.com/deep-learning-visual-approach">Deep
          Learning: a Visual Approach</a>. I read it cover to cover and
        am now working through the online supplement — bonus chapters
        that apply the concepts discussed in the book to specific
        implementations with modern libraries. My goal is to build a
        system like EvoCamoGame, where the human predator is replaced by
        a deep learning predator based on convolutional neural networks.<br>
      </p>
    </div>
    <div class="post" id="20210729"> <a href="#20210729" class="date">July








































































































































































































































































































































        29, 2021</a>
      <h1>Huntington hedge camouflage, and a training set</h1>
      <p>In preparation for future work with hybrid closed loop
        camouflage discovery, I wanted to create a set of labeled
        training data. The results below were produced like previous
        interactive camouflage runs. But in addition “tournament
        images”, like the four below, were saved automatically, along
        with “ground truth” data specifying the location of each
        camouflaged prey texture, as a bounding box in pixel
        coordinates. For each saved image, there is a line in the data
        file with the step number, and three sets (one for each prey
        texture) of four integers (min x, min y, max x, max y). This
        data set is available here: <code><a moz-do-not-send="true"
href="https://drive.google.com/file/d/1-slGi-Q3X4th6eKl8xyDRNGSJL7XIG6w/view?usp=sharing">training_set_20210729.zip</a></code>.
        The folder contains 202 files: a <code>bounding_boxes.txt</code>
        text file with the ground truth data, and 201 image files <code>step_0.jpeg</code>
        through <code>step_2000.jpeg</code> with the step number
        counting by tens.<br>
      </p>
      <p>The four images below are from steps 1336, 1744, 1938, and 1968
        of the 2000 step run.<br>
      </p>
      <p>(The background images were taken July 7, 2021 at 2:12pm,
        looking down at the top a trimmed decorative hedge on the
        grounds of <i>The Huntington Library, Art Museum, and Botanical
          Gardens</i> in San Marino (near Pasadena, near Los Angeles,
        California). The plant is probably a Photinia, this particular
        species (hybrid?) has white/cream edges on its more mature
        leaves, perhaps “Cassini” or “Photinia Pink Marble™” as
        described <a moz-do-not-send="true"
href="https://landscapeplants.oregonstate.edu/plants/photinia-fraseri-pink-marble">here</a>.
        Run ID: <font size="-1"><code>huntington_hedge_20210726_1204</code></font>.)<br>
      </p>
      <img src="images/20210729_step_1336.jpg" alt="" title=""
        class="img_flow" height="533" width="800"> <img
        src="images/20210729_step_1744.jpg" alt="" title=""
        class="img_flow" height="533" width="800"> <img
        src="images/20210729_step_1938.jpg" alt="" title=""
        class="img_flow" height="533" width="800"> <img
        src="images/20210729_step_1968.jpg" alt="" title=""
        class="img_flow" height="533" width="800">
      <p>These are twelve hand-selected prey camouflage textures from
        steps 1108 to 1990:</p>
      <img src="images/20210729_thumbnail_1108_c.png"
        alt="20210729_thumbnail_1108_c"
        title="20210729_thumbnail_1108_c" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1313_c.png"
        alt="20210729_thumbnail_1313_c"
        title="20210729_thumbnail_1313_c" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1427_a.png"
        alt="20210729_thumbnail_1427_a"
        title="20210729_thumbnail_1427_a" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1477_b.png"
        alt="20210729_thumbnail_1477_b"
        title="20210729_thumbnail_1477_b" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1523_b.png"
        alt="20210729_thumbnail_1523_b"
        title="20210729_thumbnail_1523_b" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1763_a.png"
        alt="20210729_thumbnail_1763_a"
        title="20210729_thumbnail_1763_a" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1868_a.png"
        alt="20210729_thumbnail_1868_a"
        title="20210729_thumbnail_1868_a" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1882_b.png"
        alt="20210729_thumbnail_1882_b"
        title="20210729_thumbnail_1882_b" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1927_c.png"
        alt="20210729_thumbnail_1927_c"
        title="20210729_thumbnail_1927_c" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1936_c.png"
        alt="20210729_thumbnail_1936_c"
        title="20210729_thumbnail_1936_c" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1949_b.png"
        alt="20210729_thumbnail_1949_b"
        title="20210729_thumbnail_1949_b" class="img_flow" height="402"
        width="402"> <img src="images/20210729_thumbnail_1990_b.png"
        alt="20210729_thumbnail_1990_b"
        title="20210729_thumbnail_1990_b" class="img_flow" height="402"
        width="402"> </div>
    <div class="post" id="20210727"> <a href="#20210727" class="date">July








































































































































































































































































































































        27, 2021</a>
      <h1>“Simple image match” <i>really</i> not working<br>
      </h1>
      <p>More negative results to report. Beyond the application to
        camouflage, for a long time I've wanted to try using
        evolutionary texture optimization to discover novel ways to
        represent a given image. I hoped I could effectively “invent”
        visual stylizations. This would be related to, but different
        from, the large body of recent work in <a
          moz-do-not-send="true"
          href="https://en.wikipedia.org/wiki/Neural_Style_Transfer">style

















































































































































































































































































































































          transfer</a> developed in the computer vision community. I had
        grandiose plans, but as a first step, I thought I could try <i>simple

















































































































































































































































































































































          image matching</i> based on a “target image” like this
        photograph of a springtime hillside near our house:</p>
      <img src="images/20210727_IMG_3812_511_square.jpeg" alt="target
        image" title="target image" class="img_flow" height="511"
        width="511">
      <p>I planned to evolve a population of textures using a fitness
        function based on image similarity. It seemed like it should be
        easy to find evolved textures similar to the target image, then
        to gradually improve that similarity over evolutionary time.
        Either something is deeply wrong with my code, or none of the
        several metrics of image similarity I tried were appropriate for
        this task. I made a series of experiments from July 4 to 24. I
        got several results where the entire population converged to
        minor variations on a nearly uniform texture whose overall
        color—sort of a mid-range blue-green—was roughly the average
        color of the target image. I got some results with a very
        simplified “cartoonish” representation of the target image, like
        a child might make with some colored shape stickers. My last run
        seemed to be <i>trying</i> to represent the scene, but it was <b>so</b>
        abstract, I might have been fooling myself.</p>
      <p><b>Note:</b> in the images below, made by my very prototype
        code, (1) the image in the upper left can be ignored, it is just
        the most recently created texture, (2) the second is the target
        image at 255² pixels, and (3) the rest of the 58 textures in the
        grid are about the top half of the evolutionary population,
        sorted by fitness from highest to lowest. So the third texture
        on the top row is the highest fitness individual in the
        population being evolved by LazyPredator.</p>
      <p>In this run, the texture seems arbitrary, and the only
        “matching” is that the overall hue is close to the average color
        of the target image. A texture in the third column, fourth row,
        seems to almost have a blue sky above and green hill below, yet
        has fitness rank of 31 out of 120, so clearly this fitness
        function was not measuring the right thing:</p>
      <img src="images/20210727_7-16_7.15am.jpg" alt="20210716 7:15am"
        title="20210716 7:15am" class="img_flow" height="625"
        width="1000">
      <p>This run converged to a simplistic dark green hill with a blue
        sky, while ignoring all textural details:</p>
      <img src="images/20210727_7-18_4.25pm.jpg" alt="20210718 4:25pm"
        title="20210718 4:25pm" class="img_flow" height="623"
        width="1000">
      <p>This run converged to a (circular?) dark gray “hill” with a
        textured “sky” in blues and pinks, which might charitably be
        seen as trying to represent the clouds:</p>
      <img src="images/20210727_7-15_7.02am.jpg" alt="20210715 7:02am"
        title="20210715 7:02am" class="img_flow" height="623"
        width="1000">
      <p>Here is the almost “cubist” representation, as a simple radial
        texture, which vaguely puts dark green below like the hills, and
        bluish above like the sky, with some of the textures showing
        some desaturation in the sky, perhaps suggestive of clouds?
        These seem to be mostly a single <i>SliceToRadial</i> texture
        operator (see <a moz-do-not-send="true" href="#20200124">January

















































































































































































































































































































































          24, 2020</a>) at top level, with other textures used to
        generate that 1d “slice” of colors. In a few cases, as in the
        lower left corner of this grid, two overlaid <i>SliceToRadial</i>
        textures are visible. (This run began on July 22, 2021 at
        4:37pm.)</p>
      <img src="images/20210727_7-23_5.19pm_v2.jpg" alt="20210723
        5:19pm" title="20210723 5:19pm" class="img_flow" height="652"
        width="1000">
      <p>A few seconds of screen recording of the run begun July 19,
        2021 at 2:27pm. This is the upper right hand corner of the
        display, showing the newest texture (created by crossover and
        mutation on that single evolutionary step) next to the target
        image. Perhaps the prominent, often blue, circular feature in
        the “sky” region may be created by a <i>Hyperbolic</i> texture
        operator (see <a moz-do-not-send="true" href="#20200711">July
          11, 2020</a>) based on the way detail seems pressed against
        the circular boundary.<br>
      </p>
      <video controls="controls" height="136" width="268"> <source
          src="images/20210727_realtime_7-19_4.58pm.mp4"
          type="video/mp4">Your browser does not support the video tag.</video>
    </div>
    <div class="post" id="yyyymmdd"> <a href="#yyyymmdd" class="date">July






































































































































































































































































































































































        1, 2021</a>
      <h1>Unit tests for historical repeatability</h1>
      <p>Yesterday an idea popped into my head. I have been a fan of
        “test driven development” since I first encountered it in a job
        about six years ago. Developing a test suite in parallel with
        your application code allows you to verify it works correctly
        initially, and to verify it is still operating correctly after
        subsequent changes. It also helps to procedurally generate large
        numbers of test cases, more than you would try by hand. When
        bugs are found, they often become new test cases, to make sure
        that failure case never reappears. Most of these are somehow
        measuring internal consistency: comparing two results obtained
        by different methods.<br>
      </p>
      <p>What occurred to me was I had insufficient “test coverage” of
        whether top level results—such as the texture produced by a tree
        of operators—was the same today as it was yesterday. When I
        recently refactored the Color class (described below on <a
          moz-do-not-send="true" href="#20210623">June 23</a>) this was
        what I really needed to know. Is a color today exactly the same
        as a color yesterday? The way to do this is experimentally
        record results produced today and copy those into the program
        text of the test suite. Future versions of the code are then
        required to continue to produce those same result values. The
        downside of this is that I should have done it a long time ago,
        but better now than later. Certainly there have been intentional
        changes, such as a major rework (on <a href="#20200506">May 6,
          2020</a>) having to do with gamma correction and linear color
        space, which would have required remeasuring and updating the
        test.<br>
      </p>
      <p>The new tests are in the function <code>UnitTests::historical_repeatability()</code>.
        I defined four TexSyn textures (Spot , Turbulence, NoiseWarp,
        AdjustHue) with “random” parameters (I printed out a list of
        random floats on [-1, +1] then copied them into the source code
        as needed). Then I sampled the textures at two random positions
        within the unit radius disk, those positions and resulting color
        samples are all similarly “frozen” into the source code. The
        test passes as long as the frozen values continue to match newly
        computed results.<br>
      </p>
    </div>
    <div class="post" id="20210628"> <a href="#20210628" class="date">June









































































































































































































































































































































































        28, 2021</a>
      <h1>Camouflage: pebbles in concrete<br>
      </h1>
      <p>Any non-trivial software change is an excuse for me to do
        another “reality check” that <code>evo_camo_game</code> is
        still running as expected. The hand-selected camouflaged “prey”
        shown below are from steps 1300 through 1995 of a total of 2000
        steps in the run. The background photos are of multi-colored
        pebbles pressed into concrete between the street and a
        neighbor's front yard.<br>
      </p>
      <img src="images/20210628_thumbnail_1300_c.png"
        alt="20210628_thumbnail_1300_c"
        title="20210628_thumbnail_1300_c" class="img_flow" height="402"
        width="402"> <img src="images/20210628_thumbnail_1599_b.png"
        alt="20210628_thumbnail_1599_b"
        title="20210628_thumbnail_1599_b" class="img_flow" height="402"
        width="402"> <img src="images/20210628_thumbnail_1702_c.png"
        alt="20210628_thumbnail_1702_c"
        title="20210628_thumbnail_1702_c" class="img_flow" height="402"
        width="402"> <img src="images/20210628_thumbnail_1792_c.png"
        alt="20210628_thumbnail_1792_c"
        title="20210628_thumbnail_1792_c" class="img_flow" height="402"
        width="402"> <img src="images/20210628_thumbnail_1940_a.png"
        alt="20210628_thumbnail_1940_a"
        title="20210628_thumbnail_1940_a" class="img_flow" height="402"
        width="402"> <img src="images/20210628_thumbnail_1982_b.png"
        alt="20210628_thumbnail_1982_b"
        title="20210628_thumbnail_1982_b" class="img_flow" height="402"
        width="402"> <img src="images/20210628_thumbnail_1984_a.png"
        alt="20210628_thumbnail_1984_a"
        title="20210628_thumbnail_1984_a" class="img_flow" height="402"
        width="402"> <img src="images/20210628_thumbnail_1995_b.png"
        alt="20210628_thumbnail_1995_b"
        title="20210628_thumbnail_1995_b" class="img_flow" height="402"
        width="402"> </div>
    <div class="post" id="20210623"> <a href="#20210623" class="date">June









































































































































































































































































































































































        23, 2021</a>
      <h1>“...full of <strike>sound</strike> color and fury, signifying
        nothing...”</h1>
      <p>For the last couple of weeks I have been working on two other
        projects, with just a little puttering on TexSyn in the
        background. I previously removed some unused <code>.cpp</code>
        files, where all the content was in the <code>.h</code> file. I
        should admit that I lean toward “header only” c++ libraries.
        Many c++ designers would disagree, but I think the modest
        increase in compilation time is worth the <i>much</i> easier
        distribution, installation, and usage of header-only libraries.</p>
      <p>One of the not-unused files was <code>Color.cpp</code>. I have
        been refactoring and making lots of little edits to clean up the
        <code>Color</code> class and to move its entire definition to <code>Color.h</code>.
        The class is native RGB, but supports <a moz-do-not-send="true"
          href="https://en.wikipedia.org/wiki/HSL_and_HSV">HSV color
          space</a>, first described by <a moz-do-not-send="true"
          href="http://alvyray.com">Alvy Ray Smith</a> in a <a
          moz-do-not-send="true"
          href="http://alvyray.com/Papers/PapersCG.htm#ColorGamut">1978
          SIGGRAPH paper</a>. HSV is a simple color model, but is
        sufficient for TexSyn since convenient parameterization is not
        needed for evolutionary optimization.</p>
      <p>I had left many “to do” notes in the code about awkward color
        API and especially the RGB⇄HSV conversion functions. I made a
        new class inside <code>Color</code> called <code>HSV</code> to
        contain and modularize the datatype and conversions. The <code>HSV</code>
        class is also exported to global scope so it can be used in
        other code, particularly the texture operators. One of the key
        improvement is that both <code>Color</code> and <code>HSV</code>
        have constructors that take the other type as a parameter: <code>Color(hsv)</code>
        and <code>HSV(color)</code>. This allowed a fair amount of
        streamlining. All of the texture operator code using HSV got
        shorter after the refactor.</p>
      <p>I added more <code>UnitTests</code> to ensure the conversions
        returned identical results against explicit constants written in
        the tests. (This is in addition to previous “round trip”
        conversion tests: RGB→HSV→RGB.) I verified that the texture
        operators returned the exact same result down to floating point
        accuracy. I do this by duplicating the old operator and renaming
        it, making changes to the other version, then comparing them
        with <code>Texture::diff()</code>. It counts the number of
        individual rendered pixel colors which are not exactly the same
        float values in the two textures. This mismatch count was zero
        in all cases. So in the end lots of code changes, but no changes
        to the textures themselves.<br>
      </p>
      <p><b>Update June 25, 2021:</b> similarly I merged <code>Vec2.cpp</code>
        into <code>Vec2.h</code> and deleted the former.<br>
      </p>
    </div>
    <div class="post" id="20210615"> <a href="#20210615" class="date">June




















































































































































































































































































































































































        15, 2021</a>
      <h1>Late-blooming clover camouflage</h1>
      <p>After adjusting selection weights for the phasor noise
        operators, on <a moz-do-not-send="true" href="#20210611">June
          11</a>, I intended to do a quick run to convince myself that
        everything was still behaving normally. I found myself
        unconvinced! I ran for a couple hundred steps and while it was
        generally tending toward the green color of the “clover”
        background, I saw little to no improvement in camouflage. I kept
        expecting it to start getting better, even as I got to 1000 then
        2000 steps.</p>
      <p>It seemed unlikely that I had actually broken anything. I
        assumed that it was just “bad luck” — that randomized algorithms
        sometimes fail. But I wanted to believe that it would work,
        given time. As I got past 2000 steps, things did appear to be
        improving: the optimization toward effective camouflage patterns
        began to occur. These five samples show camouflage “prey”
        between step 2500 and 3000. This is longer than I normally run
        the simulations, I assume due to bad luck, but eventually the
        population began to self organize — “get its feet under it” —
        and began to produce effective camouflage.</p>
      <img src="images/20210615_thumbnail_2546_c.png"
        alt="20210615_thumbnail_2546_c"
        title="20210615_thumbnail_2546_c" class="img_flow" height="402"
        width="402"> <img src="images/20210615_thumbnail_2765_a.png"
        alt="20210615_thumbnail_2765_a"
        title="20210615_thumbnail_2765_a" class="img_flow" height="402"
        width="402"> <img src="images/20210615_thumbnail_2918_a.png"
        alt="20210615_thumbnail_2918_a"
        title="20210615_thumbnail_2918_a" class="img_flow" height="402"
        width="402"> <img src="images/20210615_thumbnail_3215_a.png"
        alt="20210615_thumbnail_3215_a"
        title="20210615_thumbnail_3215_a" class="img_flow" height="402"
        width="402"> <img src="images/20210615_thumbnail_3290_b.png"
        alt="20210615_thumbnail_3290_b"
        title="20210615_thumbnail_3290_b" class="img_flow" height="402"
        width="402"> </div>
    <div class="post" id="20210611"> <a href="#20210611" class="date">June





















































































































































































































































































































































































        11, 2021</a>
      <h1>Adjust selection weight for phasor noise operators</h1>
      <p>As on <a moz-do-not-send="true" href="#20210326">March 26</a>,
        I adjusted selection weights used in building random <code>GpTrees</code>
        for the initial population at the beginning of an evolution run.
        As then, the goal is keep consistent the probability that a
        given <code>GpFunction</code> will be selected, in the face of
        multiple versions of the same “type” of texture operator. This
        was the motivation of <i>MultiNoise</i> (see <a
          moz-do-not-send="true" href="#20200112">January 12, 2020</a>)
        which combined the five variations of Perlin noise into one
        texture operator, rather than make it five time more likely one
        of that family would be selected. The selection weight mechanism
        was added (on <a moz-do-not-send="true"
          href="file:///Users/cwr/Documents/code/TexSyn/docs/index.html#20210326">March






















































































































































































































































































































































































          26</a>)&nbsp; to make a similar adjustment for the three
        variations of “spots” operators: <i>LotsOfSpots</i>, <i>ColoredSpots</i>,
        and <i>LotsOfButtons</i>.<br>
      </p>
      <p>Today I adjusted <i>PhasorNoiseRanges</i> and <i>PhasorNoiseTextures</i>
        to each have half the default selection weight.<br>
      </p>
    </div>
    <div class="post" id="20210607"> <a href="#20210607" class="date">June



























































































































































































































































































































































































        7, 2021</a>
      <h1>Function usage over evolutionary time</h1>
      <p>I am still fiddling with the presentation, but this is the
        “function usage” time series data from my second <code>evo_camo_game</code>
        run with background image set “kitchen_granite.” During the
        first run, my logging code had a bug. In the second run, logging
        worked fine, but the camouflage quality was not as good. (See
        some examples after the plot.) After creating an initial random
        population of 120 textures, each “program”/“nested
        expression”/“abstract syntax tree”/<code>GpTree</code> was
        inspected to count the occurrences of TexSyn texture operators.
        At every tenth step of the 2000 step simulation, usage counts
        for 45 function, summed over the entire population, were
        appended to a CSV file. Shown below is a spreadsheet plot of
        that file: counts on the vertical axis and evolutionary time
        across the horizontal axis. I excluded from this plot the “leaf”
        values of the program trees. These include floating point
        constants and calls to both <code>Vec2(x, y)</code> and <code>Uniform(r,























































































































































































































































































































































































          g, b)</code>. The latter two had counts 2-4 times larger than
        the other data, so were inconvenient to include on the same
        plot.<br>
      </p>
      <p>The only analysis I will offer is that some of the usage count
        clearly increase over time. That blue one that peaks above 300
        is for the texture operator <i>Blur</i>. This suggests that in
        this run, the average tree contains 2 to 3 <i>Blur</i>
        operators. Conversely some function counts clearly fall off over
        time. The <i>Ring</i> operator (gray plot line near bottom)
        starts at 19 occurrences in the initial population and falls to
        zero just before step 400. By the end of the run, 9 operators
        had fallen to zero count, disappearing from the population. Some
        of the counts do not seem to rise or fall, staying at about the
        same level with some small wobble up and down.</p>
      <p>There is a loose, conceptual, parallel between these function
        counts and the relative frequency of biological genes in the
        genome of a population of some species. As with genes, the
        frequency of a function will tend to increase when it confers a
        survival advantage to an individual, and will decrease if it
        reduces survival. But frequency can also change by “hitchhiking”
        effects where it happens to co-occur with an important gene. The
        red plot trace near the top (ends at count ~275) is for <i>NoiseWarp</i>.
        In this case, I wonder if <i>Blur</i> just “hitchhiked” on <i>NoiseWarp</i>—perhaps


















































































































































































































































































































































































        in a helpful subtree that contains them both. Intuitively, it
        seems to me that <i>NoiseWarp</i> is more important than <i>Blur</i>,
        given these granite background images which are “swirly” but not
        “blurry.”</p>
      <img src="images/20210607_function_usage.png" alt="function usage
        plot" title="function usage plot" class="img_flow" height="747"
        width="1115">
      <p>Thumbnails from steps 936, 1330, 1670, 1846, and 1940 of this
        second “kitchen_granite” run of 2000 steps:<br>
      </p>
      <img src="images/20210607_thumbnail_936_c.png"
        alt="20210607_thumbnail_936_c" title="20210607_thumbnail_936_c"
        class="img_flow" height="402" width="402"> <img
        src="images/20210607_thumbnail_1330_a.png"
        alt="20210607_thumbnail_1330_a"
        title="20210607_thumbnail_1330_a" class="img_flow" height="402"
        width="402"> <img src="images/20210607_thumbnail_1670_c.png"
        alt="20210607_thumbnail_1670_c.png"
        title="20210607_thumbnail_1670_c.png" class="img_flow"
        height="402" width="402"> <img
        src="images/20210607_thumbnail_1846_b.png"
        alt="20210607_thumbnail_1846_b"
        title="20210607_thumbnail_1846_b" class="img_flow" height="402"
        width="402"> <img src="images/20210607_thumbnail_1940_a.png"
        alt="20210607_thumbnail_1940_a"
        title="20210607_thumbnail_1940_a" class="img_flow" height="402"
        width="402"> </div>
    <div class="post" id="20210606"> <a href="#20210606" class="date">June





























































































































































































































































































































































































        6, 2021</a>
      <h1>Good news, bad news, kitchen granite</h1>
      <p>I made another <code>evo_camo_game</code> run with the intent
        of testing my new analysis tool for collecting time-series data
        for “function usage.” But I discovered a bug in the new code so
        the data I recorded is not useful. I will repeat this run soon
        and make a second attempt at recording function usage time
        series.</p>
      <p>In the meantime, here are images from this first run. The
        backgrounds are photos of a granite counter-top in our kitchen.
        (It is illuminated in these photos by direct sunlight, so
        appears brighter than it normally would.) This run developed
        some effective camouflage patterns, which do a good job of
        obscuring their boundaries. But the camouflage is slightly
        off-kilter. Early on it stumbled into some cryptic patterns,
        which were in a pink and magenta color scheme, with some green
        accents. It is noticeably offset in hue from the
        orange/tan/brown colors in the granite. I expected the hue to
        adjust to be closer to the background during evolution. However
        most of the prey population remained stuck on the pink color
        scheme for the entire 2000 step run. Here are images of three
        hand-selected tournaments (each with three camouflaged prey) at
        steps 1413, 1863, and 1931.</p>
      <img src="images/20210606_step_1413.png" alt="20210606_step_1413"
        title="20210606_step_1413" class="img_flow" height="533"
        width="800"> <img src="images/20210606_step_1863.png"
        alt="20210606_step_1863" title="20210606_step_1863"
        class="img_flow" height="533" width="800"> <img
        src="images/20210606_step_1931.png" alt="20210606_step_1931"
        title="20210606_step_1931" class="img_flow" height="533"
        width="800">
      <p> Below are 12 hand selected examples of camouflaged prey
        spanning steps 310 to 1962 of the 2000 step run. Note that some
        “brownish” patterns arose but they never became as cryptically
        effective as the pink/green phenotype.</p>
      <img src="images/20210606_thumbnail_310_a.png"
        alt="20210606_thumbnail_310_a" title="20210606_thumbnail_310_a"
        class="img_flow" height="402" width="402"> <img
        src="images/20210606_thumbnail_351_a.png"
        alt="20210606_thumbnail_351_a" title="20210606_thumbnail_351_a"
        class="img_flow" height="402" width="402"> <img
        src="images/20210606_thumbnail_573_a.png"
        alt="20210606_thumbnail_573_a" title="20210606_thumbnail_573_a"
        class="img_flow" height="402" width="402"> <img
        src="images/20210606_thumbnail_834_c.png"
        alt="20210606_thumbnail_834_c" title="20210606_thumbnail_834_c"
        class="img_flow" height="402" width="402"> <img
        src="images/20210606_thumbnail_968_b.png"
        alt="20210606_thumbnail_968_b" title="20210606_thumbnail_968_b"
        class="img_flow" height="402" width="402"> <img
        src="images/20210606_thumbnail_1113_b.png"
        alt="20210606_thumbnail_1113_b"
        title="20210606_thumbnail_1113_b" class="img_flow" height="402"
        width="402"> <img src="images/20210606_thumbnail_1148_b.png"
        alt="20210606_thumbnail_1148_b"
        title="20210606_thumbnail_1148_b" class="img_flow" height="402"
        width="402"> <img src="images/20210606_thumbnail_1461_a.png"
        alt="20210606_thumbnail_1461_a"
        title="20210606_thumbnail_1461_a" class="img_flow" height="402"
        width="402"> <img src="images/20210606_thumbnail_1740_b.png"
        alt="20210606_thumbnail_1740_b"
        title="20210606_thumbnail_1740_b" class="img_flow" height="402"
        width="402"> <img src="images/20210606_thumbnail_1797_b.png"
        alt="20210606_thumbnail_1797_b"
        title="20210606_thumbnail_1797_b" class="img_flow" height="402"
        width="402"> <img src="images/20210606_thumbnail_1880_c.png"
        alt="20210606_thumbnail_1880_c"
        title="20210606_thumbnail_1880_c" class="img_flow" height="402"
        width="402"> <img src="images/20210606_thumbnail_1962_c.png"
        alt="20210606_thumbnail_1962_c"
        title="20210606_thumbnail_1962_c" class="img_flow" height="402"
        width="402"> </div>
    <div class="post" id="20210531"> <a href="#20210531" class="date">May



































































































































































































































































































































































































        31, 2021</a>
      <h1>Camouflage on “Scot's Broom”</h1>
      <p>This camouflage run (<font size="-2"><code>yellow_flower_on_green_20210529_1236</code></font>)
        includes the new <i>NoiseWarp</i> texture operator. I assume it
        produces the wavy shape of yellow and green elements in these
        camouflage patterns. <br>
      </p>
      <p>The background images are of a neighbor's landscaping, probably
        “Scot's Broom” (<a moz-do-not-send="true"
          href="https://en.wikipedia.org/wiki/Cytisus_scoparius"><i>Cytisus









































































































































































































































































































































































































            scoparius</i></a>) or perhaps the similar “French broom” (<i><a
            moz-do-not-send="true"
            href="https://en.wikipedia.org/wiki/Genista_monspessulana">Genista









































































































































































































































































































































































































            monspessulana</a></i>). These common European shrubs are
        technically “invasive” here in California, but have become so
        extremely widespread that they are “effectively native.” In the
        spring they put out dense clusters of bright yellow blossoms
        contrasting with the vivid green foliage.<br>
      </p>
      <p>I ran this simulation for 2500 steps. Shown below are three
        “tournaments” each with three camouflaged prey, from late in the
        run at steps 2375, 2419, and 2485:<br>
      </p>
      <img src="images/20210531_step_2375.png"
        alt="yellow_flower_on_green_20210529_1236 step 2375"
        title="yellow_flower_on_green_20210529_1236 step 2375"
        class="img_flow" height="533" width="800"> <img
        src="images/20210531_step_2419.png"
        alt="yellow_flower_on_green_20210529_1236 step 2419"
        title="yellow_flower_on_green_20210529_1236 step 2419"
        class="img_flow" height="533" width="800"> <img
        src="images/20210531_step_2485.png"
        alt="yellow_flower_on_green_20210529_1236 step 2485"
        title="yellow_flower_on_green_20210529_1236 step 2485"
        class="img_flow" height="533" width="800">
      <p>Individual camouflaged prey, hand-selected, from steps 635
        through 2483:<br>
      </p>
      <img src="images/20210531_thumbnail_635_c.png"
        alt="20210531_thumbnail_635_c" title="20210531_thumbnail_635_c"
        class="img_flow" height="402" width="402"> <img
        src="images/20210531_thumbnail_644_c.png"
        alt="20210531_thumbnail_644_c" title="20210531_thumbnail_644_c"
        class="img_flow" height="402" width="402"> <img
        src="images/20210531_thumbnail_719_a.png"
        alt="20210531_thumbnail_719_a" title="20210531_thumbnail_719_a"
        class="img_flow" height="402" width="402"> <img
        src="images/20210531_thumbnail_827_a.png"
        alt="20210531_thumbnail_827_a" title="20210531_thumbnail_827_a"
        class="img_flow" height="402" width="402"> <img
        src="images/20210531_thumbnail_1063_b.png"
        alt="20210531_thumbnail_1063_b"
        title="20210531_thumbnail_1063_b" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_1122_a.png"
        alt="20210531_thumbnail_1122_a"
        title="20210531_thumbnail_1122_a" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_1325_c.png"
        alt="20210531_thumbnail_1325_c"
        title="20210531_thumbnail_1325_c" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_1508_a.png"
        alt="20210531_thumbnail_1508_a"
        title="20210531_thumbnail_1508_a" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_1577_c.png"
        alt="20210531_thumbnail_1577_c"
        title="20210531_thumbnail_1577_c" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_1793_a.png"
        alt="20210531_thumbnail_1793_a"
        title="20210531_thumbnail_1793_a" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_2389_c.png"
        alt="20210531_thumbnail_2389_c"
        title="20210531_thumbnail_2389_c" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_2473_b.png"
        alt="20210531_thumbnail_2473_b"
        title="20210531_thumbnail_2473_b" class="img_flow" height="402"
        width="402"> <img src="images/20210531_thumbnail_2483_a.png"
        alt="20210531_thumbnail_2483_a"
        title="20210531_thumbnail_2483_a" class="img_flow" height="402"
        width="402"> </div>
    <div class="post" id="20210527"> <a href="#20210527" class="date">May





















































































































































































































































































































































































































        27, 2021</a>
      <h1>Random trees with <i>NoiseWarp</i> at root</h1>
      <p>These are randomly generated <code>GpTree</code>s with a <i>
          NoiseWarp</i> operator at the root (top). This uses basically
        the same procedure as on <a moz-do-not-send="true"
          href="#20210508">May 8</a>: generating a set of examples, then
        hand selecting some visually interesting ones. The “source code”
        (textual representation of the <code>GpTree</code>) is given
        for the first two. The others are analogous but rather too long
        to be readable.</p>
      <pre>// Example 1
NoiseWarp(1.05963,
          0.420626,
          0.325789,
          AdjustHue(0.619672,
                    ColorNoise(Vec2(3.44575, 2.55265),
                               Vec2(-1.01872, 1.04531),
                               0.905149)))

// Example 2
NoiseWarp(9.48588,
          0.358824,
          0.549201,
          MultiNoise(Vec2(-3.3431, -2.40702),
                     Vec2(3.38479, 1.45248),
                     BrightnessWrap(0.192339,
                                    0.412704,
                                    BrightnessToHue(0.793737,
                                                    ColorNoise(Vec2(1.48892, 2.592),
                                                               Vec2(1.13958, 0.671887),
                                                               0.555035))),
                     AdjustBrightness(0.893365,
                                      HueOnly(0.363777,
                                              0.957132,
                                              ColorNoise(Vec2(-3.20501, 2.04162),
                                                         Vec2(-3.83085, 3.93283),
                                                         0.684134))),
                     0.499956))</pre>
      <img src="images/20210527_NoiseWarp_1.png"
        alt="20210527_NoiseWarp_1" title="20210527_NoiseWarp_1"
        class="img_flow" height="511" width="511"> <img
        src="images/20210527_NoiseWarp_2.png" alt="20210527_NoiseWarp_2"
        title="20210527_NoiseWarp_2" class="img_flow" height="511"
        width="511"> <img src="images/20210527_NoiseWarp_3.png"
        alt="20210527_NoiseWarp_3" title="20210527_NoiseWarp_3"
        class="img_flow" height="511" width="511"> <img
        src="images/20210527_NoiseWarp_4.png" alt="20210527_NoiseWarp_4"
        title="20210527_NoiseWarp_4" class="img_flow" height="511"
        width="511"> </div>
    <div class="post" id="20210526"> <a href="#20210526" class="date">May






























































































































































































































































































































































































































        26, 2021</a>
      <h1>Parameter for “style” of noise in <i>NoiseWarp</i></h1>
      <p>The <i>NoiseWarp</i> examples on <a moz-do-not-send="true"
          href="#20210524">May 24</a> all used <i>Brownian</i> noise
        for generating the lateral warp displacement vector. It was
        actually using the TexSyn concept of “multi-noise” (like the <i>MultiNoise</i>
        texture operator, see <a moz-do-not-send="true"
          href="#20200112">January 12, 2020</a>, based on the utility <code>PerlinNoise::multiNoise2d()</code>)
        where an additional parameter (called <code>which</code>,
        between 0 and 1) selects between five styles of noise: Perlin,
        Brownian, turbulence, “furbulence,” and “wrapulence.” Here is
        one more “wedge” through the parameter space of <i>NoiseWarp</i>,
        operating on the same blue and white cloudy pattern, with scale
        of 3, amplitude of 0.5, and “which” of 0.1 (Perlin), 0.3
        (Brownian), 0.5 (turbulence), 0.7 (furbulence), and 0.9
        (wrapulence):<br>
      </p>
      <img src="images/20210526_NoiseWarp_0.1.png" alt="NoiseWarp(3,
        0.5, 0.1, test)" title="NoiseWarp(3, 0.5, 0.1, test)"
        class="img_flow" height="402" width="402"> <img
        src="images/20210526_NoiseWarp_0.3.png" alt="NoiseWarp(3, 0.5,
        0.3, test)" title="NoiseWarp(3, 0.5, 0.3, test)"
        class="img_flow" height="402" width="402"> <img
        src="images/20210526_NoiseWarp_0.5.png" alt="NoiseWarp(3, 0.5,
        0.5, test)" title="NoiseWarp(3, 0.5, 0.5, test)"
        class="img_flow" height="402" width="402"> <img
        src="images/20210526_NoiseWarp_0.7.png" alt="NoiseWarp(3, 0.5,
        0.7, test)" title="NoiseWarp(3, 0.5, 0.7, test)"
        class="img_flow" height="402" width="402"> <img
        src="images/20210526_NoiseWarp_0.9.png" alt="NoiseWarp(3, 0.5,
        0.9, test)" title="NoiseWarp(3, 0.5, 0.9, test)"
        class="img_flow" height="402" width="402"> </div>
    <div class="post" id="20210524"> <a href="#20210524" class="date">May







































































































































































































































































































































































































































        24, 2021</a>
      <h1><i>NoiseWarp</i> progress</h1>
      <p>I liked the “domain warp” textures (via Perlin, Quilez, and
        Bostock) described on <a moz-do-not-send="true"
          href="#20210521">May 21</a>. However they are scalar textures.
        So visually they are black and white, or used to matte between
        two other textures (as in the May 21 example), or colorized by
        some <i>ad hoc</i> rules as Ken Perlin did in <a
          moz-do-not-send="true"
href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.2248&amp;rep=rep1&amp;type=pdf">1985</a>
        and Inigo Quilez did in <a moz-do-not-send="true"
          href="https://www.iquilezles.org/www/articles/warp/warp.htm">2012</a>.
        To integrate this domain/noise warp into TexSyn, I felt it ought
        to take an arbitrary input texture (and some parameters) and
        return a texture which is warped version of the input texture.
        That it should have the same color structure, just pushed around
        in texture space. So I took the same basic approach as described
        on Inigo's page: an output pixel color is read from input
        texture at a point “slightly offset” from the output pixel
        position. The noise offset's coordinates are from calls to a
        scalar noise function (2d Brownian in these examples) at
        positions offset by two perpendicular basis vectors. Two
        parameters <code>noise_scale</code> and <code>noise_amplitude</code>
        adjust this mapping:<br>
      </p>
      <pre>noise_offset = Vec2(noise2d(basis0 + (output_position * noise_scale)),
                    noise2d(basis1 + (output_position * noise_scale)))
input_position = output_position + (noise_offset * noise_amplitude)
output_color = input_texture.getColor(input_position)</pre>
      <p>Also, since I have re-cut-and-pasted this code several times, I
        made a TexSyn texture class that generate my oft-used “plaid”
        text pattern:</p>
      <img src="images/20210524_Plaid.png" alt="“plaid” test pattern"
        title="“plaid” test pattern" class="img_flow" height="402"
        width="402">
      <p>Here are three results of applying this prototype <i>NoiseWarp</i>
        to <i>Plaid</i> with scale of 2 and amplitude of 0.1, 0.2, and
        0.3:</p>
      <img src="images/20210524_NoiseWarp_2_0.1.png" alt="NoiseWarp
        scale=2 amplitude=0.1" title="NoiseWarp scale=2 amplitude=0.1"
        class="img_flow" height="402" width="402"> <img
        src="images/20210524_NoiseWarp_2_0.2.png" alt="NoiseWarp scale=2
        amplitude=0.2" title="NoiseWarp scale=2 amplitude=0.2"
        class="img_flow" height="402" width="402"> <img
        src="images/20210524_NoiseWarp_2_0.3.png" alt="NoiseWarp scale=2
        amplitude=0.3" title="NoiseWarp scale=2 amplitude=0.3"
        class="img_flow" height="402" width="402">
      <p>Here are results with scale of 1, 2, and 4 with amplitude of
        0.2:</p>
      <img src="images/20210524_NoiseWarp_1_0.2.png" alt="NoiseWarp
        scale=1 amplitude=0.2" title="NoiseWarp scale=1 amplitude=0.2"
        class="img_flow" height="402" width="402"> <img
        src="images/20210524_NoiseWarp_2_0.2.png" alt="NoiseWarp scale=2
        amplitude=0.2" title="NoiseWarp scale=2 amplitude=0.2"
        class="img_flow" height="402" width="402"> <img
        src="images/20210524_NoiseWarp_4_0.2.png" alt="NoiseWarp scale=4
        amplitude=0.2" title="NoiseWarp scale=4 amplitude=0.2"
        class="img_flow" height="402" width="402">
      <p>And one more, to tie this in with the previous examples, a
        similar blue and white <i>Brownian</i> noise warped with a
        scale of 10 and an amplitude of 0.5:</p>
      <img src="images/20210524_NoiseWarp_10_0.5.png" alt="NoiseWarp
        scale=10 amplitude=0.5" title="NoiseWarp scale=10 amplitude=0.5"
        class="img_flow" height="511" width="511"> </div>
    <div class="post" id="20210521"> <a href="#20210521" class="date">May









































































































































































































































































































































































































































        21, 2021</a>
      <h1>Prototype of “noise warp”</h1>
      <p>I ran across <a moz-do-not-send="true"
          href="https://bost.ocks.org/mike/">Mike Bostock</a>'s <a
          moz-do-not-send="true"
          href="https://observablehq.com/@mbostock/domain-warping">Domain









































































































































































































































































































































































































































          Warping</a> which is based on <a moz-do-not-send="true"
          href="https://www.iquilezles.org/index.html">Inigo Quilez</a>’s









































































































































































































































































































































































































































        <a moz-do-not-send="true"
          href="https://www.iquilezles.org/www/articles/warp/warp.htm">Domain









































































































































































































































































































































































































































          Warping</a>. The examples below are essentially the first
        three examples on Inigo's page. “Essentially” because I started
        with my existing <i>Brownian</i> texture generator for the
        zeroth level, then built on that using Inigo's parameters for
        the next two. The phrase “domain warping” felt a little too
        ambiguous, given that quite a few of TexSyn's operators perform
        some sort of domain warping. This specific technique is based on
        recursively reapplying the same “fractional Browning motion”
        map. As Inigo points out, this is is closely related to Ken
        Perlin's procedural “marble” textures, described in his SIGGRAPH
        1985 paper <a moz-do-not-send="true"
href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.2248&amp;rep=rep1&amp;type=pdf">An









































































































































































































































































































































































































































          Image Synthesizer</a>. This code (I'm calling it <i>NoiseWarp</i>
        for now) could be used in TexSyn as is, but I am giving some
        thought to ways to generalize it.<br>
      </p>
      <img src="images/20210521_NoiseWarp_test_0.png" alt="NoiseWarp
        test 0" title="NoiseWarp test 0" class="img_flow" height="511"
        width="511"> <img src="images/20210521_NoiseWarp_test_1.png"
        alt="NoiseWarp test 1" title="NoiseWarp test 1" class="img_flow"
        height="511" width="511"> <img
        src="images/20210521_NoiseWarp_test_2.png" alt="NoiseWarp test
        2" title="NoiseWarp test 2" class="img_flow" height="511"
        width="511"> </div>
    <div class="post" id="20210517"> <a href="#20210517" class="date">May

















































































































































































































































































































































































































































        17, 2021</a>
      <h1>Camouflage for “Michael's gravel”</h1>
      <p>This run of <code>evo_camo_game</code> uses photos of a
        neighbor's front yard gravel bed for its background (run id: <font
          size="-1"><code>michaels_gravel_20210513_1743</code></font>).
        It prominently uses the new “phasor noise” texture operators.
        Here are images of the three camouflaged “prey” in tournaments
        at steps 1260, 2564, and 2858 of a run 3000 step long.<br>
      </p>
      <img src="images/20210517_step_1260.png" alt="michaels_gravel step
        1260" title="michaels_gravel step 1260" class="img_flow"
        height="533" width="800"> <img
        src="images/20210517_step_2564.png" alt="michaels_gravel step
        2564" title="michaels_gravel step 2564" class="img_flow"
        height="533" width="800"> <img
        src="images/20210517_step_2858.png" alt="michaels_gravel step
        2858" title="michaels_gravel step 2858" class="img_flow"
        height="533" width="800">
      <p>Below are 15 hand-selected thumbnail images of camouflaged
        prey. They correspond to steps: 150, 755, 840, 1558, 2024, 2098,
        2411, 2456, 2529, 2607, 2702, 2788, 2867, 2932, and 2974 of a
        run of 3000 steps total.</p>
      <img src="images/20210517_thumbnail_150_c.png"
        alt="20210517_thumbnail_150_c" title="20210517_thumbnail_150_c"
        class="img_flow" height="402" width="402"> <img
        src="images/20210517_thumbnail_755_a.png"
        alt="20210517_thumbnail_755_a" title="20210517_thumbnail_755_a"
        class="img_flow" height="402" width="402"> <img
        src="images/20210517_thumbnail_840_c.png"
        alt="20210517_thumbnail_840_c" title="20210517_thumbnail_840_c"
        class="img_flow" height="402" width="402"> <img
        src="images/20210517_thumbnail_1558_a.png"
        alt="20210517_thumbnail_1558_a"
        title="20210517_thumbnail_1558_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2024_a.png"
        alt="20210517_thumbnail_2024_a"
        title="20210517_thumbnail_2024_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2098_c.png"
        alt="20210517_thumbnail_2098_c"
        title="20210517_thumbnail_2098_c" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2411_a.png"
        alt="20210517_thumbnail_2411_a"
        title="20210517_thumbnail_2411_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2456_a.png"
        alt="20210517_thumbnail_2456_a"
        title="20210517_thumbnail_2456_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2529_c.png"
        alt="20210517_thumbnail_2529_c"
        title="20210517_thumbnail_2529_c" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2607_a.png"
        alt="20210517_thumbnail_2607_a"
        title="20210517_thumbnail_2607_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2702_a.png"
        alt="20210517_thumbnail_2702_a"
        title="20210517_thumbnail_2702_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2788_a.png"
        alt="20210517_thumbnail_2788_a"
        title="20210517_thumbnail_2788_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2867_c.png"
        alt="20210517_thumbnail_2867_c"
        title="20210517_thumbnail_2867_c" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2932_a.png"
        alt="20210517_thumbnail_2932_a"
        title="20210517_thumbnail_2932_a" class="img_flow" height="402"
        width="402"> <img src="images/20210517_thumbnail_2974_c.png"
        alt="20210517_thumbnail_2974_c"
        title="20210517_thumbnail_2974_c" class="img_flow" height="402"
        width="402"> </div>
    <div class="post" id="20210508"> <a href="#20210508" class="date">May

















































































































































































































































































































































































































































        8, 2021</a>
      <h1>Random phasor noise with kernel parameters from textures<br>
      </h1>
      <p>Analogous to the random “mixed kernel” phasor noise samples
        shown on <a moz-do-not-send="true" href="#20210428">April 28</a>,
        these are random textures whose <code>GpTree</code>s have a <i>PhasorNoiseTextures</i>
        operator at the tree root. (They were constructed using the
        utility <code>FunctionSet::makeRandomTreeRoot()</code>.) That
        is, there is a <i>PhasorNoiseTextures</i> operator at top, two
        random floats, and five random TexSyn trees as parameters. For
        example, the second texture shown below—pinkish swirls over
        muted multi-color—is defined by this code:<br>
      </p>
      <pre>PhasorNoiseTextures(0.966585,
                    0.575451,
                    BrightnessToHue(0.9796,
                                    CotsMap(Vec2(2.06722, 1.95574),
                                            Vec2(-3.62892, 3.08396),
                                            Vec2(0.504355, -4.94588),
                                            Vec2(2.33541, -3.70556),
                                            Uniform(0.132475, 0.122944, 0.706272))),
                    Uniform(0.323459, 0.203465, 0.943876),
                    PhasorNoiseTextures(0.595613,
                                        0.0889437,
                                        Uniform(0.174884, 0.365976, 0.10978),
                                        Uniform(0.912784, 0.611449, 0.870163),
                                        Uniform(0.444307, 0.799821, 0.897265),
                                        Uniform(0.696277, 0.989898, 0.628882),
                                        Uniform(0.537117, 0.611029, 0.653296)),
                    Uniform(0.963422, 0.0551525, 0.181825),
                    Gamma(1.16942,
                          ColorNoise(Vec2(2.06214, 0.309379),
                                     Vec2(0.983534, 3.74675),
                                     0.00450438)))</pre>
      <p>In that code, the “profile function” is nearly a sine wave. The
        textures defining kernel radius and wavelength are constant (“<i>Uniform</i>”)


















































































































































































































































































































































































































































        while the texture defining kernel orientation is itself another
        <i>PhasorNoiseTextures</i>. The last two parameters are the
        pinkish color and the multi-color background.<br>
      </p>
      <p>I generated 100 such textures (the whole set can be seen <a
          moz-do-not-send="true"
href="https://drive.google.com/file/d/1bwSVLZfLCXapT6kOgOhIOlB_QwkKnotu/view?usp=sharing">here</a>)
        then hand-selected out these 20:<br>
      </p>
      <img src="images/20210508_phasor_noise_9.png"
        alt="20210508_phasor_noise_9" title="20210508_phasor_noise_9"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_12.png"
        alt="20210508_phasor_noise_12" title="20210508_phasor_noise_12"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_17.png"
        alt="20210508_phasor_noise_17" title="20210508_phasor_noise_17"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_18.png"
        alt="20210508_phasor_noise_18" title="20210508_phasor_noise_18"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_22.png"
        alt="20210508_phasor_noise_22" title="20210508_phasor_noise_22"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_25.png"
        alt="20210508_phasor_noise_25" title="20210508_phasor_noise_25"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_29.png"
        alt="20210508_phasor_noise_29" title="20210508_phasor_noise_29"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_31.png"
        alt="20210508_phasor_noise_31" title="20210508_phasor_noise_31"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_37.png"
        alt="20210508_phasor_noise_37" title="20210508_phasor_noise_37"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_43.png"
        alt="20210508_phasor_noise_43" title="20210508_phasor_noise_43"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_63.png"
        alt="20210508_phasor_noise_63" title="20210508_phasor_noise_63"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_68.png"
        alt="20210508_phasor_noise_68" title="20210508_phasor_noise_68"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_71.png"
        alt="20210508_phasor_noise_71" title="20210508_phasor_noise_71"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_73.png"
        alt="20210508_phasor_noise_73" title="20210508_phasor_noise_73"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_78.png"
        alt="20210508_phasor_noise_78" title="20210508_phasor_noise_78"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_86.png"
        alt="20210508_phasor_noise_86" title="20210508_phasor_noise_86"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_88.png"
        alt="20210508_phasor_noise_88" title="20210508_phasor_noise_88"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_89.png"
        alt="20210508_phasor_noise_89" title="20210508_phasor_noise_89"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_91.png"
        alt="20210508_phasor_noise_91" title="20210508_phasor_noise_91"
        class="img_flow" height="320" width="320"> <img
        src="images/20210508_phasor_noise_97.png"
        alt="20210508_phasor_noise_97" title="20210508_phasor_noise_97"
        class="img_flow" height="320" width="320"> </div>
    <!-- --------------------------------------------------------------------------- -->
    <div class="post" id="20210507"> <a href="#20210507" class="date">May




































































































































































































































































































































































































































































        7, 2021</a>
      <h1>Phasor noise angles: radians → revolutions</h1>
      <p>Just a note about non-backward-compatibility. I had been
        following the paper <a
          href="https://doi.org/10.1145/3306346.3322990">Procedural
          phasor noise</a>—and mathematical conventions—by using units
        of <a href="https://en.wikipedia.org/wiki/Radian">radians</a>
        for Gabor kernel orientation angles. Today I switched to using
        units of “revolutions” a normalized measure of rotation: <b>1 </b><b><i>revolution</i></b><b>
          = 2π </b><b><i>radians</i></b><b> = 360 </b><b><i>degrees</i></b>.</p>
      <p>There was only one place in the phasor noise code where this
        mattered. <code>PhasorNoiseBase::samplePhasorFieldOfKernel()</code>
        now scales the kernel's angle by 2π. The reason for this change
        is to better fit in with TexSyn's interface to LazyPredator, the
        evolutionary optimizer. One aspect is which <code>GpType</code>
        to use for an angle's literal value? These constants have an
        associated numeric range. None of the existing ones correspond
        to a float value on the range [0, 2π]. More significantly, the <i>PhasorNoiseTextures</i>
        operator takes its Gabor kernel angles from the luminance of
        colors in a given <code>Texture</code> object. While a
        Texture's luminance is unbounded, the vast majority of them will
        be inside the unit RGB cube, so <code>Color::</code><code>luminance()</code>
        will be on [0, 1]. Mapping this value on to an angle in
        “revolutions” seems the most natural.</p>
      <p>The only impact is that some code snippets below may no longer
        produce the pictured texture.</p>
    </div>
    <div class="post" id="20210506"> <a href="#20210506" class="date">May








































































































































































































































































































































































































































































        6, 2021</a>
      <h1>Oh! <i>That’s</i> better<br>
      </h1>
      <p>I tried the rubber duck. I tried convincing myself that I'd
        exchanged angle and wavelength. Then finally I took a closer
        look at <code>PhasorNoiseBase::samplePhasorFieldOfKernel()</code>
        which I “knew” was correct, because I'd used it for <code>PhasorNoiseRanges</code>.
        There was confusion between “global” texture coordinates and the
        “local” space of a given kernel. <i>Et voilà</i>:<br>
      </p>
      <pre><span class="comment">// Parameters to PhasorNoiseTextures:</span>
PhasorNoiseTextures(float profile_softness,
                    float profile_duty_cycle,
                    const Texture&amp; texture_for_kernel_radius,
                    const Texture&amp; texture_for_kernel_wavelength,
                    const Texture&amp; texture_for_kernel_angle,
                    const Texture&amp; source_texture_0,
                    const Texture&amp; source_texture_1)

<span class="comment">// The three textures below:</span>
PhasorNoiseTextures(1, 0.5, Uniform(0.1), Uniform(0.05), <b>noise</b>, black, white)
PhasorNoiseTextures(1, 0.5, Uniform(0.1), Uniform(0.05), <b>spot</b>,  black, white)
PhasorNoiseTextures(1, 0.5, Uniform(0.1), <b>noise</b>,         <b>spot</b>,  black, white)</pre>
      <img src="images/20210506_perlin_angle.png"
        alt="20210506_perlin_angle" title="20210506_perlin_angle"
        class="img_flow" height="511" width="511"> <img
        src="images/20210506_spot_angle.png" alt="20210506_spot_angle"
        title="20210506_spot_angle" class="img_flow" height="511"
        width="511"> <img src="images/20210506_spot_angle_noise_wl.png"
        alt="20210506_spot_angle_noise_wl"
        title="20210506_spot_angle_noise_wl" class="img_flow"
        height="511" width="511"> </div>
    <div class="post" id="20210505"> <a href="#20210505" class="date">May






















































































































































































































































































































































































































































































        5, 2021</a>
      <h1>Trouble getting phasor noise parameters from textures</h1>
      <p>Normally I post here after something is working properly. Today
        I'm posting about a mysterious bug that has had me stuck for a
        couple of days. My goal here is something like <a
          href="https://en.wikipedia.org/wiki/Rubber_duck_debugging">rubber






















































































































































































































































































































































































































































































          duck debugging</a>, where by describing the problem I may gain
        insight into its cause. After seemingly getting the “mixed
        kernel” version of phasor noise working (see <a
          href="#20210428">April 28</a>) I refactored that <code>PhasorNoisePrototype</code>
        operator into a <code>PhasorNoiseBase</code> class plus a small
        <code>PhasorNoiseRanges</code> derived class for the “mixed
        kernel” version. Then I wrote a new derived class <code>PhasorNoiseTextures</code>
        to handle the version where parameters vary across the texture
        plane by three input textures to specify: kernel radius, kernel
        angle, and kernel wavelength. That ran OK, but the resulting
        textures did not fit with what I expected to see.</p>
      <p>Back on <a href="#20210411">April 11</a> I posted a sample of
        phasor noise made with a c++ version of the <a
          href="https://www.shadertoy.com/view/wlsXWf">GLSL sample code</a>
        provided by the authors. That is on the left below. It has
        constant values for kernel radius and wavelength, and uses a
        Perlin noise texture to provide the smoothly varying angle
        (orientation) values. This produced the labyrinth-like “sand
        dune” pattern characteristic of phasor noise. On the right below
        is a texture produced by the new <code>PhasorNoiseTextures</code>
        operator that “should” look like the one on the left. It looks
        more like a “contour map” of the angle field.</p>
      <img src="images/20210411_first_light.png" alt="first phasor
        noise" title="first phasor noise" class="img_flow" height="511"
        width="511"> <img src="images/20210505_contour_map_symptom.png"
        alt="20210505_contour_map_symptom"
        title="20210505_contour_map_symptom" class="img_flow"
        height="511" width="511">
      <p>And here is the result if the angle field is given as a <i>Spot</i>
        texture. I think this should have an angle of zero on the outer
        edge (as it does) then with that angle increasing radially
        toward the center (origin) of the texture, eventually getting
        back to 2π. That is, I expected to see a radially symmetric
        pattern, but instead see this:</p>
      <img src="images/20210505_spot_for_angle.png"
        alt="20210505_spot_for_angle" title="20210505_spot_for_angle"
        class="img_flow" height="511" width="511"> </div>
    <div class="post" id="20210428"> <a href="#20210428" class="date">April





















































































































































































































































































































































































































































































        28, 2021</a>
      <h1>Random “mixed kernel” phasor noise</h1>
      <p>[<b>Update May 8, 2021</b>: as described above (on <a
          moz-do-not-send="true" href="#20210505">May 5</a>, <a
          moz-do-not-send="true" href="#20210506">6</a>, and <a
          moz-do-not-send="true" href="#20210507">7</a>) the textures
        originally shown in this post were wrong. Now “wrong” is a
        slippery concept when talking about random textures, but their
        randomness was not constructed as intended. Two things have
        changed: a bug in <code>PhasorNoiseBase::samplePhasorFieldOfKernel()</code>
        has been fixed, and angles are now specified in units of <i>revolutions</i>
        instead of <i>radians</i>. Today I went back and rerendered
        these textures, starting from the same random seed. The old
        versions can be found in the git repository in the unlikely
        event they are needed for anything.]</p>
      <p>The texture operator for “mixed kernel” phasor noise seems to
        be working now. The operator takes eight float parameters: min
        and max bounds for each of three properties of kernels (radius,
        wavelength, and orientation), two parameters for an asymmetrical
        “soft square wave” used as the “profile” reconstruction
        function, plus two input textures (here just uniform black and
        white). These “quasi-Gabor kernels” have a finite radius because
        they are based on cosinusoidal spots rather than Gaussians. This
        finite support allows them to be used with TexSyn's <code>DiskOccupancyGrid</code>
        spatial data structure for efficiently determining which kernels
        contribute to a given texture sample (pixel). The frequencies
        for each kernel are given as wavelengths (1/f) to make them
        linear in distances, to better fit in with the optimization
        framework (LazyPredator's genetic programming evolutionary
        computation). To construct a single kernel for this phasor
        noise, a uniform random selection is made for each kernel
        parameter from within the given min/max bounds. The 12 random
        variations on phasor noise below are each created from executing
        code like this (where <code>rs</code> is a <code>RandomSequence</code>
        object and <code>frandom01()</code> returns the next random
        value from the sequence as a <code> float</code> on [0, 1]):</p>
      <pre>PhasorNoiseRanges(rs.frandom01(),  <span class="comment">// profile softness</span>
                  rs.frandom01(),  <span class="comment">// profile duty_cycle</span>
                  rs.frandom01(),  <span class="comment">// min kernel radius</span>
                  rs.frandom01(),  <span class="comment">// max kernel radius</span>
                  rs.frandom01(),  <span class="comment">// min kernel wavelength</span>
                  rs.frandom01(),  <span class="comment">// max kernel wavelength</span>
                  rs.frandom01(),  <span class="comment">// min kernel orientation</span>
                  rs.frandom01(),  <span class="comment">// max kernel orientation</span>
                  black,           <span class="comment">// source texture 0</span>
                  white)           <span class="comment">// source texture 1</span></pre>
      <img src="images/20210508_phasor_r_0.png"
        alt="“20210508_phasor_r_0”" title="“20210508_phasor_r_0”"
        class="img_flow" height="400" width="400"> <img
        src="images/20210508_phasor_r_1.png" alt="“20210508_phasor_r_1”"
        title="“20210508_phasor_r_1”" class="img_flow" height="400"
        width="400"> <img src="images/20210508_phasor_r_2.png"
        alt="“20210508_phasor_r_2”" title="“20210508_phasor_r_2”"
        class="img_flow" height="400" width="400"> <img
        src="images/20210508_phasor_r_3.png" alt="“20210508_phasor_r_3”"
        title="“20210508_phasor_r_3”" class="img_flow" height="400"
        width="400"> <img src="images/20210508_phasor_r_4.png"
        alt="“20210508_phasor_r_4”" title="“20210508_phasor_r_4”"
        class="img_flow" height="400" width="400"> <img
        src="images/20210508_phasor_r_5.png" alt="“20210508_phasor_r_5”"
        title="“20210508_phasor_r_5”" class="img_flow" height="400"
        width="400"> <img src="images/20210508_phasor_r_6.png"
        alt="“20210508_phasor_r_6”" title="“20210508_phasor_r_6”"
        class="img_flow" height="400" width="400"> <img
        src="images/20210508_phasor_r_7.png" alt="“20210508_phasor_r_7”"
        title="“20210508_phasor_r_7”" class="img_flow" height="400"
        width="400"> <img src="images/20210508_phasor_r_8.png"
        alt="“20210508_phasor_r_8”" title="“20210508_phasor_r_8”"
        class="img_flow" height="400" width="400"> <img
        src="images/20210508_phasor_r_9.png" alt="“20210508_phasor_r_9”"
        title="“20210508_phasor_r_9”" class="img_flow" height="400"
        width="400"> <img src="images/20210508_phasor_r_10.png"
        alt="“20210508_phasor_r_10”" title="“20210508_phasor_r_10”"
        class="img_flow" height="400" width="400"> <img
        src="images/20210508_phasor_r_11.png"
        alt="“20210508_phasor_r_11”" title="“20210508_phasor_r_11”"
        class="img_flow" height="400" width="400"> </div>
    <div class="post" id="20210424"> <a href="#20210424" class="date">April








































































































































































































































































































































































































































































































        24, 2021</a>
      <h1>Phasor noise progress</h1>
      <p>I posted my first phasor noise texture on <a href="#20210411">April








































































































































































































































































































































































































































































































          11</a>, using a c++ rewrite of a (GLSL?) <a
          href="https://www.shadertoy.com/view/wlsXWf">shader</a>
        provided by the authors of <a
          href="https://doi.org/10.1145/3306346.3322990">Procedural
          phasor noise</a> (SIGGRAPH 2019, Tricard, Efremov,&nbsp; <em>et








































































































































































































































































































































































































































































































          al.</em>). Special thanks to <a
          href="http://thibaulttricard.fr/">Thibault Tricard</a> who has
        patiently answered my questions about their code. I have been
        refactoring the code to better integrate with the rest of
        TexSyn. I am getting closer, but this is still prototype code.
        Phasor noise is an improvement on the earlier <a
          href="https://doi.org/10.1145/1531326.1531360">Gabor noise</a>
        approach (SIGGRAPH 2009, Lagae, et al.) as prototyped here on <a
          href="#20210328">March 28</a>.</p>
      <p>My current plan is to define two TexSyn texture operators based
        on phasor noise. One will generate phasor noise using Gabor
        kernel parameters taken from a uniform random distribution
        between numeric bounds. As such the kernels will be independent
        of each other. The other operator will provide locally coherent
        Gabor kernel parameters by reading them from TexSyn textures
        provided to the operator. The example shown on <a
          href="#20210411">April 11</a> had constant values for kernel
        radius and frequency, and used a Perlin noise pattern to supply
        the orientation of the kernel. Shown below are examples of the
        “uniform random distribution between bounds” version. Shown
        below are the scalar noise field, used in TexSyn operators to
        blend (<em>SoftMatte</em>) between two other given textures.</p>
      <pre><span class="comment">// Parameters are min and max for: radius, wavelength, and angle, then the two input textures.</span>
PhasorNoisePrototype(0.7, 1.5,   0.06, 0.12,   pi * 0.3, pi * 0.8,   black, white)  <span class="comment">// On left, with "soft square wave" profile.</span>
PhasorNoisePrototype(2.0, 2.0,   0.05, 0.20,          0, pi * 2.0,   black, white)  <span class="comment">// On right, with sinusoid profile.</span></pre>
      <img src="images/20210424_phasor_1.png" alt="20210424_phasor_1"
        title="20210424_phasor_1" style="padding-right: 1em;
        padding-bottom: 1em;" height="511" width="511"> <img
        src="images/20210424_phasor_2.png" alt="20210424_phasor_2"
        title="20210424_phasor_2" style="padding-right: 1em;
        padding-bottom: 1em;" height="511" width="511">
      <p>To help illustrate what is going on here, the example below has
        its min and max radius set so low that (restricted by a
        performance-related limit on the total number of kernels within
        a 10x10-unit tile) they do not fill the texture plane. Unlike
        actual Gabor kernels—which are a Gaussian spot times a
        sinusoidal grating (and so have infinite support)—these are a
        cosine-spot times the sinusoidal grating (so have finite
        support, going to zero by a given radius). Where these kernels
        overlap, you can see the phasor addition forming intermediate
        “high contrast” waveforms, so avoiding interference and loss of
        contrast. I have not decided what to do in this case. Certainly
        one option is to do nothing special and “let evolution decide” —
        presumably that an ill formed texture like this has low fitness
        and should be selected against. Another option is to notice this
        case and return a texture result identical to one of the input
        textures (likely black in this case).</p>
      <p><strong>Update on April 26</strong>: I decided to go with the
        latter option. This was easy to do, since the kernels are added
        to the Texture's definition one at a time until there are
        sufficiently many to cover the field to sufficient depth
        (currently 4 kernels on average per pixel). If it reaches the
        max kernel count before satisfying this depth requirement, a
        flag is set to indicate failure. Rendering the texture below now
        produces a flat black color field—<code>Uniform(0)</code>—equivalent









































































































































































































































































































































































































































































































        to its next-to-last parameter.</p>
      <pre>PhasorNoisePrototype(0.07, 0.15,   0.02, 0.08,   0, 2 * pi,   black, white)  <span class="comment">// Min/max radius too small to cover texture plane.</span></pre>
      <img src="images/20210424_phasor_3.png" alt="20210424_phasor_3"
        title="20210424_phasor_3" style="padding-right: 1em;
        padding-bottom: 1em;" height="511" width="511"> </div>
    <div class="post" id="20210414"> <a href="#20210414" class="date">April








































































































































































































































































































































































































































































































        14, 2021</a>
      <h1>Camouflage on redwood leaf litter</h1>
      <p>This <code>evo_camo_game</code> run (tagged
        redwood_leaf_litter_20210413_1441) uses the same parameters as
        other recent runs, except for one. I was curious if I had been
        forcing the individual prey to limit the complexity of their
        texture specification. I had been using the default of the
        parameter which limits the max <code>GpTree</code> size during
        crossover. (It does not enforce a hard limit, but once parent
        trees exceed this size threshold, crossover fragments are chosen
        to reduce the size of offspring trees.) Parameter <code>max_init_tree_size</code>
        defaults to 100, and <code>max_crossover_tree_size</code>
        defaults to 1.5 times that: 150. Instead I specified a value of
        200. That seemed to make no difference at all in tree sizes
        (averaged over the whole population). In this run, the highest
        average tree size was 132, lower that the default max limit of
        150, and well below 200, the max limit specified for this run.</p>
      <p>These background images show fallen leaves/needles of some
        redwood trees in my neighborhood. The wind had blown them into
        the gutter on the opposite side of the street. As they slowly
        dry, they change from bright green to reddish brown. Shown
        below, two images of all three prey in competition at steps 2575
        and 2618, of the total 3000 steps of this run:</p>
      <img src="images/20210414_step_2575.png" alt="20210414_step_2575"
        title="20210414_step_2575" height="533" width="800"> <br>
      <br>
      <img src="images/20210414_step_2618.png" alt="20210414_step_2618"
        title="20210414_step_2618" height="533" width="800">
      <p>Many of the better camouflage patterns blend in by both
        matching the mix of background colors, and by “disrupting” their
        circular boundary with black features that visually match up
        with inky shadows in these photos shot in bright afternoon sun.
        Shown below are thumbnails of individual prey from step 1294 to
        2867:</p>
      <img src="images/20210414_thumbnail_1294_a.png"
        alt="20210414_thumbnail_1294_a"
        title="20210414_thumbnail_1294_a" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_1903_a.png"
        alt="20210414_thumbnail_1903_a"
        title="20210414_thumbnail_1903_a" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_1979_a.png"
        alt="20210414_thumbnail_1979_a"
        title="20210414_thumbnail_1979_a" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_2016_a.png"
        alt="20210414_thumbnail_2016_a"
        title="20210414_thumbnail_2016_a" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_2083_b.png"
        alt="20210414_thumbnail_2083_b"
        title="20210414_thumbnail_2083_b" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_2108_b.png"
        alt="20210414_thumbnail_2108_b"
        title="20210414_thumbnail_2108_b" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_2375_a.png"
        alt="20210414_thumbnail_2375_a"
        title="20210414_thumbnail_2375_a" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_2420_a.png"
        alt="20210414_thumbnail_2420_a"
        title="20210414_thumbnail_2420_a" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_2845_a.png"
        alt="20210414_thumbnail_2845_a"
        title="20210414_thumbnail_2845_a" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> <img
        src="images/20210414_thumbnail_2867_b.png"
        alt="20210414_thumbnail_2867_b"
        title="20210414_thumbnail_2867_b" style="padding-right: 1em;
        padding-bottom: 1em;" height="402" width="402"> </div>
    <div class="post" id="20210411"> <a href="#20210411" class="date">April








































































































































































































































































































































































































































































































        11, 2021</a>
      <h1>Phasor noise prototype</h1>
      <p>As mentioned on <a href="#20210328">March 28</a>, I started
        experimenting with “phasor noise” as described in the SIGGRAPH
        2019 paper <a href="https://doi.org/10.1145/3306346.3322990">Procedural









































































































































































































































































































































































































































































































          phasor noise</a> by Thibault Tricard, Semyon Efremov, Cédric
        Zanni, Fabrice Neyret, Jonàs Martínez, and Sylvain Lefebvre.
        They provide source code as a <a
          href="https://www.shadertoy.com/view/wlsXWf">shader</a>. I
        made a quick edit to allow it to compile in c++, but the
        resulting code produced uncorrelated “confetti” noise.
        Eventually I realized that in shader languages, global variables
        are treated like c++'s <code>thread_local</code>, with a
        separate copy for each thread. Fixing that allowed it to work:</p>
      <img src="images/20210411_first_light.png" alt="first phasor
        noise" title="first phasor noise" height="511" width="511"> </div>
    <div class="post" id="20210410"> <a href="#20210410" class="date">April








































































































































































































































































































































































































































































































        10, 2021</a>
      <h1>Camouflage run on Eric's redbud</h1>
      <p>We have a small tree in our front yard planted around the time
        our son was born. It is a “western redbud” (<em><a
            href="https://en.wikipedia.org/wiki/Cercis_occidentalis">Cercis









































































































































































































































































































































































































































































































            occidentalis</a></em>) selected because it blossoms near his
        birthday. As the pink flowers began opening this year, I took a
        set of photos with sky for background. These photos are similar
        to the “parking lot trees” posted on <a href="#20210315">March
          15</a>. As with several of my runs, the quality of these
        evolved camouflage patterns are “just OK.” They blend in to the
        background but are not especially cryptic. Generally they do a
        poor job of “disrupting” their circular boundary. I let this run
        go for 5000 steps, the longest so far, but it never got to the
        quality I hoped. Here are some images of all three prey in a
        tournament at step 3613, 4087, and 4866:</p>
      <img src="images/20210410_step_3613.png" alt="20210410_step_3613"
        title="20210410_step_3613" height="533" width="800"> <br>
      <br>
      <img src="images/20210410_step_4087.png" alt="20210410_step_4087"
        title="20210410_step_4087" height="533" width="800"> <br>
      <br>
      <img src="images/20210410_step_4866.png" alt="20210410_step_4866"
        title="20210410_step_4866" height="533" width="800">
      <p>Here are a selection of individual prey “thumbnail” images from
        steps 171 through 4974:</p>
      <img src="images/20210410_thumbnail_171_a.png"
        alt="20210410_thumbnail_171_a" title="20210410_thumbnail_171_a"
        style="padding-right: 1em; padding-bottom: 1em; " height="402"
        width="402"> <img src="images/20210410_thumbnail_475_a.png"
        alt="20210410_thumbnail_475_a" title="20210410_thumbnail_475_a"
        style="padding-right: 1em; padding-bottom: 1em; " height="402"
        width="402"> <img src="images/20210410_thumbnail_869_b.png"
        alt="20210410_thumbnail_869_b" title="20210410_thumbnail_869_b"
        style="padding-right: 1em; padding-bottom: 1em; " height="402"
        width="402"> <img src="images/20210410_thumbnail_1381_a.png"
        alt="20210410_thumbnail_1381_a"
        title="20210410_thumbnail_1381_a" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_1813_b.png"
        alt="20210410_thumbnail_1813_b"
        title="20210410_thumbnail_1813_b" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_2073_b.png"
        alt="20210410_thumbnail_2073_b"
        title="20210410_thumbnail_2073_b" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_2311_a.png"
        alt="20210410_thumbnail_2311_a"
        title="20210410_thumbnail_2311_a" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_2834_a.png"
        alt="20210410_thumbnail_2834_a"
        title="20210410_thumbnail_2834_a" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_3104_b.png"
        alt="20210410_thumbnail_3104_b"
        title="20210410_thumbnail_3104_b" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_3674_a.png"
        alt="20210410_thumbnail_3674_a"
        title="20210410_thumbnail_3674_a" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_3685_c.png"
        alt="20210410_thumbnail_3685_c"
        title="20210410_thumbnail_3685_c" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_4213_b.png"
        alt="20210410_thumbnail_4213_b"
        title="20210410_thumbnail_4213_b" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_4525_c.png"
        alt="20210410_thumbnail_4525_c"
        title="20210410_thumbnail_4525_c" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_4624_a.png"
        alt="20210410_thumbnail_4624_a"
        title="20210410_thumbnail_4624_a" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_4716_a.png"
        alt="20210410_thumbnail_4716_a"
        title="20210410_thumbnail_4716_a" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> <img
        src="images/20210410_thumbnail_4974_a.png"
        alt="20210410_thumbnail_4974_a"
        title="20210410_thumbnail_4974_a" style="padding-right: 1em;
        padding-bottom: 1em; " height="402" width="402"> </div>
    <div class="post" id="20210401"> <a href="#20210401" class="date">April








































































































































































































































































































































































































































































































        1, 2021</a>
      <h1><em>Mirror</em> should not scale, and other stories</h1>
      <p>Today I found a bug in the <em>Mirror</em> texture operator.
        (See post on <a href="#20200309">March 9, 2020</a>, making it a
        388 day old bug, yikes!) I would suggest that by the <a
          href="https://en.wikipedia.org/wiki/Principle_of_least_astonishment">principle









































































































































































































































































































































































































































































































          of least surprise</a>, applying <em>Mirror</em> to a texture
        should simply mirror it about the given line. Unfortunately, as
        originally written, the given tangent(-to-the-mirror-line)
        vector was treated as a basis vector of the new space, so had
        the unintended effect of scaling the input texture by 1/length.</p>
      <p>This bug was very simple to fix, but shows a problem with doing
        your testing with self-designed examples. I am a fan of unit
        tests (and <a
          href="https://en.wikipedia.org/wiki/Test-driven_development">TDD</a>)
        but in the end, they only test for the failures you anticipate.
        I made simple test cases, by hand, when working on <em>Mirror</em>,
        which in retrospect were too simple. For example: passing in
        pre-normalized vectors, parallel to the main axes. In 1994 <a
          href="https://www.karlsims.com/">Karl Sims</a> did pioneering
        work <a href="https://www.karlsims.com/papers/siggraph94.pdf">evolving</a>(/<a
          href="https://www.karlsims.com/papers/alife94.pdf">co-evolving</a>)
        morphogenesis and behavior for virtual creatures. He noted that
        evolution was great at coming up with unanticipated test cases,
        so exposing bugs, which evolution would then exploit to increase
        fitness.</p>
      <p>How this came up: I was doing a quick check of <code>evo_camo_game</code>
        after a code change, to verify things were behaving correctly.
        This camouflaged prey appeared at step 324 of a run on my
        “clover” background:</p>
      <img src="images/20210401_thumbnail_324_b.png" alt="thumbnail"
        title="thumbnail" height="402" width="402">
      <p>I had been surprised by similar textures before, so decided to
        dig in. This texture exhibits an annoyingly obvious square
        tiling. TexSyn supports such textures (see last example on <a
          href="#20200312">March 12, 2020</a>) but I've seen too many
        like this (perpendicular, square, axis-aligned) to believe that
        was the source. I suspected it was a scaled down version of the
        “large” tiling used for spot patterns. I saved the thumbnail
        above, which now also saves the source code in a text file:</p>
      <pre>Mirror(Vec2(3.21816, -3.75822),
       Vec2(3.35559, -2.45618),
       LotsOfButtons(0.525498,
                     0.961501,
                     0.142252,
                     0.435903,
                     0.113291,
                     Vec2(1.57994, 4.01201),
                     Gradation(Vec2(1.80012, -2.66427),
                               Gamma(9.51904,
                                     Uniform(0.960211, 0.096008, 0.597687)),
                               Vec2(1.78565, 2.38711),
                               Blur(0.409173,
                                    BrightnessWrap(0.271493,
                                                   0.0877538,
                                                   BrightnessToHue(0.55085,
                                                                   AdjustSaturation(0.231542,
                                                                                    EdgeDetect(0.598311,
                                                                                               Uniform(0.814141, 0.52608, 0.431994))))))),
                     0.754812,
                     Twist(2.92905,
                           1.55438,
                           Vec2(-2.18284, 4.04233),
                           BrightnessWrap(0.887923,
                                          0.72951,
                                          Affine(Vec2(-0.3471, 4.48348),
                                                 Vec2(-3.05151, -0.422401),
                                                 AdjustBrightness(0.635429,
                                                                  Subtract(Uniform(0.165418, 0.545178, 0.589462),
                                                                           BrightnessWrap(0.940368,
                                                                                          0.6996,
                                                                                          Uniform(0.538748, 0.0987566, 0.542896)))))))))</pre>
      <p>Ignoring the details, it is a <em>LotsOfButtons</em> operator,
        wrapped by a <em>Mirror</em> operator:</p>
      <pre>Mirror(Vec2(...), Vec2(...), LotsOfButtons(...))</pre>
      <p>I re-rendered the texture from that source code:</p>
      <img src="images/20210401_original.png" alt="original"
        title="original" height="511" width="511">
      <p>Then after I changed <em>Mirror</em> to normalize its <code>line_tangent</code>
        parameter, I got:</p>
      <img src="images/20210401_normalized.png" alt="normalized"
        title="normalized" height="511" width="511">
      <p>And indeed, when I rendered just the third parameter to <em>Mirror</em>,
        the <em>LotsOfButtons</em> operator itself, I get the same
        thing. I <em>think</em> that is the desired result, but I was
        wrong before:</p>
      <img src="images/20210401_buttons.png" alt="buttons"
        title="buttons" height="511" width="511">
      <p>But this is a good news / bad news situation, or perhaps a
        “twofer” of old bug discovery. I suspect the “spots” (the
        textured “buttons”) should all be visually similar, the same
        gradient between two shades of orange. Instead, I see some
        orange and some a more red color. More on that as I dig further.</p>
    </div>
    <div class="post" id="20210328"> <a href="#20210328" class="date">March








































































































































































































































































































































































































































































































        28, 2021</a>
      <h1>Gabor noise experiments</h1>
      <p>And now for the first time since <a href="#20200731">July 31,
          2020</a>, a post in this TexSyn devo blog about texture
        synthesis! Have you ever built something incorrectly, then
        repeatedly tried tweaking it, while failing to understand the
        underlying problem? What...no? Hmm, right, me neither...</p>
      <p>On a completely unrelated topic, here is a (<em>now finally
          working!</em>) prototype of noise pattern synthesis using
        sparse Gabor kernels. This is roughly based on the SIGGRAPH
        paper <a href="https://doi.org/10.1145/1576246.1531360">Procedural









































































































































































































































































































































































































































































































          noise using sparse Gabor convolution</a> (2009, Lagae, <em>et
          al.</em>). A preprint of that appeared while I was working on
        an earlier version of this library, so I make a <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20090509">quick









































































































































































































































































































































































































































































































          prototype</a>. It was too slow to use as-was, so I did not use
        this category of noise texture in my earlier work. Much later, I
        found another SIGGRAPH paper <a
          href="https://doi.org/10.1145/3306346.3322990">Procedural
          phasor noise</a> (2019, Tricard/Efremov, <em>et al.</em>)
        which improved on this approach to noise synthesis. That
        technique is next on my agenda.</p>
      <p>I wanted to start with the 2009 approach. This <em>GaborNoisePrototype</em>
        texture operator combines two given textures based on a scalar
        texture composed of many randomized Gabor kernels. The
        parameters are: the number of kernels (<span style="font-size:
          80%;">over a 10x10 region, whereas the renderings below have a
          diameter of 2</span>), then min and max bounds for uniform
        random selection of: kernel radius, kernel wavelength, kernel
        rotation, and lastly the two input textures:</p>
      <pre>Uniform orange   (1.0, 0.5, 0.1)
Uniform cyan_blue(0.1, 0.5, 1.0)
GaborNoisePrototype(1000,   0.3, 0.6,   0.01, 0.03,           0, pi * 0.25,   orange, cyan_blue))
GaborNoisePrototype(1000,   0.2, 0.8,   0.01, 0.03,   pi * 0.33, pi * 0.66,   orange, cyan_blue))</pre>
      <img src="images/20210328_GaborNoise_test_a.png"
        alt="20210328_GaborNoise_test_a"
        title="20210328_GaborNoise_test_a" height="511" width="511">
      &nbsp; <img src="images/20210328_GaborNoise_test_b.png"
        alt="20210328_GaborNoise_test_b"
        title="20210328_GaborNoise_test_b" height="511" width="511"> </div>
    <div class="post" id="20210326"> <a href="#20210326" class="date">March








































































































































































































































































































































































































































































































        26, 2021</a>
      <h1>Selection weights for initial random <code>GpTrees</code></h1>
      <p>Yesterday I added a new feature to LazyPredator: a new optional
        parameter to the constructor for <code>GpFunction</code> to
        adjust the likelihood a given function is selected during
        construction of the initial random population of <code>GpTrees</code>
        for a run. See details <a
          href="https://cwreynolds.github.io/LazyPredator/#20210325">here</a>.
        It seemed to me that the three “lots of spots” texture patterns
        were overrepresented. Now this might have been my own fault for
        favoring those as better camouflage. Or it could have been that
        there are three such operators (<em>LotsOfSpots</em>, <em>ColoredSpots</em>,
        and <em>LotsOfButtons</em>) and so, in combination, they were
        chosen three times more often. So in TexSyn's <code>GP::fs()</code>
        I changed these three operators to have ⅓ the chance of being
        selected. In addition “because I could” I doubled the likelihood
        of choosing <em>SoftMatte</em>, my favorite texture operator.</p>
      <p>I did a quick run to “shake down” these code changes, and in
        the process added some illustrations to the “<a
          href="https://cwreynolds.github.io/TexSyn/evo_camo_game_doc.html">doc</a>”
        for the alpha version of the <strong><code>evo_camo_game</code></strong>
        app. Step ~2000 of a run using a “clover” background is shown
        below:</p>
      <img src="images/20210325_clover_step_1982.png"
        alt="20210325_clover_step_1982"
        title="20210325_clover_step_1982" height="800" width="1200"> </div>
    <div class="post" id="20210322"> <a href="#20210322" class="date">March








































































































































































































































































































































































































































































































        22, 2021</a>
      <h1>UV lichen</h1>
      <p>This camouflage run was based on another unique image from
        helpful folks on the Facebook group <a
          href="https://www.facebook.com/groups/172285406121262">Lichens,









































































































































































































































































































































































































































































































          Mosses, Ferns and Fungi</a>. <a
          href="https://www.facebook.com/steve.axford.12">Stephen Axford</a>
        kindly gave me permission to use <a
          href="https://www.facebook.com/steve.axford.12/posts/10219530105028665">this









































































































































































































































































































































































































































































































          image</a>. He lives in Australia but travels the world to
        photograph and study fungi. (See his lovely video <a
          href="https://www.youtube.com/watch?v=KYunPJQWZ1o">autobiography</a>.)









































































































































































































































































































































































































































































































        This photo however was taken at his home. It is the
        lichen-covered trunk of a “<a
          href="https://en.wikipedia.org/wiki/Archontophoenix_cunninghamiana">Bangalow









































































































































































































































































































































































































































































































          Palm</a>” illuminated by 365nm UV (ultraviolet) light. These
        lichens (but apparently not all?) fluoresce in ultraviolet. So
        the unusual colors are from the UV and the irregular shape of
        the colored patches are due to the growth and competition for
        space between the various lichens. For context, here is <a
          href="https://www.facebook.com/photo?fbid=10219361101363679">another









































































































































































































































































































































































































































































































          view</a> from further away.</p>
      <p>Below are views of two competitive “tournaments” each showing
        three circular prey with their evolved camouflage textures. In
        the first image (at step 1374, not quite half way through the
        run) they were trying, without much success, to find a version
        of those rainbow noise patterns that would blend in the with
        background. The brightness and saturation were in the correct
        range, but the distribution of hues are off (there is no green
        in the background). Also the spatial frequencies do not match
        well.</p>
      <img src="images/20210321_step_1374.png" alt="20210321_step_1374"
        title="20210321_step_1374" height="533" width="800">
      <p>By the end of the run at step 3000, things had improved, but
        the quality of camouflage is “only OK.” The circular features
        allow coloring something like the scale of patches in the
        background. But to my eyes, their geometric regularity stands
        out and “breaks” the camouflage:</p>
      <img src="images/20210321_step_3000.png" alt="20210321_step_3000"
        title="20210321_step_3000" height="533" width="800">
      <p>Here are “thumbnail” images of interesting evolved camouflage
        textures during roughly the second half of this run, at steps
        1205, 1724, 2033, 2260, 2414, 2551, 2882, 2896, 2979, and 2983:</p>
      <img src="images/20210321_thumbnail_2983_a.png"
        alt="20210321_thumbnail_2983_a"
        title="20210321_thumbnail_2983_a" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210321_thumbnail_1205_c.png"
        alt="20210321_thumbnail_1205_c"
        title="20210321_thumbnail_1205_c" height="402" width="402"> <br>
      <br>
      <img src="images/20210321_thumbnail_1724_b.png"
        alt="20210321_thumbnail_1724_b"
        title="20210321_thumbnail_1724_b" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210321_thumbnail_2033_b.png"
        alt="20210321_thumbnail_2033_b"
        title="20210321_thumbnail_2033_b" height="402" width="402"> <br>
      <br>
      <img src="images/20210321_thumbnail_2260_a.png"
        alt="20210321_thumbnail_2260_a"
        title="20210321_thumbnail_2260_a" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210321_thumbnail_2414_c.png"
        alt="20210321_thumbnail_2414_c"
        title="20210321_thumbnail_2414_c" height="402" width="402"> <br>
      <br>
      <img src="images/20210321_thumbnail_2551_c.png"
        alt="20210321_thumbnail_2551_c"
        title="20210321_thumbnail_2551_c" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210321_thumbnail_2882_c.png"
        alt="20210321_thumbnail_2882_c"
        title="20210321_thumbnail_2882_c" height="402" width="402"> <br>
      <br>
      <img src="images/20210321_thumbnail_2896_a.png"
        alt="20210321_thumbnail_2896_a"
        title="20210321_thumbnail_2896_a" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210321_thumbnail_2979_b.png"
        alt="20210321_thumbnail_2979_b"
        title="20210321_thumbnail_2979_b" height="402" width="402"> <br>
      <br>
    </div>
    <div class="post" id="20210315"> <a href="#20210315" class="date">March








































































































































































































































































































































































































































































































        15, 2021 🎂</a>
      <h1>Parking lot trees</h1>
      <p>The ID for this camouflage run was <strong><span
            style="font-size: 90%;"><code>tree_leaf_blossom_sky_20210313_1907</code></span></strong>
        but I think of them as “parking lot trees” — small trees in
        concrete planter areas at the end of each row of parking places.
        The photos were taken from below, so the background is deep sky
        blue. Also visible are tree branches, leaves of various colors,
        and small white blossoms. (Photos taken on December 26, 2017,
        probably in Foster City, California.)</p>
      <p>Keep in mind: <strong>this is a purely 2d model</strong> of
        camouflage. Since these background images show a strongly 3d
        scene, this causes some cognitive dissonance. It would be as if
        there was a photographic paper print of the background, lying on
        a desk. Paper disks, printed with synthetic texture, are placed
        on top of the photo, then the user make a judgement on which is
        most conspicuous. The fact that sky blue color could be
        incorporated into these evolved camouflage patterns is related
        to this “purely 2d” abstraction. (Sky blue is rare (to say the
        least) in nature but has been used in the past for military <a
          href="https://imgur.com/gallery/Uhbhe">aircraft camouflage</a>.)</p>
      <p>This run was composed of <strong>3000 steps</strong>. Below
        are two “tournament” images showing the whole window and three
        competing camouflaged prey textures. These are at steps 2092 and
        2596 of the run, shown at ⅔ of original size. I hand-selected
        these steps based on all three prey being well camouflaged. And
        by that I mean a combination of good quality camouflage and the
        luck of being placed on a part of the background where it is
        effective. </p>
      <p>In the first image the three prey use similar camouflage: a
        black background (similar to the shadowed tree limbs) with blue
        spots (which mimic the sky showing through bits of the tree) and
        over that a tracing of edgy green and white patterns (suggesting
        leaves and blossoms):</p>
      <img src="images/20210313_step_2092.png" alt="20210313_step_2092"
        title="20210313_step_2092" height="533" width="800">
      <p>In the second image, the upper right prey uses that same
        approach, while the other two are variants of a new line of
        phenotypes. These have striped patterns alternating between two
        complex textures, with irregular “wiggly” edges between stripes:</p>
      <img src="images/20210313_step_2596.png" alt="20210313_step_2596"
        title="20210313_step_2596" height="533" width="800">
      <p>Shown below are “thumbnail” images of an individual camouflaged
        prey, and a bit of its surrounding background for context. These
        were hand selected by similar criteria—good quality camouflage
        in effective locations—but in isolation, without regard to the
        other prey in the tournament. These are in order of
        “evolutionary time” at step: 131, 257, 1332, 1593, 1858, 2270,
        2571, 2662, 2845, and 2976:</p>
      <img src="images/20210313_thumbnail_131_c.png"
        alt="20210313_thumbnail_131_c" title="20210313_thumbnail_131_c"
        height="402" width="402"> &nbsp;&nbsp; <img
        src="images/20210313_thumbnail_257_b.png"
        alt="20210313_thumbnail_257_b" title="20210313_thumbnail_257_b"
        height="402" width="402"> <br>
      <br>
      <img src="images/20210313_thumbnail_1332_b.png"
        alt="20210313_thumbnail_1332_b"
        title="20210313_thumbnail_1332_b" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210313_thumbnail_1593_a.png"
        alt="20210313_thumbnail_1593_a"
        title="20210313_thumbnail_1593_a" height="402" width="402"> <br>
      <br>
      <img src="images/20210313_thumbnail_1858_a.png"
        alt="20210313_thumbnail_1858_a"
        title="20210313_thumbnail_1858_a" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210313_thumbnail_2270_a.png"
        alt="20210313_thumbnail_2270_a"
        title="20210313_thumbnail_2270_a" height="402" width="402"> <br>
      <br>
      <img src="images/20210313_thumbnail_2571_b.png"
        alt="20210313_thumbnail_2571_b"
        title="20210313_thumbnail_2571_b" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210313_thumbnail_2662_c.png"
        alt="20210313_thumbnail_2662_c"
        title="20210313_thumbnail_2662_c" height="402" width="402"> <br>
      <br>
      <img src="images/20210313_thumbnail_2845_b.png"
        alt="20210313_thumbnail_2845_b"
        title="20210313_thumbnail_2845_b" height="402" width="402">
      &nbsp;&nbsp; <img src="images/20210313_thumbnail_2976_a.png"
        alt="20210313_thumbnail_2976_a"
        title="20210313_thumbnail_2976_a" height="402" width="402"> </div>
    <div class="post" id="20210307"> <a href="#20210307" class="date">March








































































































































































































































































































































































































































































































        7, 2021</a>
      <h1>Bilateral symmetry</h1>
      <p>One more “thumbnail” from the run on the <code>fungus_pores_2</code>
        background. One of the “edgy” phenotype, not very effective as
        camouflage, but interesting because it happens to exhibit
        bilateral symmetry. This run was started before I made the
        thumbnail-saving utility also save the&nbsp; source code text of
        an <code>Individual</code>'s <code>GpTree</code>. So I don't
        know for sure, but it looks like the root of the tree, the
        outermost texture operator, was <em>Mirror</em> (see <a
          href="#20200309">here</a>) with parameters that caused the
        line of symmetry to pass very close to the center of this
        texture.</p>
      <p>I sometimes think of these disks of camouflaged textures as
        something like a round beetle (like a “<a
href="https://upload.wikimedia.org/wikipedia/commons/a/af/Harmonia_axyridis_-_botanischer_Garten_Sch%C3%B6nbrunn.jpg">ladybug</a>”)









































































































































































































































































































































































































































































































        crawling on a textured surface. But the coloration of beetles,
        like so many other species, has bilateral symmetry. This post is
        primarily to remind myself that sometime I should try a
        camouflage run where bilateral symmetry is imposed on top of the
        evolved texture pattern. Beyond visual interest, it would add
        another “twist” to the search for effective camouflage, since
        symmetry is often a clue that helps to break camouflage.
        Sometimes a well-camouflaged animal is given away only by its
        symmetry. (Such as in this <a
href="https://static.boredpanda.com/blog/wp-content/uuuploads/animal-camouflage-photography-art-wolfe/animal-camouflage-photography-art-wolfe-1.jpg">amazing









































































































































































































































































































































































































































































































          owl photo</a>, by the excellent nature photographer <a
          href="https://artwolfe.com/">Art Wolfe</a>. Buy his fantastic
        book <a
href="https://bookshop.org/books/vanishing-act-the-artistry-of-animal-camouflage/9781937359669">Vanishing









































































































































































































































































































































































































































































































          Act: The Artistry of Animal Camouflage</a> originally
        published 2005 in hardcover.)</p>
      <img src="images/20210303_fungus_pores_2_1358_a.png" alt=""
        title="" height="402" width="402"> </div>
    <div class="post" id="20210305"> <a href="#20210305" class="date">March








































































































































































































































































































































































































































































































        5, 2021</a>
      <h1>Hiding among the fungus pores</h1>
      <p>First of all, a big thank you to Colin Brown a(n amateur?)
        mycologist and photographer. I saw his photography in the
        Facebook group <a
          href="https://www.facebook.com/groups/172285406121262">Lichens,









































































































































































































































































































































































































































































































          Mosses, Ferns and Fungi</a>. He posted a close-up of the pores
        on the bottom of the mushroom <em>Trametes aesculi</em>, and a
        couple of weeks later, <em>Trametes gibbosa</em>, both taken in
        Rock Creek, West Virginia, USA. He kindly gave me permission to
        use his <em>aesculi</em> image for my experiments and I assumed
        the same would go for <em>gibbosa</em>. I don't know a lot
        about fungus, but most mushrooms have “gills” under their caps.
        Whereas these species are polypore mushrooms with small
        holes/slots under the cap. In both cases, these are where the
        fungus releases its powder-like reproductive spores. (I've since
        learned there is an intermediate form called <a
href="https://commons.wikimedia.org/w/index.php?search=Daedalea+quercina&amp;title=Special%3ASearch&amp;go=Go&amp;ns0=1&amp;ns6=1&amp;ns12=1&amp;ns14=1&amp;ns100=1&amp;ns106=1#/media/File:Daedalea_quercina_%2825969437344%29.jpg">mazegill</a>.)</p>
      <p>What jumped out at me is how similar these are to the labyrinth
        variety of abstract <a
          href="https://www.google.com/search?tbm=isch&amp;q=turing+textures">Turing









































































































































































































































































































































































































































































































          textures</a> (Turing instabilities, reaction-diffusion
        textures). I wanted to try evolving a camouflage for a critter
        trying to hide on these labyrinth patterns.</p>
      <p>While useful experiments, I felt the resulting camouflage was
        not particularly successful. Both of these backgrounds have
        contrasty “large scale” features, relative to the “body size”
        (diameter) of the camouflaged “prey” individuals. Neither run
        managed to convincingly mimic the background texture. Even if
        they had, misalignment of contrasty features serves to visually
        emphasize the boundary of the camouflaged “body” so making the
        camouflage less effective.</p>
      <p>This run, using the <em>Trametes gibbosa</em> photo, developed
        two phenotypes. One had a very edgy noise pattern that often
        seemed to <em>almost</em> align with background features along
        its boundary, but its interior was not much like the background.
        (See first image below, of a tournament with three individuals
        of this first phenotype, at ⅔ of original size.) The other
        phenotype was a network of wiggly light and dark features that <em>somewhat</em>
        mimics the background, but not very well.</p>
      <img src="images/20210303_fungus_pores_2_1188.png"
        alt="20210303_fungus_pores_2_1188"
        title="20210303_fungus_pores_2_1188" height="533" width="800">
      <p>Here are “thumbnails” of three individuals of both types:</p>
      <img src="images/20210303_fungus_pores_2_920_a.png"
        alt="20210303_fungus_pores_2_920_a"
        title="20210303_fungus_pores_2_920_a" height="402" width="402">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210303_fungus_pores_2_1014_b.png"
        alt="20210303_fungus_pores_2_1014_b"
        title="20210303_fungus_pores_2_1014_b" height="402" width="402">
      <br>
      <br>
      <img src="images/20210303_fungus_pores_2_1032_c.png"
        alt="20210303_fungus_pores_2_1032_c"
        title="20210303_fungus_pores_2_1032_c" height="402" width="402">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210303_fungus_pores_2_1318_c.png"
        alt="20210303_fungus_pores_2_1318_c.png"
        title="20210303_fungus_pores_2_1318_c.png" height="402"
        width="402"> <br>
      <br>
      <img src="images/20210303_fungus_pores_2_1403_a.png"
        alt="20210303_fungus_pores_2_1403_a"
        title="20210303_fungus_pores_2_1403_a" height="402" width="402">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210303_fungus_pores_2_1545_a.png"
        alt="20210303_fungus_pores_2_1545_a"
        title="20210303_fungus_pores_2_1545_a" height="402" width="402">
      <p><br>
        Below are three cropped tournament groups (⅔ of original screen
        size) from an earlier run with the <em>Trametes aesculi</em>
        photo. Throughout an extended run of 5000 steps it failed to
        develop effective camouflage textures. At various times it would
        do a good job of matching the light or dark browns of the
        background, or the spatial frequencies, or the wiggly quality of
        the background. But it never got all those things right at the
        same time.</p>
      <img src="images/20210208_fungus_pores_4639.png"
        alt="20210208_fungus_pores_4639"
        title="20210208_fungus_pores_4639" height="350" width="573"> <br>
      <br>
      <img src="images/20210208_fungus_pores_5036.png"
        alt="20210208_fungus_pores_5036"
        title="20210208_fungus_pores_5036" height="353" width="674"> <br>
      <br>
      <img src="images/20210208_fungus_pores_5130.png"
        alt="20210208_fungus_pores_5130"
        title="20210208_fungus_pores_5130" height="313" width="548"> <br>
      <br>
    </div>
    <div class="post" id="20210222"> <a href="#20210222" class="date">February









































































































































































































































































































































































































































































































        22, 2021</a>
      <h1>One more run on “oak leaf litter”</h1>
      <p>Not that anyone cares, but this is out of order. I will soon
        post images from a run on a different background. Recent code
        edits include changes to placement of textures on background for
        camouflage. (This is prep for saving “thumbnail” images of
        textures with clean backgrounds.) I also turned up the default
        “evolution power” for camouflage by 20%. (Previously it used a
        population of 100 individuals in 5 breeding subpopulations
        (demes), now it is 120 individuals in 6 subpopulations. These
        parameters can be overridden in the command line version.) </p>
      <p>Otherwise the run shown below is like the previous two. I ran
        it for almost 2000 steps. It seemed to develop some effective
        camouflage patterns. It seemed to have a “healthy” diverse
        population, which continued to invent new patterns throughout
        the run.</p>
      <img src="images/20210220_oak_leaf_800.png"
        alt="20210220_oak_leaf_800" title="20210220_oak_leaf_800"
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210220_oak_leaf_939.png"
        alt="20210220_oak_leaf_939" title="20210220_oak_leaf_939"
        height="400" width="400">
      <p> </p>
      <img src="images/20210220_oak_leaf_985.png"
        alt="20210220_oak_leaf_985" title="20210220_oak_leaf_985"
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210220_oak_leaf_1047.png"
        alt="20210220_oak_leaf_1047" title="20210220_oak_leaf_1047"
        height="400" width="400">
      <p> </p>
      <img src="images/20210220_oak_leaf_1338.png"
        alt="20210220_oak_leaf_1338" title="20210220_oak_leaf_1338"
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210220_oak_leaf_1353.png"
        alt="20210220_oak_leaf_1353" title="20210220_oak_leaf_1353"
        height="400" width="400">
      <p> </p>
      <img src="images/20210220_oak_leaf_1376.png"
        alt="20210220_oak_leaf_1376" title="20210220_oak_leaf_1376"
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210220_oak_leaf_1519.png"
        alt="20210220_oak_leaf_1519" title="20210220_oak_leaf_1519"
        height="400" width="400">
      <p> </p>
      <img src="images/20210220_oak_leaf_1569.png"
        alt="20210220_oak_leaf_1569" title="20210220_oak_leaf_1569"
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210220_oak_leaf_1793.png"
        alt="20210220_oak_leaf_1793" title="20210220_oak_leaf_1793"
        height="400" width="400">
      <p> </p>
      <img src="images/20210220_oak_leaf_1902.png"
        alt="20210220_oak_leaf_1902" title="20210220_oak_leaf_1902"
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210220_oak_leaf_1932.png"
        alt="20210220_oak_leaf_1932" title="20210220_oak_leaf_1932"
        height="400" width="400">
      <p> </p>
    </div>
    <div class="post" id="20210209"> <a href="#20210209" class="date">February









































































































































































































































































































































































































































































































        9, 2021</a>
      <h1>Another prototype camouflage run</h1>
      <p>After adding handling of “argc/argv” unix-style command line
        arguments to the <code>Camouflage</code> class, I made another
        run using that form of specification. (Previously I mostly
        fiddled with parameters inside my code via my IDE and
        recompiling.) The results were pretty good, entertaining even,
        as shown below. For the previous week or so I was looking into
        ways to distribute binaries of the tool but ran into some
        roadblocks. For the time being, the only way to use it is to
        build it yourself from sources. In case anyone reading this is
        interested, please contact <a href="https://www.red3d.com/cwr/">me</a>
        for details.</p>
      <p>I used the same set of background images (“oak leaf litter
        green brown”) but chose a different seed for the psuedo-random
        sequence generator. I need to add a command line argument for
        specifying that. This software is designed to be strongly
        reproducible (even across platforms) despite being based on
        randomized algorithms. The results will be identical if you make
        two runs with all the same parameters (including, in this case
        of interactive evolution of camouflage, making the same
        interactive selections). If you <em>want</em> the runs to be
        different, changing the random seed is required.</p>
      <p>Near the top of my to-do list is better tools for logging these
        camouflage runs, and recording images from the run. Initially I
        want to have a way to say “save the whole window showing the
        current tournament of three textures” and “save an image of the
        indicated texture along with its nearby background.” I may also
        want to add the ability to save the program (<code>GpTree</code>)
        for a texture. So far I have just been using my laptop's screen
        capture utility to collect these images. Here is a cropped image
        of a tournament at around step 700:</p>
      <img src="images/20210208_2255_oak_leaf.png" alt="" title=""
        height="762" width="882">
      <p>Some examples of cryptic (well camouflaged) “prey”, from later
        in the run, with a bit of the background texture on which they
        were found:</p>
      <img src="images/20210208_2257_oak_leaf.png" alt="" title=""
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210209_1519_oak_leaf.png" alt="" title=""
        height="400" width="400">
      <p> </p>
      <img src="images/20210209_1542_oak_leaf.png" alt="" title=""
        height="400" width="400">&nbsp;&nbsp;&nbsp;&nbsp;<img
        src="images/20210209_1658_oak_leaf.png" alt="" title=""
        height="400" width="400"> <br>
      <br>
      <p><strong>A side note:</strong> as you may recall, the user's
        task in this “game” is to pick out the texture which is “worst”
        — the <em>least</em> well camouflaged. In the beginning of a
        run this is difficult since almost all of the textures are bad:
        they stand out against the background and are not at all
        camouflaged. It can be hard to decide which of the bad textures
        are the worst. Early in a run there are lots of textures which
        are just a single uniform color. While selecting for non-uniform
        textures, I seem to have given too much of a boost to a species
        of multi-spot patterns. They were fun to look at but not very
        well camouflaged:</p>
      <img src="images/20210208_1919_oak_leaf.png" alt="" title=""
        height="400" width="400"> <br>
      <br>
      <img src="images/20210208_1937_oak_leaf.png" alt="" title=""
        height="663" width="714"> <br>
      <br>
      <img src="images/20210208_1946_oak_leaf.png" alt="" title=""
        height="684" width="714"> <br>
      <p>Here is a point where the “spot” species is doing its best to
        be camouflaged, but losing the race to two different
        noise-pattern-based species are starting to perform better:</p>
      <img src="images/20210208_1957_oak_leaf.png" alt="" title=""
        height="416" width="832"> </div>
    <div class="post" id="20210129"> <a href="#20210129" class="date">January









































































































































































































































































































































































































































































































        29, 2021</a>
      <h1>Early interactive evolution of camouflage</h1>
      <p>Since the last entry here, there was some work in LazyPredator
        on tree size limits, then for the last two weeks I've been
        prototyping a user interface for “interactive evolution of
        camouflage”, the subject of my <a
          href="https://www.red3d.com/cwr/iec/">2011 paper</a>. My work
        on TexSyn and LazyPredator has been aimed as restarting that
        line of research with a more robust software foundation. <a
          href="https://rebeccaallen.com/">Rebecca Allen</a>, my
        friend-since-grad-school, gave me a nudge to get back to
        camouflage already. My first thought was that there was still
        lots of infrastructure work to do, but then decided the time was
        about right.</p>
      <p>And indeed, except for a small tweak in LazyPredator, most of
        the work has been on the GUI side. Two weeks in, I have an
        “almost working” version of the 2010 concept of interactive
        evolution of camouflage. It is based on an instance of
        coevolution in nature. Imagine a type of predator that uses
        vision to locate their prey—and prey who use camouflage to
        escape detection. Effective camouflage helps prey survive to
        reproduce and pass on their genes. Effective “camouflage
        breaking” allows predators to eat and so survive to reproduce
        and pass on <em>their</em> genes. These adversarial processes
        coevolve, producing mutual improvement, leading to predators
        with excellent visual perception, and prey with sometimes
        astoundingly effective camouflage.</p>
      <p>In this simple simulation—essentially half of the larger goal—a
        human user plays the part of the predator. The evolutionary
        texture synthesis software plays the part of the prey whose
        camouflage is evolving. This can be thought of as a “game” where
        the human player competes with the simulation. Each turn of the
        game corresponds to one tournament/step in the steady-state
        evolutionary computation: </p>
      <ul>
        <li>A random background image is chosen to represent the
          environment.</li>
        <ul>
          <li>A random choice is made from a set of photographs, then a
            random rectangle is selected in that photo.</li>
          <li>In the examples below I am using six photos of oak leaf
            litter, shot on the side of a road near my house last
            August.</li>
        </ul>
        <li>Three unique textures are selected at random from the
          LazyPredator population.</li>
        <li>The three textures are placed on the background in random,
          non-overlapping positions.</li>
        <li>The software waits for the user/player to indicate (with a
          mouse click) which of the three is most conspicuous — least
          well camouflaged.</li>
        <li>This texture “loses” the tournament, and is replaced in the
          population by a new offspring, a crossover whose parents are
          the other two textures in the tournament.</li>
      </ul>
      <p>Each time I made a code change, I would play a few rounds of
        the game to make sure everything was still working as expected.
        This time I got to step 70 and the texture in the upper left
        caught my eye:</p>
      <img src="images/20210129_1746_oak_leaf.png" alt="oak leaf test,
        step 70" title="oak leaf test, step 70" height="598"
        width="1051">
      <p>Hmm, that is not bad. The range of colors in that texture were
        pretty similar to the background image. The spatial frequencies
        in that texture were pretty similar to the background image. In
        fact one could say that texture is pretty well camouflaged
        against these backgrounds.</p>
      <p>In the previous steps of the evolutionary selection process,
        textures that were “obviously wrong” were being weeded out of
        the population. For example, textures with the wrong colors
        (like the multi-colored middle texture above) or had spatial
        frequencies that were too high or too low (like the flat beige
        texture with gray spots on the right), were removed and replaced
        with new offspring. In one surprising step, GP crossover created
        a new texture that had <strong>both</strong> good distributions
        of colors and frequency pattern. Dang!</p>
      <p>To be clear, I think this is “pretty good” rather than
        “excellent” camouflage. The brightness is a bit too high and/or
        the contrast range is not wide enough. The evolved texture lacks
        details as dark as some of the shadows in the background
        photograph. The ability to mechanistically determine the quality
        of camouflage is “future work” in this study. </p>
      <p>I continued to “evolve” this run for another 200 steps or so.
        In this tournament we see a nicely cryptic texture (lower left)
        and two under-performers. The middle texture seems to be a
        uniform color with “buttons” of a green/beige texture. Something
        similar seems to be a component of the camouflage texture in the
        lower left.</p>
      <img src="images/20210129_1849_oak_leaf.png" alt="oak leaf test A"
        title="oak leaf test A" height="713" width="927">
      <p>Later in the run, the well camouflaged textures are enjoying
        “reproductive success.” Their good genes are spreading through
        the population. Below is a turn of the game where all three
        textures in a tournament are variations on the same successful
        cryptic texture. If you look closely at the upper left texture,
        you can see an important property of effective camouflage, some
        of its boundary/silhouette seems to “dissolve” away. Instead of
        a hard-edged circle, parts now look nibbled away:</p>
      <img src="images/20210129_1838_oak_leaf.png" alt="oak leaf test B"
        title="oak leaf test B" height="608" width="781">
      <p>Just for a little context, the images above are cropped out of
        the full GUI. This shows (at a smaller scale) the whole window,
        full screen on my laptop, relative to three textures:</p>
      <img src="images/20210129_1827_oak_leaf.png" alt="oak leaf test,
        full window" title="oak leaf test, full window" height="326"
        width="511"> </div>
    <div class="post" id="20210110"> <a href="#20210110" class="date">January









































































































































































































































































































































































































































































































        10, 2021</a>
      <h1>LimitHue results from LazyPredator tests</h1>
      <p>I continue to use the “<code>LimitHue</code>” fitness function
        for testing recent changes to LazyPredator (see posts in its
        blog from <a
          href="https://cwreynolds.github.io/LazyPredator/#20201223">December









































































































































































































































































































































































































































































































          23</a> through <a
          href="https://cwreynolds.github.io/LazyPredator/#20210105">January









































































































































































































































































































































































































































































































          5</a>). A few of them are visually interesting, especially as
        better optimization leads to higher fitness. To recap: <code>LimitHue</code>
        prefers textures which contain primarily four distinct hues
        (using a discrete 12 step hue space), wanting those hues to be
        “non adjacent” (meaning hues 1, 2, 3, 4 are not as good as hues
        1, 3, 5, 7), and close to zero hues in the other 8 “buckets.” In
        addition, there are fitness penalties for very bright or very
        dark colors, insufficiently saturated colors, and textures with
        too much high frequency (“confetti”) noise.</p>
      <p>Here are two results from a test of subpopulations (“demes”) in
        a <code>LimitHue::comparison()</code> run called <code>LimitHue_comparison_20210105</code>.
        On the left, with a fitness of 0.956924 and on the right with
        fitness of 0.904232, both from the condition using four
        subpopulations (versus the control condition of a single
        population):</p>
      <img src="images/20210105_1839_1_b_LimitHue.png"
        alt="1_b_20210105_1839_LimitHue.png"
        title="1_b_20210105_1839_LimitHue.png" height="511" width="511">
      <img src="images/20210106_0018_3_b_LimitHue.png"
        alt="3_b_20210106_0018_LimitHue.png"
        title="3_b_20210106_0018_LimitHue.png" height="511" width="511">
      <p>And these two results from <code>LimitHue_comparison_20210103</code>
        run. On the left, with fitness 0.932506 (from the single
        population condition). It captures the “mostly flat regions of
        constant vivid color” I was picturing when I designed <code>LimitHue</code>.
        On the right, with fitness 0.94534 (from the four subpopulation
        condition) which seems to be near the lower bound on acceptable
        saturation level:</p>
      <img src="images/20210103_1210_1_a_LimitHue.png"
        alt="1_a_20210103_1210_LimitHue.png"
        title="1_a_20210103_1210_LimitHue.png" height="511" width="511">
      <img src="images/20210103_1722_4_b_LimitHue.png"
        alt="4_b_20210103_1722_LimitHue.png"
        title="4_b_20210103_1722_LimitHue.png" height="511" width="511">
    </div>
    <div class="post" id="20201203"> <a href="#20201203" class="date">December









































































































































































































































































































































































































































































































        3, 2020</a>
      <h1>Recent infrastructure work</h1>
      <p>During the HueLimit runs described below on <a
          href="#20201118">November 18</a>, a nagging problem was that
        the virtual memory partition of TexSyn grew at a significant
        rate, indicating a “memory leak.” My laptop could run for 12-18
        hours before exhausting its memory partition at about 50 GB. I
        did know that I had prototyped <code>GpTree::eval()</code> to
        construct <code>Texture</code> objects on the heap and realized
        they were not being freed/deleted when the <code> Individual</code>
        containing them was deleted. The <code>Texture</code> objects
        themselves are pretty small, but I assumed the OpenCV <code>cv::Mat</code>
        (in which Textures store their rendered pixels for display and
        file output) were not getting deleted, causing the leak.</p>
      <p>Turns out that was <strong>not</strong> the problem, it was
        actually a memory leak in OpenCV itself, for each window it
        opens on macOS. (See my OpenCV <a
          href="https://github.com/opencv/opencv/issues/18962">bug
          report</a>.) But I didn't track <em>that</em> down until I
        put in the <code>GpTree::deleteCachedValues()</code> to free
        those <code>Texture</code> instances created during <code>eval()</code>.
        But to do <em>that</em> I had to track down a very confusing
        bug which happened to be introduced by a typo in <code>GP.h</code>
        where the specification for the <code>GpFunction</code> for <em>CotsMap</em>
        indicated an extra parameter of type <code>Texture</code>. This
        caused the GpTree to have a “phantom limb” which produced
        symptoms taking me a long time to understand.</p>
      <p>Having diagnosed the memory leak, the fix was simply to avoid
        opening so many windows. The many windows came from my
        goofy-prototype-hack of a user interface which opened and closed
        thousands of windows. I needed to do the obvious and normal
        thing: to have a single window, and update it as required. I
        also wanted to be able to put text labels on that GUI (graphical
        user interface) control panel. So I started digging into
        OpenCV's support for drawing text on images. That led to finding
        another <a href="https://github.com/opencv/opencv/issues/19005">bug</a>,
        which led me to reconsider a decision I'd made almost a year ago
        about storing Textures in <code>cv::Mats</code> of type <code>CV_32FC3</code>.
        That is, my image data represents the color of each pixel as
        three 32 bit floating point numbers (representing red, green,
        and blue) for a total of <strong>96</strong> bits per pixel.
        Its is <strong>much</strong> more common to use <strong>24</strong>
        bits per pixel with an 8 bit byte to represent red, green, and
        blue (aka <code>CV_8UC3</code>). I have made the change and can
        see no difference, since my screen used 24 bits per pixel color
        space. At the moment I don't know how to verify there has been
        no change. I was going to post some images here, but as far as I
        can tell, they are identical so there is nothing to see.</p>
    </div>
    <div class="post" id="20201118"> <a href="#20201118" class="date">November









































































































































































































































































































































































































































































































        18, 2020</a>
      <h1><em>HueLimit</em> test for “absolute” fitness</h1>
      <p>Preparing some “work in progress” <a
href="http://www.red3d.com/cwr/presentations/20201118_TexSyn_LazyPredator.pdf">slides</a>
        for a presentation at Dan Shapiro's UCSC class, I made an
        extended series of runs of the <code>HueLimit</code> evolution
        test. I was a bit disappointed with previous runs using
        “multi-objective fitness functions” — related to <a
          href="https://en.wikipedia.org/wiki/Pareto_efficiency">Pareto
          optimality</a>. I had been time-multiplexing the various
        fitness components, for example to optimize for both A and B, I
        would randomly choose between optimizing either A or B on each
        evolution step. The results seemed a little “weak” — the
        population improved according to both A and B, but were not
        exceptionally good at either.</p>
      <p>So I backed up to some previous work in this area (<a
          href="http://www.red3d.com/cwr/gots/">Goal Oriented Texture
          Synthesis</a>, 2011-2013) and followed the approach used
        there. The multiple fitness measures are mapped onto the range
        [0, 1] and then multiplied together for a final numerical
        fitness. This is “absolute fitness” that can be measured for a
        single individual in isolation. (As opposed to “relative
        fitness” measured in competitive tournaments.) The fitness
        components can be “weighted” by limiting the portion of range
        they occupy. For example a component on the entire [0, 1] range
        is “strong” while one remapped to [0.9, 1] has only a mild
        strength.</p>
      <p>The <code>HueLimit</code> fitness function was primarily based
        on limiting hue distribution. Each TexSyn texture was randomly
        sampled, and each sample was assigned to one of twelve “buckets”
        in a hue histogram. A perfect fitness was to have exactly four
        of the twelve hue buckets with ¼ of the samples each, with the
        other eight buckets empty. Deviation from this pattern reduced
        the fitness. In addition two other minor fitness components
        required the texture have average saturation values above 0.5,
        and for the average brightness to be in the middle 80% of the
        brightness range (basically, neither black nor white). The
        result, then is a “random texture” which meets certain
        constraints defined by the fitness function. I made about 50
        runs like this, saving a render of the best individual at the
        end of each run. Then I hand selected the textures that seemed
        most visually interesting to me. Each <code>HueLimit</code> run
        has a population of 100 individuals, with max initial size of
        100, and 1000 evolution steps (10 “generation equivalents”).
        These each took about 15 minutes to run on my laptop.</p>
      <img src="images/20201113_0806_abs_fit.png"
        alt="20201113_0806_abs_fit" title="20201113_0806_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201115_0833_abs_fit.png"
        alt="20201115_0833_abs_fit" title="20201115_0833_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201113_0851_abs_fit.png"
        alt="20201113_0851_abs_fit" title="20201113_0851_abs_fit"
        height="340" width="340"> <img
        src="images/20201114_1100_abs_fit.png"
        alt="20201114_1100_abs_fit" title="20201114_1100_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201114_1111_abs_fit.png"
        alt="20201114_1111_abs_fit" title="20201114_1111_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201114_0927_abs_fit.png"
        alt="20201114_0927_abs_fit" title="20201114_0927_abs_fit"
        height="340" width="340"> <img
        src="images/20201115_1224_abs_fit.png"
        alt="20201115_1224_abs_fit" title="20201115_1224_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201115_0328_abs_fit.png"
        alt="20201115_0328_abs_fit" title="20201115_0328_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201116_0858_abs_fit.png"
        alt="20201116_0858_abs_fit" title="20201116_0858_abs_fit"
        height="340" width="340"> <img
        src="images/20201114_0118_abs_fit.png"
        alt="20201114_0118_abs_fit" title="20201114_0118_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201114_1129_abs_fit.png"
        alt="20201114_1129_abs_fit" title="20201114_1129_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201115_0617_abs_fit.png"
        alt="20201115_0617_abs_fit" title="20201115_0617_abs_fit"
        height="340" width="340"> <img
        src="images/20201113_1152_abs_fit.png"
        alt="20201113_1152_abs_fit" title="20201113_1152_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201114_1218_abs_fit.png"
        alt="20201114_1218_abs_fit" title="20201114_1218_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201114_0439_abs_fit.png"
        alt="20201114_0439_abs_fit" title="20201114_0439_abs_fit"
        height="340" width="340"> <img
        src="images/20201114_0214_abs_fit.png"
        alt="20201114_0214_abs_fit" title="20201114_0214_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201114_1146_abs_fit.png"
        alt="20201114_1146_abs_fit" title="20201114_1146_abs_fit"
        height="340" width="340">&nbsp;<img
        src="images/20201114_1127_abs_fit.png"
        alt="20201114_1127_abs_fit" title="20201114_1127_abs_fit"
        height="340" width="340"> </div>
    <div class="post" id="20201104"> <a href="#20201104" class="date">November









































































































































































































































































































































































































































































































        4, 2020</a>
      <h1>Hard limit on <code>LotsOfSpotsBase</code> constructor
        runtime</h1>
      <p>Back around <a href="#20200403">April 3</a> when I started
        working on the <em>LotsOfSpots</em> family of texture operators
        (with <em>ColoredSpots</em> and <em>LotsOfButtons</em>, all
        now derived from <code>LotsOfSpotsBase</code>) it was hard to
        know how they would actually get used in evolutionary
        optimization. A texture render time that is “ok” in ones and
        twos starts to become uncomfortable in the thousands. On <a
          href="#20200927">September 27</a> I used a big hammer to
        prevent unreasonable runtime while rendering textures with
        nested <em>Blur</em> operations. Recently I have found that the
        embarrassingly long pauses during evolution test runs are caused
        by the constructor for <code>LotsOfSpotsBase</code> which
        builds the random pattern of “spots” then runs a relaxation
        process to reduce any overlap between these randomly places
        spots. The basic problem is an O(<em>n²</em>) algorithm, and a
        possibility that <em>n</em> can get fairly large, depending on
        the interactions of several other parameters. There were 11,000
        “spots” in one case that ran so long I paused the run to poke
        around with my debugger. (It is not literally an O(<em>n²</em>)
        algorithm, but not worth getting into here.)</p>
      <p>I decided that I ought to put a hard ceiling on the number of
        spots in these textures. It is in a global variable, so can be
        changed, or set to “infinity” if needed, but by default I have
        decide there will be no more than 5000 spots. Similarly I put
        the parameter controlling the max number of relaxation steps in
        a global variable. Its value remains 200. To work on this I
        defined a test case whose parameters produce a large number of
        spots:</p>
      <pre>spots = LotsOfSpots(0.95,               // density
                    0.02,               // min radius
                    0.05,               // max radius
                    0.01,               // soft edge width
                    0.01,               // margin
                    Uniform(1, 1, 0),   // spot color
                    Uniform(0, 0, 0.8)) // background color</pre>
      <p>The high density combined with the small radii lead to a
        texture with 21694 spots, which it tries up to 200 times to
        “relax” any overlaps. On my laptop that ran for about 6.7
        seconds. For reference here is that <code>spots</code> texture,
        and it “zoomed out” with <code>Scale(0.2, spots)</code> to see
        the whole of the tiling texture. (Normally the tile is five
        times larger than the default render range of [-1, +1], so the
        second image show the full tiling range of [-5, +5]:</p>
      <img src="images/20201104_spots_1_0.png" alt="spots" title="spots"
        height="511" width="511">&nbsp; <img
        src="images/20201104_spots_0_2.png" alt="Scale(0.2, spots)"
        title="Scale(0.2, spots)" height="511" width="511">
      <p>Those were rendered with:</p>
      <pre>LotsOfSpotsBase::max_spots_allowed = std::numeric_limits&lt;int&gt;::max();</pre>
      <p>which is the new way of getting the old behavior. The two
        corresponding images below are the exact same texture
        specification, but with the new default <code>max_spots_allowed</code>
        of 5000:</p>
      <img src="images/20201104_limit_spots_1_0.png" alt="spots (with
        limit)" title="spots (with limit)" height="511" width="511">&nbsp;








































































































































































































































































































































































































































































































      <img src="images/20201104_limit_spots_0_2.png" alt="Scale(0.2,
        spots) (with limit)" title="Scale(0.2, spots) (with limit)"
        height="511" width="511">
      <p>While the original version with 21694 spots took 6.7 seconds,
        this version with 5000 spots took 0.36 seconds. Still kind of
        slow, but I am willing to live with it for now. Note that the
        overlap minimization process runs multi-threaded, so typically
        runs up to eight threads on my laptop.</p>
      <p><br>
        <strong>Unfortunately</strong>, in the process of working on
        this change, I observed an annoying artifact, which I assume is
        a bug. The texture below is like the second one above (in the
        older no-limit case) but with a smaller scale: 0.09 versus 0.2.
        In this case, the boundaries of the tiling patter become
        visible. (In this view, one full tile is in the center with
        eight partial tiles around it. The artifact forms a subtle “#”
        pattern.) The <code>LotsOfSpotsBase</code> class goes to
        considerable effort to form a pattern which tiles seamlessly.
        Given I have rendered many zoomed-out spots textures before
        without obvious artifacts (e.g. on <a href="#20200419">April 19</a>)
        I suspect this may have to do either with the relatively small
        size of these spots, or the <code>margin</code> (spacing
        between spots) parameter which was added late in the development
        of the “spots family” of operators. I've put this issue on the
        bug list.</p>
      <pre>LotsOfSpotsBase::max_spots_allowed = std::numeric_limits&lt;int&gt;::max()
Scale(<strong>0.09</strong>, spots)</pre>
      <img src="images/20201104_spots_0_09.png" alt="annoying artifact"
        title="annoying artifact" height="511" width="511"> </div>
    <div class="post" id="20201027"> <a href="#20201027" class="date">October









































































































































































































































































































































































































































































































        27, 2020</a>
      <h1>Gamma overflow</h1>
      <p>While experimenting with evolution and the structure of
        tournament selection in LazyPredator, I ran into a TexSyn bug.
        In one run, after about 5000 steps I hit an <code>assert</code>
        failure in <code>Color::convertRGBtoHSV()</code>. The problem
        was an <code>inf</code> as one of the input RGB components.
        This was coming from the output of a <em>Gamma</em> texture
        operator, which had raised ~200,000 to the ~8.5 power which is
        outside the <code>float</code> range. (I suppose if I had been
        using 64 bit floats (“<code>double</code>”) this case would have
        been ok.) But in general, TexSyn allows RGB color components to
        range over the entire <code>float</code> range, so <em>Gamma</em>
        would eventually hit a large enough input value and exponent to
        cause this kind of range overflow. I fixed the bug by adding a
        special case in function <code>Color::gamma()</code> — if the
        input is finite (<code>std::finite()</code>) but the result of
        exponentiation is <strong>not</strong> finite, then it returns
        white. </p>
      <p>To recap why <code>Colors</code> are allowed to have
        “infinite” range: the idea is to allow composition of texture
        operators without regard to order. I want it to work, for
        example, to add two textures in the range [0, 1] — the “unit
        positive RGB color cube” — producing a texture whose colors
        range on [0, 2], <strong>then</strong> scale the brightness
        back into [0, 1]:</p>
      <pre>AdjustBrightness(0.5, Add(ColorNoise(...), ColorNoise(...)))</pre>
      <p>The expression above computes the average of those two noise
        patterns. Similarly, the subtraction below produces RGB values
        ranging on [-1, 1], adding white offsets it to [0, 2], which is
        then scaled to [0, 1]:</p>
      <pre>AdjustBrightness(0.5, Add(white, Subtract(ColorNoise(...), ColorNoise(...))))</pre>
      <p>If color values were clipped to the unit interval after each
        operator it would cause texture synthesis to require certain
        operator ordering. (E.G. scaling before adding to keep the
        result in range.) My goal is to remove that constraint and so
        allow evolution to more easily optimize the textures.</p>
    </div>
    <div class="post" id="20201005"> <a href="#20201005" class="date">October









































































































































































































































































































































































































































































































        5, 2020</a>
      <h1>Experiment: minimum size for crossover snippet</h1>
      <p>As mentioned on <a href="#20201003">October 3</a>, treating
        all nodes of a <code>GpTree</code> as candidates for crossover
        implies that “many” crossover operations serve only to move a
        leaf node from one tree to another. (For binary trees, about
        half the nodes are leaves.) There is something minimal and
        elegant to this approach. But I wonder if it is the best choice,
        especially for the TexSyn <code>FunctionSet</code> where those
        numeric leaf values are just the “knobs” on various texture
        operators. In broad terms, about half the crossovers would
        actually change the tree of texture operators, while the other
        half would make changes to their parameters.</p>
      <p>Relevant to this is that next on the “to do” list is
        jitter/jiggle <strong>mutations</strong> of constant leaf
        values. I intend to jiggle constant values after crossover, so
        “wasting” half the crossover operations on merely changing a
        single constant in the tree (of size 100-200 nodes) seems a poor
        choice.</p>
      <p>I wanted to experiment with concentrating crossover operations
        on “structure changing” operations rather than “parameter
        changing”—so that a typical crossover would actually replace one
        subtree (of nested texture operators) with another one. In the
        current version, each node (including leaves) have uniform
        probability of being selected for participating in crossover. I
        considered weighting the probability of choosing a node to be
        proportional the the size of the tree below it. But this seems
        to give far too much weight to the root node and its immediate
        subnodes. I also considered just limiting crossover to subtrees
        that return a value of type <code>Texture</code> (as opposed to
        <code>Vec2</code> or <code>float</code>). But that seemed too <em>ad









































































































































































































































































































































































































































































































          hoc</em>.</p>
      <p>The experiments below add a new parameter to crossover, the
        minimum size of a subtree (from the “donor” parent) that can be
        considered (for insertion into a copy of the “recipient”
        parent). By default, in the minimalist neutral strategy, that
        value is 1, meaning an individual numeric leaf node can be
        crossed over. If <code>min_size</code> is 2 (or more) it means
        that leaf nodes (or other small subtrees) are excluded as
        crossover candidates. While <a href="#20201003">as before</a>,
        many many of the textures are obviously “variations on a theme”
        of the two parent texures, using a larger <code>min_size</code>
        clearly produced a wider variety of changes. I am leaning toward
        making “min size for crossover” a parameter of <code>FunctionSet</code>,
        defaulting to the neutral value of 1, but probably set to 2 for
        TexSyn. (Note: the eight examples below were hand-selected from
        a run of 50 crossovers, based on the same parents in the <a
          href="#20201003">October 3</a> experiments, with a <code>min_size</code>
        of 5.)</p>
      <img src="images/20201005_offspring_24.png" alt="offspring_24"
        title="offspring_24" height="511" width="511"> <img
        src="images/20201005_offspring_6.png" alt="offspring_6"
        title="offspring_6" height="511" width="511"> <br>
      <br>
      <img src="images/20201005_offspring_34.png" alt="offspring_34"
        title="offspring_34" height="511" width="511"> <img
        src="images/20201005_offspring_35.png" alt="offspring_35"
        title="offspring_35" height="511" width="511"> <br>
      <br>
      <img src="images/20201005_offspring_32.png" alt="offspring_32"
        title="offspring_32" height="511" width="511"> <img
        src="images/20201005_offspring_41.png" alt="offspring_41"
        title="offspring_41" height="511" width="511"> <br>
      <br>
      <img src="images/20201005_offspring_47.png" alt="offspring_47"
        title="offspring_47" height="511" width="511"> <img
        src="images/20201005_offspring_49.png" alt="offspring_49"
        title="offspring_49" height="511" width="511"> </div>
    <div class="post" id="20201003"> <a href="#20201003" class="date">October









































































































































































































































































































































































































































































































        3, 2020</a>
      <h1>Crossover</h1>
      <p>LazyPredator's <code>FunctionSet::crossover()</code> operator
        for <em>strongly typed genetic programming</em> appears to be
        working. After a few simple abstract tests, I tried this test
        with the TexSyn <code>FunctionSet</code>. I created two random
        “parent” <code>GpTree</code>s, then made 30 “offspring” <code>GpTree</code>s,








































































































































































































































































































































































































































































































        which combine elements of both parents. Here are eight of those
        variations, each of which is close to one parent (stripes) or
        the other (mostly uniform).</p>
      <p>The difference tend to be fairly subtle. I assume that “most”
        of the nodes in the parent trees of size 100 are numeric values
        at the leaves. As a result, many of the offspring differ from
        their parent by just one number being replaced by another. I <em>think</em>
        that is correct but may experiment with biasing the crossover
        points to be higher up the trees.</p>
      <img src="images/20201003_offspring_0.png" alt="offspring_0"
        title="offspring_0" height="511" width="511"> <img
        src="images/20201003_offspring_1.png" alt="offspring_1"
        title="offspring_1" height="511" width="511"> <br>
      <br>
      <img src="images/20201003_offspring_13.png" alt="offspring_13"
        title="offspring_13" height="511" width="511"> <img
        src="images/20201003_offspring_10.png" alt="offspring_10"
        title="offspring_10" height="511" width="511"> <br>
      <br>
      <img src="images/20201003_offspring_18.png" alt="offspring_18"
        title="offspring_18" height="511" width="511"> <img
        src="images/20201003_offspring_11.png" alt="offspring_11"
        title="offspring_11" height="511" width="511"> <br>
      <br>
      <img src="images/20201003_offspring_28.png" alt="offspring_28"
        title="offspring_28" height="511" width="511"> <img
        src="images/20201003_offspring_15.png" alt="offspring_15"
        title="offspring_15" height="511" width="511"> </div>
    <div class="post" id="20200927"> <a href="#20200927" class="date">September









































































































































































































































































































































































































































































































        27, 2020</a>
      <h1>Reconsidering convolution</h1>
      <p>The most “heavyweight” Texture operators are those based on
        convolution: <em>Blur</em>, <em>EdgeDetect</em>, and <em>EdgeEnhance</em>.
        (Only <em>Blur</em> actually does a convolution, <em>EdgeDetect</em>
        and <em>EdgeEnhance</em> are derived from <em>Blur</em> by
        “unsharp masking.”) For all other Texture operators, each output
        pixel <code>getColor()</code> requires one, two, or some small
        number of input <code>getColor()</code> lookups in textures
        further down the tree. For each pixel of a <em>Blur</em>, a
        large number of lookups are needed to sample the input texture
        under the support of the convolution kernel. That value had been
        set to 11x11 or 121 samples per output pixel <code>getColor()</code>
        lookup. </p>
      <p>In normal image processing, blurring an image can be done
        quickly, especially when using a GPU. The whole image can be
        blurred at once, in place in a memory array. With a Gaussian
        kernel, that can be separated into two sequential
        one-dimensional passes. None of these speedups apply to TexSyn
        where textures have “infinite” extent and are routinely
        addressed via non-linear warps. A TexSyn textures has no way to
        determine the spacing between adjacent pixel samples—and cannot
        know <em>a priori</em> the bounds of its “infinite” extent that
        will be sampled—so cannot build memory arrays that cache the
        pixel colors. (I write “infinite” in quotes because these
        textures are actually defined over the cross product of two c++
        <code>float</code>s, which is not equivalent to ℝ² the cross
        product of two reals.)</p>
      <p>Nonetheless the current design allows warping the result of
        applying <em>Blur</em> to a warped texture, at the cost of
        taking about 100 times longer than other operators. But until I
        started generating random TexSyn trees with <em>Blur</em>, <em>EdgeDetect</em>,
        and <em>EdgeEnhance</em>, I was chagrined that I had not
        considered the cost of <strong>composing</strong> multiple
        convolution operators by nesting. Now we run into some subtrees
        — such as <em>Blur(EdgeEnhance(...)) </em>— that are 10,000
        slower than normal operators!<br>
        <br>
        One possibility is deciding we didn't really want blur and edge
        operations in TexSyn, and I guess that could still happen. But
        first I want to see what I can do to chip away at the cost. One
        approach is to disallow the deadly nested convolutions. This
        could be done as a constraint on <code>GpTree</code>
        construction. But that would add a lot of complexity for this
        one edge case. Another way is to say that a nested convolution
        operator turns into the identity operator. This is appealing
        because it only touches the relevant TexSyn operators, and in
        fact merely <em>Blur</em>. So I implemented that.</p>
      <p>Another approach is to use <em>Blur</em>'s stochastic sampling
        to trade off image quality for speed. All it takes is reducing
        the number of subsamples to run faster, while causing more noise
        in the blurred image. That global parameter is called <code>
          Blur::sqrt_of_subsample_count</code> and had been set to 11
        (121 subsamples). I have now decreased it to 7 (49 subsamples).
        So it now runs in about 40% (49/121) of the time it did
        previously. Below are the input <em>Grating</em> texture, and a
        comparison of <em>Blur</em> at 11x11 (as before, on the top
        half) and 7x7 (as now, on the bottom half):</p>
      <pre>yellow = Uniform(1, 1, 0)
blue = Uniform(0, 0, 1)
grating = Grating(Vec2(), yellow, Vec2(0.2, 0), blue, 0.01, 0.5)
filter_width = 0.1

blur_7_11_compare = Gradation(Vec2(0, 0.1),
                              Blur(filter_width, grating),
                              Vec2(0, -0.1),
                              Blur(filter_width, grating))
</pre>
      <img src="images/20200927_grating.png" alt="grating"
        title="grating" height="511" width="511"> &nbsp; <img
        src="images/20200927_blur_7_11_compare.png"
        alt="blur_7_11_compare" title="blur_7_11_compare" height="511"
        width="511">
      <p>Now here is a similar texture, wrapped into a spiral by the
        COTS patch operator. Note how the zone of transition between the
        two colors grows with the spiral as do the width of the stripes.
        (this is the same <code>grating</code> texture as above, scaled
        up by 10, with the filter width also scaled by 10 to match)
        These three textures show the result with <code>Blur::sqrt_of_subsample_count</code>
        set to <strong>40</strong>, <strong>11</strong>, and <strong>7</strong>
        (corresponding to <strong>1600</strong>, <strong>121</strong>,
        and <strong>49</strong> subsamples per output pixel). 40 is
        ridiculously high and slow-running, but shows what the <em>Blur</em>
        looks like with essentially no sampling noise. The middle value
        of 11 is what was previously used, and 7 is the new default
        value. The resulting stipple pattern looks a bit like colored
        pencil on textured drawing paper.</p>
      <pre>CotsMap(Vec2(0.0, 0.0),
        Vec2(0.4, 0.0),
        Vec2(0.2, 0.2),
        Vec2(0.0, 0.4),
        Blur(filter_width * 10, Scale(10, grating)))</pre>
      <img src="images/20200927_blur_40_spiral.png" alt="blur 40 spiral"
        title="blur 40 spiral" height="511" width="511"> <br>
      <br>
      <img src="images/20200927_blur_11_spiral.png" alt="blur 11 spiral"
        title="blur 11 spiral" height="511" width="511"> &nbsp; <img
        src="images/20200927_blur_7_spiral.png" alt="blur 7 spiral"
        title="blur 7 spiral" height="511" width="511"> </div>
    <div class="post" id="20200923"> <a href="#20200923" class="date">September









































































































































































































































































































































































































































































































        23, 2020</a>
      <h1>Completed TexSyn's <code>FunctionSet</code></h1>
      <p>I completed building a LazyPredator <code>FunctionSet</code>
        for TexSyn's API. Or what I have come to regard as the “modern
        subset” of TexSyn's API. For example TexSyn still defines <em>Scale</em>,
        <em>Rotate</em>, and <em>Translate</em>, but I consider them
        deprecated, replaced by <em>Affine</em>. I will post an updated
        list soon, but of the 52(?) Texture operators in TexSyn, I
        defined 43(?) in the GP <code>FunctionSet</code>. This is all
        subject to change. I also defined some “helper functions” in the
        new <code>GP.h</code> file to reduce the repetition of boiler
        place, especially in the <code>eval()</code> lambda specified
        for each <code>GpFunction</code>. </p>
      <p>Shown below are some of the random textures produced with this
        full <code>FunctionSet</code>, all made with a max size of 100.
        As I was testing I would save off examples that I thought were a
        bit interesting. Here a six pointed star motif, with confetti,
        all in pastel colors.</p>
      <pre><span class="comment">// random program texsyn43_0</span>
Ring(6.05007,
     Vec2(0.913583, 0.577358),
     Vec2(0.563676, 0.0549508),
     Max(Mirror(Vec2(-0.8868, 0.936961),
                Vec2(-0.438223, -0.375759),
                LotsOfButtons(0.863516, 0.690535, 0.715235, 0.137752, 0.989513,
                              Vec2(0.0813249, 0.105461),
                              Gamma(1.23506,
                                    Uniform(0.752434, 0.716533, 0.128006)),
                              0.925224,
                              LotsOfSpots(0.337259, 0.297271, 0.342979, 0.0796881, 0.829768,
                                          ColorNoise(Vec2(0.64538, 0.832061),
                                                     Vec2(-0.825276, -0.968037), 0.730953),
                                          ColorNoise(Vec2(0.355079, -0.307888),
                                                     Vec2(-0.931093, -0.272665), 0.60784)))),
         CotsMap(Vec2(-0.0810903, -0.351246),
                 Vec2(-0.227743, 0.102102),
                 Vec2(0.0972011, 0.137803),
                 Vec2(0.138099, 0.0864316),
                 Hyperbolic(Vec2(0.305696, -0.83856),
                            1.37151, 9.30557, 7.92249,
                            Uniform(0.748912, 0.938295, 0.342172),
                            Uniform(0.342688, 0.907577, 0.417378)),
                 SoftMatte(Uniform(0.210974, 0.980838, 0.843935),
                           Gamma(2.74955,
                                 Uniform(0.97905, 0.950607, 0.0890492)),
                           Gamma(8.93982,
                                 Uniform(0.400887, 0.487526, 0.870267))))))</pre>
      <img src="images/20200923_texsyn43_0.png" alt="texsyn43_0"
        title="texsyn43_0" height="511" width="511">
      <p>Here an elaborate pattern of cream drops on pea soup:</p>
      <pre><span class="comment">// random program texsyn43_4</span>
MobiusTransform(Vec2(-0.0825154, 0.589955),
                Vec2(-0.88387, 0.422129),
                Vec2(0.372059, 0.443364),
                Vec2(-0.51013, 0.358481),
                Add(BrightnessWrap(0.776266,
                                   0.221761,
                                   Ring(6.85811,
                                        Vec2(-0.684147, 0.428017),
                                        Vec2(0.0123824, -0.901044),
                                        Gamma(2.203,
                                              Twist(-0.385167,
                                                    -0.0458701,
                                                    Vec2(0.184168, -0.098526),
                                                    Mirror(Vec2(-0.0467548, -0.867534),
                                                           Vec2(-0.115532, -0.309734),
                                                           BrightnessToHue(0.795387,
                                                                           AbsDiff(HueOnly(0.554474,
                                                                                           0.842341,
                                                                                           Uniform(0.252994, 0.383974, 0.92492)),
                                                                                   BrightnessToHue(0.694017,
                                                                                                   Uniform(0.257844, 0.981215, 0.992339))))))))),
                    LotsOfButtons(0.77023, 0.335993, 0.489251, 0.530601, 0.0553876,
                                  Vec2(0.317931, 0.973596),
                                  HueOnly(0.42517,
                                          0.872893,
                                          AdjustSaturation(0.647189,
                                                           Uniform(0.678778, 0.538012, 0.0466902))),
                                  0.810624,
                                  Gamma(0.90588,
                                        Min(SoftThreshold(0.345021,
                                                          0.385886,
                                                          Uniform(0.41695, 0.43103, 0.104457)),
                                            Wrap(-0.297351,
                                                 Vec2(0.358198, 0.912694),
                                                 Vec2(0.400779, 0.701969),
                                                 AdjustBrightness(0.0167935,
                                                                  Uniform(0.644239, 0.161153, 0.287077))))))))</pre>
      <img src="images/20200923_texsyn43_4.png" alt="texsyn43_4"
        title="texsyn43_4" height="511" width="511">
      <p>Fried space egg, sunny side up.</p>
      <pre><span class="comment">// random program texsyn37_11</span>
AbsDiff(SoftThreshold(0.298399, 0.727319,
                      Gradation(Vec2(-0.39333, 0.136865),
                                ColoredSpots(0.28252, 0.863896, 0.742588, 0.784091, 0.452963,
                                             Uniform(0.964614, 0.625866, 0.411626),
                                             Uniform(0.866328, 0.899633, 0.20739)),
                                Vec2(0.886619, -0.30344),
                                BrightnessWrap(0.193428, 0.158088,
                                               Mirror(Vec2(0.91596, -0.785272),
                                                      Vec2(0.0809205, 0.892906),
                                                      StretchSpot(0.985838, 0.88553,
                                                                  Vec2(0.153175, 0.517769),
                                                                  AdjustHue(0.449036,
                                                                            SoftThreshold(0.0201699,
                                                                                          0.983103,
                                                                                          Uniform(0.160094, 0.334016, 0.528756)))))))),
        Max(BrightnessToHue(0.333162,
                            AdjustSaturation(0.0850126,
                                             Max(ColorNoise(Vec2(0.336733, 0.632002),
                                                            Vec2(0.371583, 0.644038),
                                                            0.275456),
                                                 BrightnessToHue(0.528467,
                                                                 Min(Uniform(0.821086, 0.168073, 0.15639),
                                                                     Uniform(0.00221804, 0.328301, 0.751129)))))),
            LotsOfButtons(0.610481, 0.363803, 0.9173, 0.298918, 0.701165,
                          Vec2(-0.717095, -0.271969),
                          Uniform(0.36265, 0.211855, 0.388246),
                          0.353225,
                          Add(Uniform(0.0283104, 0.951895, 0.792557),
                              Uniform(0.0151601, 0.389861, 0.339307)))))</pre>
      <img src="images/20200923_texsyn37_11.png" alt="texsyn37_11"
        title="texsyn37_11" height="511" width="511">
      <!--
      <p>Fuzzy spots, in limbo.</p>      <pre><span class="comment">// random program texsyn37_17</span>LotsOfButtons(0.933154,              0.468443,              0.290417,              0.921633,              0.196655,              Vec2(-0.108668, 0.362876),              Spot(Vec2(0.60092, 0.346435),                   0.505766,                   AdjustSaturation(0.487457,                                    Uniform(0.0883881, 0.484707, 0.128435)),                   1.87522,                   Mirror(Vec2(-0.254049, 0.387378),                          Vec2(-0.115558, 0.445999),                          Max(Uniform(0.941028, 0.640188, 0.661906),                              BrightnessToHue(0.0447968,                                              Uniform(0.732487, 0.42239, 0.726667))))),              0.594073,              SoftThreshold(0.176752,                            0.651301,                            Subtract(Uniform(0.923531, 0.26259, 0.00413869),                                     AbsDiff(Colorize(Vec2(0.704127, -0.96683),                                                      Vec2(0.434464, 0.536132),                                                      Subtract(Uniform(0.149775, 0.951417, 0.157127),                                                               Uniform(0.612073, 0.00658208, 0.0262323)),                                                      ColorNoise(Vec2(0.668097, -0.470961),                                                                 Vec2(0.0229161, -0.327769),                                                                 0.940818)),                                             Row(Vec2(-0.329714, -0.38582),                                                 Vec2(-0.206665, -0.232556),                                                 Max(AdjustBrightness(0.183757,                                                                      BrightnessWrap(0.526949,                                                                                     0.963419,                                                                                     Uniform(0.0419757, 0.998081, 0.398214))),                                                     StretchSpot(0.880503,                                                                 0.214836,                                                                 Vec2(-0.823275, -0.788442),                                                                 Uniform(0.784191, 0.197271, 0.717908))))))))</pre>      <img src="images/20200923_texsyn37_17.png" alt="texsyn37_17" title="texsyn37_17"        height="511" width="511">      -->
      <p>Lots of stretched-out spots:</p>
      <pre><span class="comment">// random program texsyn37_26</span>
SoftMatte(StretchSpot(1.65618, 1.70558,
                      Vec2(0.220822, -0.484598),
                      AdjustSaturation(0.0869728,
                                       Wrap(0.780284,
                                            Vec2(-0.0948536, -0.586182),
                                            Vec2(0.0117428, 0.372895),
                                            SoftThreshold(0.7921, 0.337999,
                                                          BrightnessToHue(0.445559,
                                                                          Uniform(0.838806, 0.327221, 0.385638)))))),
          BrightnessToHue(0.294684,
                          AdjustHue(0.766938,
                                    AbsDiff(Add(BrightnessWrap(0.379944,
                                                               0.0777347,
                                                               Uniform(0.398626, 0.405375, 0.0107769)),
                                                BrightnessToHue(0.480222,
                                                                AdjustSaturation(0.813353,
                                                                                 Uniform(0.641829, 0.874743, 0.77074)))),
                                            AdjustBrightness(0.959231,
                                                             Max(AdjustSaturation(0.42562,
                                                                                  Uniform(0.987204, 0.325772, 0.653587)),
                                                                 AdjustSaturation(0.762236,
                                                                                  Uniform(0.545807, 0.552129, 0.0672275))))))),
          Stretch(Vec2(0.0765375, 0.257372),
                  Vec2(-0.00445917, 0.112877),
                  StretchSpot(1.03807, 1.39177,
                              Vec2(-0.870523, 0.750637),
                              LotsOfButtons(0.78265, 0.235866, 0.890119, 0.293386, 0.130943,
                                            Vec2(-0.0170715, 0.425005),
                                            Uniform(0.758016, 0.290013, 0.198833),
                                            0.581423,
                                            Max(Uniform(0.294325, 0.677582, 0.432185),
                                                AdjustHue(0.145721,
                                                          Uniform(0.77274, 0.34552, 0.668212)))))))</pre>
      <img src="images/20200923_texsyn37_26.png" alt="texsyn37_26"
        title="texsyn37_26" height="511" width="511">
      <p>A few others whose source code I forgot to save:</p>
      <img src="images/20200923_texsyn29_21.png" alt="texsyn29_21"
        title="texsyn29_21" height="511" width="511"> &nbsp; <img
        src="images/20200923_texsyn23_7.png" alt="texsyn23_7"
        title="texsyn23_7" height="511" width="511"> &nbsp; <img
        src="images/20200923_texsyn21_27.png" alt="texsyn21_27"
        title="texsyn21_27" height="511" width="511"> &nbsp; </div>
    <div class="post" id="20200922"> <a href="#20200922" class="date">September









































































































































































































































































































































































































































































































        22, 2020</a>
      <h1>WIP: filling out TexSyn's <code>FunctionSet</code></h1>
      <p>Adding existing TexSyn texture operators one by one to the
        LazyPredator <code>FunctionSet</code>. When it contained only <em>Uniform</em>,
        <em>Spot</em>, <em>Gradation</em>, <em>Grating</em>, and <em>SoftMatte</em>,
        I generated trees with a max size of 100. Here is a small (size
        28) tree which combines a <em>Grating</em> over a <em>Gradation</em>.
        Also a more complex Texture tree (size 96) producing what looks
        a bit like a shaded fuzzy green ball over a multicolored
        background. </p>
      <pre><span class="comment">// random program texsyn5 22</span><br>Grating(Vec2(0.26869, 0.461954),
        Gradation(Vec2(0.167604, 0.86406),
                  Uniform(0.56669, 0.347976, 0.12547),
                  Vec2(0.295079, 0.870558),
                  Uniform(0.550106, 0.721061, 0.691719)),
        Vec2(0.0471458, 0.182742),
        Uniform(0.169519, 0.670011, 0.662775),
        0.67318,
        0.592581)</pre>
      <img src="images/20200922_texsyn5_22.png" alt="texsyn5 22"
        title="texsyn5 22" height="511" width="511">
      <pre><span class="comment">// random program texsyn5 25</span><br>Spot(Vec2(0.0270606, -0.111671),
     0.640568,
     Gradation(Vec2(0.156171, 0.479821),
               Uniform(0.452769, 0.821953, 0.671481),
               Vec2(0.567867, -0.488934),
               Gradation(Vec2(0.370465, 0.987898),
                         Uniform(0.143607, 0.216632, 0.422456),
                         Vec2(-0.164897, 0.402079),
                         Uniform(0.13807, 0.450368, 0.214707))),
     0.837218,
     Gradation(Vec2(-0.391453, -0.186617),
               Spot(Vec2(0.680379, -0.826925), 1.27738,
                    Uniform(0.0325629, 0.82633, 0.607977),
                    1.71849,
                    Uniform(0.60406, 0.181758, 0.368314)),
               Vec2(-0.46888, -0.338268),
               SoftMatte(Spot(Vec2(0.454638, 0.315729),
                              0.633048,
                              Uniform(0.568433, 0.801311, 0.28252),
                              1.72779,
                              Uniform(0.784091, 0.452963, 0.820982)),
                         Spot(Vec2(-0.176747, -0.982708),
                              1.73266,
                              Uniform(0.20739, 0.027998, 0.94331),
                              0.69656,
                              Uniform(0.193428, 0.158088, 0.722144)),
                         Spot(Vec2(-0.785272, 0.54517),
                              1.08092,
                              Uniform(0.47373, 0.492919, 0.442765),
                              1.18617,
                              Uniform(0.758885, 0.0847335, 0.449036)))))</pre>
      <img src="images/20200922_texsyn5_25.png" alt="texsyn5 25"
        title="texsyn5 25" height="511" width="511">
      <p>Layering <em>Spot</em> over <em>Grating</em> over <em>Spot</em>,
        size 96:</p>
      <pre><span class="comment">// random program texsyn6_7</span><br>Spot(Vec2(-0.540721, -0.326729),
     0.72255,
     Gradation(Vec2(0.350523, 0.213815),
               Add(Uniform(0.745849, 0.375344, 0.166896),
                   Uniform(0.834834, 0.803454, 0.587308)),
               Vec2(0.872269, 0.444256),
               Uniform(0.479733, 0.906918, 0.109246)),
     0.319564,
     SoftMatte(Grating(Vec2(0.364624, 0.295435),
                       Uniform(0.991602, 0.404537, 0.452058),
                       Vec2(0.5166, 0.767411),
                       Uniform(0.420852, 0.0787707, 0.225527),
                       0.230418,
                       0.368269),
               Spot(Vec2(0.920896, -0.192638),
                    0.577384,
                    Uniform(0.670203, 0.348083, 0.3225),
                    0.913576,
                    Gradation(Vec2(-0.570529, 0.95223),
                              Uniform(0.0178784, 0.215667, 0.744309),
                              Vec2(-0.208699, -0.774712),
                              Uniform(0.674122, 0.761577, 0.649836))),
               SoftMatte(Uniform(0.304042, 0.14227, 0.810219),
                         SoftMatte(Uniform(0.51514, 0.513601, 0.831815),
                                   Uniform(0.993132, 0.452829, 0.808701),
                                   Uniform(0.549863, 0.550531, 0.796817)),
                         Add(Uniform(0.977974, 0.551423, 0.140097),
                             Uniform(0.333083, 0.762814, 0.591103)))))</pre>
      <img src="images/20200922_texsyn6_7.png" alt="texsyn6_7"
        title="texsyn6_7" height="511" width="511">
      <p>...size=89 </p>
      <pre><span class="comment">// random program texsyn14_15</span><br>Multiply(Wrap(-0.393609,
              Vec2(-0.0156267, 0.45183),
              Vec2(0.48514, -0.622225),
              ColorNoise(Vec2(0.290586, -0.532596),
                         Vec2(0.435022, -0.625395), 0.924635)),
         Min(Add(Grating(Vec2(0.505614, 0.319813),
                         Uniform(0.227292, 0.0494925, 0.652759),
                         Vec2(-0.248363, 0.751465),
                         Uniform(0.221749, 0.452565, 0.0321766),
                         0.759366,
                         0.435336),
                 Spot(Vec2(0.302574, -0.269131),
                      1.29765,
                      Uniform(0.600034, 0.192719, 0.256547),
                      1.82173,
                      Multiply(Uniform(0.675937, 0.0819467, 0.744247),
                               BrightnessToHue(0.0572891,
                                               Uniform(0.167216, 0.135414, 0.93294))))),
             MultiNoise(Vec2(0.931529, -0.997263),
                        Vec2(-0.00019452, 0.621546),
                        Wrap(0.180313,
                             Vec2(-0.155344, 0.141332),
                             Vec2(0.600042, -0.499762),
                             Uniform(0.926494, 0.143688, 0.277342)),
                        Wrap(-0.631191,
                             Vec2(-0.292852, 0.484951),
                             Vec2(0.934354, -0.623622),
                             Uniform(0.893133, 0.0314834, 0.616199)),
                        0.0939481)))</pre>
      <img src="images/20200922_texsyn14_15.png" alt="texsyn14_15"
        title="texsyn14_15" height="511" width="511"> </div>
    <div class="post" id="20200921"> <a href="#20200921" class="date">September









































































































































































































































































































































































































































































































        21, 2020</a>
      <h1>Runtime connection between LazyPredator and TexSyn</h1>
      <p>In the <a href="#20200815">August 15</a> entry I prototyped
        this connection using a prototype <code>FunctionSet:makeRandomTree()</code>
        to print out the “source code” of generated trees, then hand
        editing that into a test jig in TexSyn and rendering the
        textures. Now it is actually working, directly evaluating the <code>GpTree</code>
        and then passing that result to TexSyn's render utility. See
        some code details in the <a
          href="https://cwreynolds.github.io/LazyPredator/#20200921">LazyPredator









































































































































































































































































































































































































































































































          log</a>. These were made with a <code>FunctionSet</code> for
        a portion of TexSyn with <code>GpTypes</code> for <code>Texture</code>
        pointers, <code>Vec2</code> values, and <code>Float_01</code>
        values. It provides two <code>GpFunctions</code> of Texture
        operators: <em>Uniform</em> and <em>Spot</em>. Here are a
        couple of examples. Note that these are “random”
        trees(/expressions/programs) with no evolutionary optimization
        yet. The next step is to define <code>GpFunctions</code> for
        all ~50 Texture operators.</p>
      <pre>Spot(Vec2(0.609615, 0.987836),
     0.0602494,
     Uniform(0.930115, 0.125967, 0.926106),
     0.117543,
     Spot(Vec2(0.0590451, 0.83672),
          0.402121,
          Uniform(0.77876, 0.900702, 0.469339),
          0.411421,
          Uniform(0.262347, 0.042383, 0.34038)))</pre>
      <img src="images/20200920_uniform_spot_7.png" alt="Random texture
        7" title="Random texture 7" height="511" width="511">
      <pre>Spot(Vec2(0.648294, 0.628089),
     0.431737,
     Uniform(0.103924, 0.719841, 0.884774),
     0.520491,
     Spot(Vec2(0.31976, 0.662522),
          0.886379,
          Spot(Vec2(0.591504, 0.58366),
               0.804196,
               Uniform(0.897731, 0.170638, 0.0459084),
               0.927503,
               Uniform(0.935545, 0.882617, 0.0314533)),
          0.506059,
          Uniform(0.390179, 0.696574, 0.59118)))</pre>
      <img src="images/20200920_uniform_spot_9.png" alt="Random texture
        9" title="Random texture 9" height="511" width="511"> </div>
    <div class="post" id="20200815"> <a href="#20200815" class="date">August









































































































































































































































































































































































































































































































        15, 2020</a>
      <h1>Random TexSyn programs generated by LazyPredator </h1>
      <p>As described in <a
          href="https://cwreynolds.github.io/LazyPredator/#20200815">LazyPredator's









































































































































































































































































































































































































































































































          log</a>, it now has a prototype random program generator.
        There is also now a prototype “function set”/“grammar” to
        describe the TexSyn API. I used these to generate a bunch of
        random texture expressions, rendered some of those, and pick out
        a few that were a bit interesting. (Many appeared identical to <em>Uniform</em>
        or <em>ColorNoise</em>.)</p>
      <p class="wrapping_code"><span class="comment">// random program 0</span><br>
        Twist(-1.5007, -3.12824, Vec2(0.389467, 0.832243),
        LotsOfButtons(0.862353, 0.326306, 0.00177993, 0.711357, 0.76274,
        Vec2(3.0448, 3.68129), Spot(Vec2(1.48987, -4.49347), 1.78005,
        ColorNoise(Vec2(-1.32046, 2.42863), Vec2(-1.91713, 0.940589),
        0.84564), 0.764313, ColorNoise(Vec2(0.273154, -1.46148),
        Vec2(-1.18504, -4.00435), 0.724306)), 0.367926,
        ColorNoise(Vec2(-1.59627, -3.31247), Vec2(2.23131, -1.37167),
        0.698217)))</p>
      <img src="images/20200815_random_program_0.png" alt="random
        program 0" title="random program 0" height="511" width="511">
      <p class="wrapping_code"><span class="comment">// random program 1</span><br>
        ColoredSpots(0.0873328, 0.904293, 0.0523186, 0.0906169,
        0.854814, MobiusTransform(Vec2(3.11807, 1.18701),
        Vec2(-0.589404, 2.75448), Vec2(-3.33792, 2.08031),
        Vec2(-2.82157, -3.79023), Uniform(0.409955, 0.158851, 0.22622)),
        BrightnessWrap(0.0941196, 0.596546, EdgeDetect(0.985994,
        Max(MultiNoise(Vec2(-4.08933, -2.55314), Vec2(-3.52786,
        3.43654), Uniform(0.547777, 0.71849, 0.51048),
        Uniform(0.0970927, 0.656361, 0.669388), 0.0278431),
        ColorNoise(Vec2(-0.665772, -3.61144), Vec2(3.66564, -4.4826),
        0.695869)))))</p>
      <img src="images/20200815_random_program_1.png" alt="random
        program 1" title="random program 1" height="511" width="511">
      <p class="wrapping_code"><span class="comment">// random program 2</span><br>
        Ring(3.11535, Vec2(3.03757, 0.967017), Vec2(-1.42594, -1.81315),
        SliceShear(Vec2(-4.01977, -3.029), Vec2(-3.63676, 2.93948),
        Hyperbolic(Vec2(-4.63751, 3.72073), 8.21554, 9.18318, 4.19357,
        Uniform(0.642181, 0.261001, 0.428011), ColorNoise(Vec2(-4.05045,
        -1.65673), Vec2(-3.66899, 1.61807), 0.792373)), Vec2(-0.478021,
        4.5042), Vec2(4.56995, 0.0112824), ColorNoise(Vec2(3.34685,
        3.47989), Vec2(-0.249423, -0.262976), 0.175241)))</p>
      <img src="images/20200815_random_program_2.png" alt="random
        program 2" title="random program 2" height="511" width="511">
      <p class="wrapping_code"><span class="comment">// random program 3</span><br>
        Grating(Vec2(-1.47541, -0.86595), AdjustBrightness(0.0384087,
        BrightnessWrap(0.805273, 0.469073, BrightnessWrap(0.401083,
        0.670738, HueOnly(0.281567, 0.0632304, Min(Uniform(0.427763,
        0.769109, 0.859361), ColorNoise(Vec2(2.44752, -4.46087),
        Vec2(-2.27331, 4.02469), 0.0391472)))))), Vec2(-2.13785,
        -3.31639), Gamma(1.47854, SliceToRadial(Vec2(-3.44811,
        -2.35952), Vec2(-3.74908, 1.89859), ColorNoise(Vec2(-2.45512,
        3.92944), Vec2(1.00124, 3.80463), 0.657286))), 0.965485,
        0.55324)</p>
      <img src="images/20200815_random_program_3.png" alt="random
        program 3" title="random program 3" height="511" width="511">
      <p class="wrapping_code"><span class="comment">// random program 4</span><br>
        EdgeDetect(0.670139, ColoredSpots(0.7614, 0.427715, 0.698671,
        0.885313, 0.0558514, Stretch(Vec2(-4.63756, -4.60256),
        Vec2(-0.0368361, -2.01205), AbsDiff(SoftMatte(Uniform(0.817255,
        0.00670789, 0.326614), ColorNoise(Vec2(-1.51223, 2.41655),
        Vec2(-3.37003, -4.64803), 0.29183), Uniform(0.990324, 0.0578273,
        0.848225)), Uniform(0.030859, 0.428472, 0.973189))),
        Add(SliceToRadial(Vec2(-0.793439, -2.48329), Vec2(4.03724,
        2.45832), Uniform(0.442126, 0.573716, 0.949417)),
        LotsOfButtons(0.270409, 0.145435, 0.790308, 0.703025, 0.736794,
        Vec2(4.84287, 3.77388), ColorNoise(Vec2(-2.84685, -1.22626),
        Vec2(-1.97162, -3.88645), 0.157141), 0.0940529,
        Uniform(0.0183813, 0.9557, 0.00798232)))))</p>
      <img src="images/20200815_random_program_4.png" alt="random
        program 4" title="random program 4" height="511" width="511"> </div>
    <div class="post" id="20200806"> <a href="#20200806" class="date">August








































































































































































































































































































































































































































































































        6, 2020</a>
      <h1>LazyPredator — at long last, a GP sibling</h1>
      <p>As described long ago (<a href="#20191219">December 19, 2019</a>)
        TexSyn exists to be used with a system for <a
          href="https://en.wikipedia.org/wiki/Generic_programming">Genetic









































































































































































































































































































































































































































































































          Programming</a>. Starting yesterday I have been made the first
        steps toward that, a library called <strong>LazyPedator</strong>.
        Its code repository is on <a
          href="https://github.com/cwreynolds/LazyPredator">GitHub</a>
        and it has its own <a
          href="https://cwreynolds.github.io/LazyPredator/">development
          diary</a> like this one.</p>
    </div>
    <div class="post" id="20200731"> <a href="#20200731" class="date">July








































































































































































































































































































































































































































































































        31, 2020</a>
      <h1>More boring: <em>Noise</em> and <em>Affine</em> get <code>TwoPointTransform</code></h1>
      <p>Refactoring <em>Noise</em> to use the new <code>TwoPointTransform</code>
        class, completely analogous to <em>Grating</em> and <em>Gradation</em>
        <a href="#20200730">yesterday</a>. Because <em>Noise</em> is
        the base class for all other Perlin noise related operators,
        they all inherit the same change. As before, showing the visual
        different between old and new—average error was 1.606e-07—and
        the binary version showing were mismatches occur.</p>
      <pre>Texture::diff(Noise (p4, p5, black, white),
              Noise2(p4, p5, black, white))</pre>
      <img src="images/20200730_Noise_AbsDiff.png" alt="diff Noise
        before/after TwoPointTransform" title="diff Noise before/after
        TwoPointTransform" height="341" width="1023"> <br>
      <br>
      <img src="images/20200730_Noise_NotEqual.png" alt="diff Noise
        before/after TwoPointTransform" title="diff Noise before/after
        TwoPointTransform" height="341" width="1023">
      <p>Making the fix for <em>Affine</em> had one surprise. I found
        that it had not previously handled the case for a degenerate
        “scale=0” transform. This occurs when the two point specifying
        the transform are identical. In the old version, each color of
        the texture contained <strong>nan</strong> RGB values. Now in
        that case <em>Affine</em> uses an identity transform. The
        average error per pixel in this case was 5.53341e-09.</p>
      <pre>Texture::diff(Affine (p1, p2, spot),
              Affine2(p1, p2, spot))</pre>
      <img src="images/20200730_Affine_AbsDiff.png" alt="diff Affine
        before/after TwoPointTransform" title="diff Affine before/after
        TwoPointTransform" height="341" width="1023"> <br>
      <br>
      <img src="images/20200730_Affine_NotEqual.png" alt="diff Affine
        before/after TwoPointTransform" title="diff Affine before/after
        TwoPointTransform" height="341" width="1023"> </div>
    <div class="post" id="20200730"> <a href="#20200730" class="date">July








































































































































































































































































































































































































































































































        30, 2020</a>
      <h1>Boring software engineering: use <code>TwoPointTransform</code></h1>
      <p>The “two point” specification for a general affine transform
        was used in early members of this library, see <em>Gradation</em>
        and <em>Grating</em> on <a href="#20200101">January 1</a>.
        This is the same idea used in “two finger” pinch-twist-drag
        gestures in touch screen devices. On <a href="#20200531">May 31</a>
        I refactored several noise textures to more uniformly use
        two-point transformations, and on <a href="#20200708">July 8</a>,
        the generic<br>
        <em>Affine</em> operator. As a result the code for “two point”
        specification is duplicated around the operators. This <strong>really</strong>
        bothered the software engineer in me. So I wrote a new general
        purpose <code>TwoPointTransform</code> utility class to
        encapsulate this code. Now I have begun changing texture
        operators to use it.</p>
      <p><em>Grating</em> was first to be updated. Annoyingly the
        modified <em>Grating<strong>2</strong></em> is ever-so-slightly
        different from the original version. I assume that the order of
        floating point operations leads to tiny rounding differences.
        Here is the old (on left) new (center) and <em>AbsDiff</em> of
        the two (on right). The black <em>AbsDiff</em> image indicates
        they are visually identical, but about half of the pixels do not
        have identical floating point gray values. The average error per
        pixel is 1.55727e-07. That is, where black is 0 and white is 1,
        the average error is 0.00000155727. That bothers my OCD, but I
        guess it is less annoying than the original code duplication.</p>
      <pre>p1 = Vec2(0.1, 0.1)
p2 = Vec2(-0.1, -0.1)
Texture::diff(Grating (p1, black, p2, white, 1.0, 0.8),
              Grating2(p1, black, p2, white, 1.0, 0.8))</pre>
      <img src="images/20200730_AbsDiff.png" alt="diff Grating
        before/after TwoPointTransform" title="diff Grating before/after
        TwoPointTransform" height="341" width="1023">
      <p>I added a new fifth optional parameter to <code>Texture::diff()</code>,
        a <code>bool</code> called <code>binary</code> which defaults
        to <code>false</code>. When <code>true</code>, the third
        texture in a diff is made with a new <em>NotEqual</em> texture
        operator which returns white where the two textures do not match
        and black where they do. This allows seeing where those tiny
        invisible numerical errors are:</p>
      <img src="images/20200730_NotEqual.png" alt="diff Grating
        before/after TwoPointTransform" title="diff Grating before/after
        TwoPointTransform" height="341" width="1023">
      <p>Then I went on to the corresponding change to <em>Gradation</em>.
        Again the result is very slightly different—4.16738e-08—less
        than a millionth. Most display hardware only portrays 256 gray
        levels. As before one diff() showing the visual difference, and
        one showing white where the tiny differences occur.</p>
      <pre>p3 = Vec2 p3(0.6, 0.6)
p4 = Vec2(-0.6, -0.6)
Texture::diff(Gradation (p3, black, p4, white),
              Gradation2(p3, black, p4, white))</pre>
      <img src="images/20200730_Gradation_AbsDiff.png" alt="diff
        Gradation before/after TwoPointTransform" title="diff Gradation
        before/after TwoPointTransform" height="341" width="1023"> <br>
      <br>
      <img src="images/20200730_Gradation_NotEqual.png" alt="diff
        Gradation before/after TwoPointTransform" title="diff Gradation
        before/after TwoPointTransform" height="341" width="1023"> </div>
    <div class="post" id="20200724"> <a href="#20200724" class="date">July








































































































































































































































































































































































































































































































        24, 2020</a>
      <h1>How many texture operators are enough?</h1>
      <p>How many texture operators are enough? Probably less than are
        currently in the library.</p>
      <p>On <a href="#20200713">July 13</a> I mentioned giving a short
        presentation on this “work in progress” library to a class at UC
        Santa Cruz. Afterward, one of the class full of bright students
        asked a great question. I don't remember the exact words, but it
        had to do with the large number of texture operators in TexSyn
        (currently about 52). Why so many? How many are actually
        required? Are any missing that could be considered necessary? If
        there are more than required, how could we know that? Aside from
        agreeing that it was a good question, all I could answer was
        that once the Genetic Programming component is in place, I hope
        to do experiments on this issue. For example, using some subset
        of the operators, are results significantly different from using
        the entire set?</p>
      <p>Still it is fun to invent new operators, experiment with them,
        and consider the visual results they produce. Which is to say, I
        tried another prototype. While mildly interesting, it does not
        seem useful enough to keep in the library. Said a different way,
        I think visually similar results can be obtained from
        compositions of existing operators. Below are some results from
        the failed prototype <em>HsvFrom3</em> operator. It takes three
        textures as parameters, the resulting color at any point is
        defined in HSV space by taking the hue from its first texture
        parameter, the saturation from another texture, and the value
        from a third texture. In the first texture example below, the
        hue of the result comes from a red and green <em>Grating</em>,
        the saturation from a blue and white <em>Wrapulence</em>
        texture (seen as curved edges of red/green shading to gray), and
        its value (luminance) from a black and white <em>Turbulence</em>
        (seen as dark “valleys” wandering across the texture). The idea
        for <em>HsvFrom3</em> came up when thinking about problems with
        <em>HsvBox</em>. In the end, neither are included in TexSyn.</p>
      <pre>HsvFrom3(Grating(p1, red, p1 + o1, green, 1, 0.5),
         Wrapulence(p2, p2 + o3, blue, white),
         Turbulence(p3, p3 + o3, black, white))</pre>
      <img src="images/20200724_HsvFrom3_c.png" alt="" title=""
        height="511" width="511">
      <pre>HsvFrom3(ColorNoise(p1, p1 + o2, 0.4),
         ColorNoise(p2, p2 + o2, 0.8),
         Brownian(p3, p3 + o2, white, black))</pre>
      <img src="images/20200724_HsvFrom3_a.png" alt="" title=""
        height="511" width="511">
      <pre>HsvFrom3(ColorNoise(p1, p1 + o2, 0.2),
         ColorNoise(p2, p2 + o2, 0.4),
         Turbulence(p3, p3 + o3, black, white))</pre>
      <img src="images/20200724_HsvFrom3_b.png" alt="" title=""
        height="511" width="511"> </div>
    <div class="post" id="20200722"> <a href="#20200722" class="date">July








































































































































































































































































































































































































































































































        22, 2020</a>
      <h1><em>HsvBox</em> — probably not</h1>
      <p><em>HueOnly</em> had been on my to-do list for a while. I
        wondered if I wanted to have similar operators for the other two
        HSV components. I thought about how nicely <em>RgbBox</em>
        seemed to work, and wondered about an HSV version of it. I built
        a prototype, decided I was not happy with it. On <a
          href="#20200720">July 20</a> I went back to updating <em>HueOnly</em>.
        Here are some textures from the failed prototype <em>HsvBox</em>
        which I will remove from TexSyn. </p>
      <p>First of all <em>HsvBox</em> is not even a good name. <em>RgbBox</em>
        referred to an axis-aligned “sub-box” inside the unit positive
        RGB color cube. RGB space is Cartesian while HSV space uses
        cylindrical coordinates. Hue is an angular parameter. So a range
        of hues is a circular (cylindrical, actually) “sector.” Mapping
        the entire circle onto this sector implies that resulting hues
        will have discontinuities. In the second example below, the hue
        range from red to green was selected. But transforming reds
        adjacent to greens creates an infinitely sharp transition from
        green→red. (See this discontinuity in the second example below.)
        I worked around that by “folding” the hue range. In the third
        example below, the output hue ranges from red→green→red. This
        removes the hue discontinuity but causes other artifacts. For
        example, because of the hue fold, <em>HsvBox</em> has no
        parameters that produce an identity transform. In the end, this
        just seemed too arbitrary and capacious. </p>
      <pre>cn = ColorNoise(Vec2(3, 7), Vec2(3.10, 6.9), 0)<span class="comment"></span></pre>
      <img src="images/20200722_cn.png" alt="cn = ColorNoise(Vec2(3, 7),
        Vec2(3.10, 6.9), 0)" title="cn = ColorNoise(Vec2(3, 7),
        Vec2(3.10, 6.9), 0)" height="511" width="511">
      <p>Here the full hue range [0, 1] is mapped onto [0, ⅓]. Texture
        on left shows hue discontinuity between green and red. Texture
        on right has no discontinuity due to hue folding
        (red→green→red).</p>
      <pre>HsvBox(0, 0.33, 0, 1, 0, 1, cn)</pre>
      <img src="images/20200722_HsvBox_unfolded.png" alt="without fold"
        title="without fold" height="511" width="511"> <img
        src="images/20200722_HsvBox_0_3_0_1_0_1.png" alt="HsvBox(0,
        0.33, 0, 1, 0, 1, cn)" title="HsvBox(0, 0.33, 0, 1, 0, 1, cn)"
        height="511" width="511">
      <pre>HsvBox(0, 0.33, 0, 0.33, 0, 1,    cn)<span class="comment">  // Both H and S map to lower third of range, V is full range.</span>
HsvBox(0, 0.33, 0, 0.33, 0, 0.33, cn)<span class="comment">  // All three HSV components map to lower third of range.</span></pre>
      <img src="images/20200722_HsvBox_0_3_0_3_0_1.png" alt="HsvBox(0,
        0.33, 0, 0.33, 0, 1, cn)" title="HsvBox(0, 0.33, 0, 0.33, 0, 1,
        cn)" height="511" width="511"> <img
        src="images/20200722_HsvBox_0_3_0_3_0_3.png" alt="HsvBox(0,
        0.33, 0, 0.33, 0, 0.33, cn)" title="HsvBox(0, 0.33, 0, 0.33, 0,
        0.33, cn)" height="511" width="511"> </div>
    <div class="post" id="20200720"> <a href="#20200720" class="date">July








































































































































































































































































































































































































































































































        20, 2020</a>
      <h1><em>HueOnly</em></h1>
      <p>For each point in the input texture, <em>HueOnly</em> keeps
        only the hue information from its input texture. It replaces the
        other two HSV components, saturation and value, with constant
        values given as parameters to <em>HueOnly</em>. There was an
        operator called <em>HueOnly</em> in the previous version of
        this library (see <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20090323">March









































































































































































































































































































































































































































































































          23, 2009</a>). This one provides more parametric control, and
        avoids the older one's bug for gray scale input textures. (It
        considered those to be red, corresponding to hue value zero,
        leading to a replacement operator called <em>HueIfAny</em> (see
        <a href="https://www.red3d.com/cwr/texsyn/diary.html#20100226">February









































































































































































































































































































































































































































































































          26, 2010</a>).) Below is shown a color noise texture <code>cn</code>
        used as an example input, followed by four variations of the <code>saturation</code>
        and <code>value</code> parameters.</p>
      <pre>cn = ColorNoise(Vec2(3, 7), Vec2(3.10, 6.9), 0)</pre>
      <img src="images/20200720_cn.png" alt="cn = ColorNoise cn(Vec2(3,
        7), Vec2(3.10, 6.9), 0)" title="cn = ColorNoise cn(Vec2(3, 7),
        Vec2(3.10, 6.9), 0)" height="511" width="511">
      <pre>HueOnly(1.0, 1.0, cn)  <span class="comment">// Full saturation, full brightness.</span>
HueOnly(1.0, 0.5, cn)  <span class="comment">// Full saturation, half brightness.</span></pre>
      <img src="images/20200720_HueOnly_10_10.png" alt="HueOnly(1, 1,
        cn)" title="HueOnly(1, 1, cn)" height="511" width="511"> <img
        src="images/20200720_HueOnly_10_05.png" alt="HueOnly(1, 0.5,
        cn)" title="HueOnly(1, 0.5, cn)" height="511" width="511">
      <pre>HueOnly(0.5, 1.0, cn)  <span class="comment">// Half saturation, full brightness.</span>
HueOnly(0.5, 0.5, cn)  <span class="comment">// Half saturation, half brightness.</span></pre>
      <img src="images/20200720_HueOnly_05_10.png" alt="HueOnly(0.5, 1,
        cn)" title="HueOnly(0.5, 1, cn)" height="511" width="511"> <img
        src="images/20200720_HueOnly_05_05.png" alt="HueOnly(0.5, 0.5,
        cn)" title="HueOnly(0.5, 0.5, cn)" height="511" width="511"> </div>
    <div class="post" id="20200713"> <a href="#20200713" class="date">July








































































































































































































































































































































































































































































































        13, 2020</a>
      <h1>Slightly less stealthy</h1>
      <p>This “lab notebook” has always been public, and I've posted a
        few images from here on facebook. But today TexSyn went a little
        more “wide” when I gave a short presentation (slides <a
          href="http://www.red3d.com/cwr/presentations/2020_TexSyn_preview.pdf">here</a>)
        about this work-in-progress library to a Game AI class taught by
        my friend Dan Shapiro at University of California Santa Cruz.
        Mostly it was just images from this doc to provide a quick
        overview. I did make a few “worked examples” at the top to
        demonstrate the correspondence between code and graphics. </p>
      <p>Images like these color mixing diagrams were also <a
          href="http://www.red3d.com/cwr/texsyn/diary.html#20090408">included</a>
        in the doc about the previous version of the library. They were
        originally intended as an experiment to determine if the library
        needed explicit support for both additive and subtractive color
        mixing. Adding the three spots is straightforward. (This
        corresponds to, say, projecting three primary-colored spots onto
        a white wall in a dark room.) The subtractive version is made by
        inverting the luminance of the three spots—by subtracting them
        from white—so a red spot on black becomes a cyan spot on white.
        Multiplying those together produces this diagram of subtractive
        color mixing. (Which corresponds to, say, a color printer
        overlaying three spots of secondary-colored tint on white paper.
        In practice a black ink is overprinted for strong neutral
        blacks.)</p>
      <pre><span class="comment">// Additive mixing.</span>
Add(red_on_black, Add(green_on_black, blue_on_black)))

<span class="comment">// Subtractive mixing.</span>
Multiply(cyan_on_white, Multiply(magenta_on_white, yellow_on_white)))</pre>
      <img src="images/20200712_additive.png" alt="additive color
        mixing" title="additive color mixing" height="511" width="511">
      <img src="images/20200712_subtractive.png" alt="subtractive color
        mixing" title="subtractive color mixing" height="511"
        width="511">
      <pre><span class="comment">// C++ to define six spots.</span>
Uniform white(1);
Uniform black(0);
Uniform red(1, 0, 0);
Uniform green(0, 1, 0);
Uniform blue(0, 0, 1);
Vec2 p1(0.3, 0);
Vec2 p2 = p1.rotate(2 * pi / 3);
Vec2 p3 = p2.rotate(2 * pi / 3);
float ro = 0.6;
float ri = ro - 0.02;<br>Spot red_on_black(p1, ri, red, ro, black);
Spot green_on_black(p2, ri, green, ro, black);
Spot blue_on_black(p3, ri, blue, ro, black);<br>Subtract cyan_on_white(white, red_on_black);
Subtract magenta_on_white(white, green_on_black);
Subtract yellow_on_white(white, blue_on_black);</pre>
    </div>
    <div class="post" id="20200711"> <a href="#20200711" class="date">July








































































































































































































































































































































































































































































































        11, 2020</a>
      <h1><em>Hyperbolic</em> reconsidered</h1>
      <p>On <a href="#20200706">July 6</a> I described a prototype <em>Hyperbolic</em>
        texture operator which maps (warps) an entire texture plane onto
        a finite disk. <em>Hyperbolic</em> was inspired by the <a
          href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model">Poincaré









































































































































































































































































































































































































































































































          disk model</a> of the hyperbolic plane (see <a
          href="https://mathworld.wolfram.com/PoincareHyperbolicDisk.html">also</a>)
        but now I think I described too direct a link. The general
        character of the remapping is large texture scale in the center,
        becoming asymptotically tiny as it approaches the circle's edge.
        However Poincaré's disk is a special case of this general idea
        of mapping-plane-to-disk with special properties related to
        hyperbolic geometry.</p>
      <p>After several days of conversations with smarter friends on
        email and social media (thanks especially to Saty Raghavachary,
        Alyn Rockwood, David Weilmuenster, and Douglas Dunham) I slowly
        came to understand these issues. The formula I originally
        proposed was not identical to Poincaré's disk, but they both
        fall into a “family” of transforms with two real parameters (<em>a</em>
        and <em>b</em>) which all map a plane onto a disk. The inverse
        transform takes a point <em>d</em> on the destination texture
        plane to a point s on the source texture plane. If <em>d</em>
        is inside the unit disk (‖<em>d</em>‖ &lt; 1) then<em></em>:</p>
      <div style="text-align: center;"><img
          src="images/20200711_equation.png" alt="plane to disk
          equation" title="plane to disk equation" height="69"
          width="134"></div>
      <p>My original transform had <em>a</em>=<em>b</em>=1. As Douglas
        Dunham helpfully derived (via the Gans model of hyperbolic
        geometry) the Poincaré disk model has <em>a</em>=1/2 and <em>b</em>=2.








































































































































































































































































































































































































































































































        As I experimented with various value of <em>a</em> and <em>b</em>
        I decided they were all graphically interesting. So I rewrote my
        TexSyn operator to accept values for <em>a</em> and <em>b</em>
        via parameters called <code>scale</code> and <code>exponent</code>.
        This follows the TexSyn philosophy of “let evolution decide”
        where the evolutionary GP optimization process has the ability
        to manipulate all relevant parameters. The revised parameter
        list for <em>Hyperbolic</em> is now: <code>center</code> and <code>radius</code>
        (to translate and scale the unit disk into texture space), <code>scale</code>
        (the magnification at the center, which falls off toward the
        disk edge) and <code>exponent</code> (controls the rate of
        compression approaching the edge), <code>texture_to_warp</code>
        (the “source” texture), and <code>background_texture</code> (to
        supply pixels outside the disk). The examples below show various
        values of <code>scale</code> and <code>exponent</code>. </p>
      <pre>Hyperbolic(Vec2(), 0.9, 1, 1, los, black)  <span class="comment">// scale=1, exponent=1 (July 6 version)</span></pre>
      <img src="images/20200711_Hyperbolic_10_10.png" alt="" title=""
        height="511" width="511">
      <pre>Hyperbolic(Vec2(), 0.9, 0.5, 2, los, black)  <span class="comment">// scale=0.5, exponent=2 (Poincaré disk version)</span></pre>
      <img src="images/20200711_Hyperbolic_05_20.png"
        alt="Hyperbolic(Vec2(), 0.9, 0.5, 2, los, black)"
        title="Hyperbolic(Vec2(), 0.9, 0.5, 2, los, black)" height="511"
        width="511">
      <pre>Hyperbolic(Vec2(), 0.9, 0.3, 5, los, black)  <span class="comment">// scale=0.3, exponent=5</span></pre>
      <img src="images/20200711_Hyperbolic_03_50.png"
        alt="Hyperbolic(Vec2(), 0.9, 0.3, 5, los, black)"
        title="Hyperbolic(Vec2(), 0.9, 0.3, 5, los, black)" height="511"
        width="511">
      <pre>Hyperbolic(Vec2(), 0.9, 1, 5, los, black)  <span class="comment">// scale=1, exponent=5</span></pre>
      <img src="images/20200711_Hyperbolic_10_50.png"
        alt="Hyperbolic(Vec2(), 0.9, 1, 5, los, black)"
        title="Hyperbolic(Vec2(), 0.9, 1, 5, los, black)" height="511"
        width="511">
      <pre>Hyperbolic(Vec2(), 0.9, 3, 5, los, black)  <span class="comment">// scale=3, exponent=5</span></pre>
      <img src="images/20200711_Hyperbolic_30_50.png"
        alt="Hyperbolic(Vec2(), 0.9, 3, 5, los, black)"
        title="Hyperbolic(Vec2(), 0.9, 3, 5, los, black)" height="511"
        width="511"> </div>
    <div class="post" id="20200708"> <a href="#20200708" class="date">July








































































































































































































































































































































































































































































































        8, 2020</a>
      <h1><em>Affine</em> geometric transformation</h1>
      <p>While parameters to control position, rotation, and scale are
        part of many of the texture operators, I decided to add one to
        do only that. I named it <em>Affine</em>, the formal name for
        this class of transformation. Originally I had a to-do note to
        add “center” arguments to operators <em>Scale</em> and <em>Rotate</em>
        (see <a href="#20200223">February 23</a>). Then I decided to
        combine them, and use the “two point” specification which has
        become my standard API for affine transformations. Included
        below is a semi-exhaustive test suite for the operator. Recall
        the first point gives the new position of the origin (of the
        input texture plane) and the second point is the new position of
        <code>Vec(1,0)</code>, their difference is the x-basis of the
        transformed space. This first example shows the identity
        transform for (0, 0) and (1, 0):</p>
      <pre>Vec2 o(0, 0);                      <span class="comment">// origin</span>
Vec2 mh = Vec2(1, 1) * -0.5;       <span class="comment">// point in the lower left</span>
Vec2 x1(1, 0);                     <span class="comment">// unit vector in +x direction</span>
Vec2 x2 = x1 * 2;
Vec2 xh = x1 * 0.5;
Vec2 d1 = Vec2(1, 1).normalize();  <span class="comment">// unit vector in (1,1) direction</span>
Vec2 d2 = d1 * 2;
Vec2 dh = d1 * 0.5;

Affine(o,  o  + x1, test)</pre>
      <img src="images/20200708_Affine_o_x1.png" alt="Affine(o, o + x1,
        test)" title="Affine(o, o + x1, test)" height="511" width="511">
      <p>Like above, but with center of input texture translated to <code>mh</code>
        in the lower left:</p>
      <pre>Affine(mh, mh + x1, test)</pre>
      <img src="images/20200708_Affine_mh_x1.png" alt="Affine(mh, mh +
        x1, test)" title="Affine(mh, mh + x1, test)" height="511"
        width="511">
      <p>Like above, but with scale 0.5:</p>
      <pre>Affine(mh, mh + xh, test)</pre>
      <img src="images/20200708_Affine_mh_xh.png" alt="Affine(mh, mh +
        xh, test)" title="Affine(mh, mh + xh, test)" height="511"
        width="511">
      <p>Like above, but with scale 2:</p>
      <pre>Affine(mh, mh + x2, test)</pre>
      <img src="images/20200708_Affine_mh_x2.png" alt="Affine(mh, mh +
        x2, test)" title="Affine(mh, mh + x2, test)" height="511"
        width="511">
      <p>Like the previous three, but with 45° rotation
        counterclockwise, because +x basis vectors <code>d1</code>, <code>dh</code>,
        <code>d2</code> each point in the (1, 1) direction:</p>
      <pre>Affine(mh, mh + d1, test)</pre>
      <img src="images/20200708_Affine_mh_d1.png" alt="Affine(mh, mh +
        d1, test)" title="Affine(mh, mh + d1, test)" height="511"
        width="511">
      <pre>Affine(mh, mh + dh, test)</pre>
      <img src="images/20200708_Affine_mh_dh.png" alt="Affine(mh, mh +
        dh, test)" title="Affine(mh, mh + dh, test)" height="511"
        width="511">
      <pre>Affine(mh, mh + d2, test)</pre>
      <img src="images/20200708_Affine_mh_d2.png" alt="Affine(mh, mh +
        d2, test)" title="Affine(mh, mh + d2, test)" height="511"
        width="511"> </div>
    <div class="post" id="20200706"> <a href="#20200706" class="date">July








































































































































































































































































































































































































































































































        6, 2020</a>
      <h1><em>Hyperbolic</em> transform to Poincaré disk</h1>
      <p>Mathematicians will say “Poincaré disk model of the hyperbolic
        plane” while normal people might say “oh, yeah, like Escher's <em>Circle








































































































































































































































































































































































































































































































          Limit</em> series of woodcuts.” For background, see these <a
href="https://www.google.com/search?tbm=isch&amp;q=circle+limit">images</a>,
        this <a
href="http://www.ams.org/publicoutreach/feature-column/fcarc-circle-limit">description</a>
        of the connection via H. S. M. Coxeter, and the work of <a
          href="https://www.d.umn.edu/%7Eddunham/isis4/index.html">Douglas









































































































































































































































































































































































































































































































          Dunham</a>.</p>
      <p>While I was fixing <em>StretchSpot</em> (<a href="#20200702">July








































































































































































































































































































































































































































































































          2</a>) the “fisheye” transform reminded me of Escher's
        woodcuts. There is some similarity in the center of the “bulge”
        — but <em>StretchSpot</em> is a local warp that leaves the
        input texture unchanged outside the given radius — while <em>Hyperbolic</em>
        maps the <strong>entire</strong> input texture into the given
        disk. (Informally, that is why the disk is so “squished” and
        crowded at the outside: most of the infinite input plane is
        mapped very near the edge of the disk.) As a result, the <em>Hyperbolic</em>
        operator requires a second texture parameter to supply which
        that lie outside the Poincaré disk. <em>Hyperbolic</em>'s
        parameters are the <code>center</code> of the resulting disk,
        its <code>radius</code>, the <code>texture_to_warp</code> into
        the disk, and a <code>background_texture</code> for outside the
        disk.</p>
      <p>Note that this <em>Hyperbolic</em> transform is perhaps the
        “worst case scenario” for producing aliasing artifacts from
        point sampling, as used in TexSyn. These textures are smoothed
        by additional subsampling, setting <code>Texture::sqrt_of_aa_subsample_count</code>
        to 20, so using 400 (20x20) point samples per pixel. As with
        other non-linear warps in TexSyn, my assumption is that
        evolutionary optimization will generally select against
        “confetti” artifacts, so to essentially ignore the issue..</p>
      <pre>bw_stripes = Grating(Vec2(-0.05, 0), black, Vec2(0.05, 0), white, 0.1, 0.5)
Hyperbolic(Vec2(0, 0.75), 1.5, bw_stripes, white)</pre>
      <img src="images/20200706_bw_stripes.png" alt="bw_stripes"
        title="bw_stripes" height="511" width="511"> <img
        src="images/20200706_Hyperbolic_bw_stripes.png"
        alt="Hyperbolic(Vec2(0, 0.75), 1.5, bw_stripes, white)"
        title="Hyperbolic(Vec2(0, 0.75), 1.5, bw_stripes, white)"
        height="511" width="511">
      <p>(See <a href="#20200526">May 26</a> for definition of <code>plaid</code>
        texture.)</p>
      <pre>Hyperbolic(Vec2(0, 0), 0.9, plaid, black)</pre>
      <img src="images/20200706_plaid.png" alt="plaid" title="plaid"
        height="511" width="511"> <img
        src="images/20200706_Hyperbolic_plaid.png"
        alt="Hyperbolic(Vec2(0, 0), 0.9, plaid, black)"
        title="Hyperbolic(Vec2(0, 0), 0.9, plaid, black)" height="511"
        width="511">
      <pre>dark_blue = Uniform(0, 0, 0.3)
green = Uniform(0, 1, 0)
los = LotsOfSpots(0.85, 0.04, 0.17, 0.01, 0.02, green, dark_blue)

Hyperbolic(Vec2(0, 0), 0.9, los, black)</pre>
      <img src="images/20200706_los.png" alt="los" title="los"
        height="511" width="511"> <img
        src="images/20200706_Hyperbolic_los.png" alt="Hyperbolic(Vec2(0,
        0), 0.9, los, black)" title="Hyperbolic(Vec2(0, 0), 0.9, los,
        black)" height="511" width="511"> </div>
    <div class="post" id="20200702"> <a href="#20200702" class="date">July








































































































































































































































































































































































































































































































        2, 2020</a>
      <h1><em>StretchSpot</em> <span class="smaller">(“This time for
          sure!” — Bullwinkle J. Moose)</span></h1>
      <p>As mentioned before, TexSyn is the second incarnation of an an
        earlier library. Both include a <em>StretchSpot</em> operator
        which does a nonlinear radial stretch inside a given circle. The
        recent one is described on <a href="#20200116">January 16, 2020</a>,
        the older one on <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20090315">March








































































































































































































































































































































































































































































































          15, 2009</a>. Both of these were fraught with problems, as
        hinted at on <a href="#20200119">January 19, 2020</a>, and in
        the previous library on <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20081260b">December









































































































































































































































































































































































































































































































          26, 2008</a> and the following <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20090101">January









































































































































































































































































































































































































































































































          1</a> and <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20090104">4</a>.
        The previous solutions seemed <em>ad hoc</em> (even for <em>this</em>
        library), awkward (that different cases were required for
        expansion and contraction), inelegant (an inverse transform was
        implemented with a lookup table, based on piecewise linear
        interpolation), and just plain buggy (there are visible
        artifacts in the textures shown on <a href="#20200116">January
          16</a>).</p>
      <p>Yesterday I decided to start from scratch. I think I now have a
        simple, elegant(ish), closed form solution, with only one case
        parameterized by scale. The “lines of code” metric is
        notoriously unreliable, but in this case, the definition of
        class <em>StretchSpot</em> has shrunk from 81 to 24 lines of
        c++ code.</p>
      <p>The new texture operator absolutely does not match the old one.
        The whole reason for this exercise is that I felt the old one
        was wrong in principle, and badly implemented. In the textures
        pairs below, the old one (2009 version, as tweaked last January)
        is on the left and the newest one is on the right. The scale
        factors for each pair are 5.0, 2.0, 1.5, 0.67, 0.5, and 0.2. The
        input texture <code>plaid</code> is as defined most recently on
        <a href="#20200526">May 26</a>.<br>
      </p>
      <pre>radius = 0.9<br>StretchSpot(5.00, radius, Vec2(), plaid)</pre>
      <img src="images/20200702_ss_old_500.png" alt="old
        StretchSpot(5.00, radius, Vec2(), plaid)" title="old
        StretchSpot(5.00, radius, Vec2(), plaid)" height="511"
        width="511"> <img src="images/20200702_ss_new_500.png" alt="new
        StretchSpot(5.00, radius, Vec2(), plaid)" title="new
        StretchSpot(5.00, radius, Vec2(), plaid)" height="511"
        width="511">
      <pre>StretchSpot(2.00, radius, Vec2(), plaid)</pre>
      <img src="images/20200702_ss_old_200.png" alt="old
        StretchSpot(2.00, radius, Vec2(), plaid)" title="old
        StretchSpot(2.00, radius, Vec2(), plaid)" height="511"
        width="511"> <img src="images/20200702_ss_new_200.png" alt="new
        StretchSpot(2.00, radius, Vec2(), plaid)" title="new
        StretchSpot(2.00, radius, Vec2(), plaid)" height="511"
        width="511">
      <pre>StretchSpot(1.50, radius, Vec2(), plaid)</pre>
      <img src="images/20200702_ss_old_150.png" alt="old
        StretchSpot(1.50, radius, Vec2(), plaid)" title="old
        StretchSpot(1.50, radius, Vec2(), plaid)" height="511"
        width="511"> <img src="images/20200702_ss_new_150.png" alt="new
        StretchSpot(1.50, radius, Vec2(), plaid)" title="new
        StretchSpot(1.50, radius, Vec2(), plaid)" height="511"
        width="511">
      <pre>StretchSpot(0.67, radius, Vec2(), plaid)</pre>
      <img src="images/20200702_ss_old_067.png" alt="old
        StretchSpot(0.67, radius, Vec2(), plaid)" title="old
        StretchSpot(0.67, radius, Vec2(), plaid)" height="511"
        width="511"> <img src="images/20200702_ss_new_067.png" alt="new
        StretchSpot(0.67, radius, Vec2(), plaid)" title="new
        StretchSpot(0.67, radius, Vec2(), plaid)" height="511"
        width="511">
      <pre>StretchSpot(0.50, radius, Vec2(), plaid)</pre>
      <img src="images/20200702_ss_old_050.png" alt="old
        StretchSpot(0.50, radius, Vec2(), plaid)" title="old
        StretchSpot(0.50, radius, Vec2(), plaid)" height="511"
        width="511"> <img src="images/20200702_ss_new_050.png" alt="new
        StretchSpot(0.50, radius, Vec2(), plaid)" title="new
        StretchSpot(0.50, radius, Vec2(), plaid)" height="511"
        width="511">
      <pre>StretchSpot(0.20, radius, Vec2(), plaid)</pre>
      <img src="images/20200702_ss_old_020.png" alt="old
        StretchSpot(0.20, radius, Vec2(), plaid)" title="old
        StretchSpot(0.20, radius, Vec2(), plaid)" height="511"
        width="511"> <img src="images/20200702_ss_new_020.png" alt="new
        StretchSpot(0.20, radius, Vec2(), plaid)" title="new
        StretchSpot(0.20, radius, Vec2(), plaid)" height="511"
        width="511">
      <p>[<strong>Update July 3</strong>] I was a little concerned about
        the shape of some of those stripes in the previous version of
        that last image (now replaced). I tried it again using <em>ColoredSpots</em>
        to overlay finer grain details on <code>plaid</code>. The basic
        geometry of the <em>StretchSpot</em> seemed correct, but I was
        not happy with the “taper” of the radial scale transition. I
        felt like there was too much area at the center at nearly the
        lowest scale factor (0.2) and and an abrupt transition outside
        of that where the spots are stretched into long spoke-like
        ellipses. So I rewrote that one-liner in the code, here is
        before and after: </p>
      <pre>StretchSpot(0.20, radius, Vec2(), ColoredSpots(0.8, 0.02, 0.02, 0.01, 0, plaid, Uniform(0)))</pre>
      <img src="images/20200703_ss_spots_020.png" alt="yesterday's
        troubling taper" title="yesterday's troubling taper"
        height="511" width="511"> <img
        src="images/20200704_ss_spots_020.png" alt="today's improved
        taper" title="today's improved taper" height="511" width="511">
      <p>The code transforms the general case to a unit radius disk at
        the origin. Each point is remapped according to its radius. A
        non-linearity is applied to this radius to produce a scale
        factor, which is clipped to one. Before there was a sinusoid
        transition from <code>center_magnification</code> (the first
        argument to <em>StretchSpot</em>) to 1. This sinusoid worked
        well to blend the exterior scale factor of 1 (at the outside of
        the texture in these examples) into the stretched interior, but
        it was “too smooth” at the center. The new transition blends
        that sinusoid with a linear ramp, using a highly exponentiated
        mapping (like a gamma), so that the curve is mostly linear,
        except just before the “1 end” where it smooths into the
        constant 1 value outside: <code>interpolate(pow(r, 5), r,
          sinusoid(r))</code>. After this change I re-rendered the right
        side images above to use this revised “scale taper.” </p>
      <p>As noted on <a
          href="http://www.red3d.com/cwr/texsyn/diary.html#20090104">January









































































































































































































































































































































































































































































































          4, 2009</a>, the bugs and “near misses” for this operator
        produce interesting results. This was an early test:</p>
      <img src="images/20200701_cool_bug.png" alt="" title=""
        height="511" width="511"> </div>
    <div class="post" id="20200624"> <a href="#20200624" class="date">June








































































































































































































































































































































































































































































































        24, 2020</a>
      <h1>Adjust output range of fractal textures</h1>
      <p>Yesterday I ran TexSyn's unit tests for the first time in
        several months. All but one passed. I disabled the failing test
        (<code>operators_minimal_test</code>) but will return to it
        soon. I suspect it may be a gamma related issue.</p>
      <p>One lingering problem with the tests was that one of them (<code>noise_ranges</code>)
        was quite slow because it ran a million tests on each of the six
        noise generator utilities. Yes, that seems like a lot, and my
        plan today was that “now that noise output range is all fixed”
        (see <a href="#20200521">May 21</a>, <a href="#20200522">22</a>,
        and <a href="#20200523">23</a>) I could just reduce that to
        something robust but not ridiculous. When I did, the unit test
        started failing.</p>
      <p>Because the unit test is randomly sampling a random pattern,
        the bounds of the output range tend to grow as more and more
        samples are collected. That was how I originally got to such a
        ridiculous number of samples. Today I decided I would rather
        clip off those “one in a million” (litteraly) extreme values. I
        found that each of the fractal noise generators (brownian,
        turbulence, furbulence, and wrapulence) already had clipping
        remappers on their output, so it was just a matter of tweaking
        the “trim pots” for the output level. This effectively
        “tightened” the bounds of raw noise that get mapped to the
        nominal output range [0, 1].</p>
      <p>The upshot is that now the unit tests run very fast, and the
        fractal noise textures have slightly more contrast. This is the
        sort of not-backward-compatibility I have tried to avoid, but I
        think this is such a subtle change I am willing to live with it.
        Here are two textures from recent posts in this document (see <a
          href="#20200620">June 20</a> and <a href="#20200603">June 3</a>)
        before this change on the left, after this change on the right:</p>
      <img src="images/20200620_noise0.png" alt="by_noise before today's
        change" title="by_noise before today's change" height="511"
        width="511"> <img src="images/20200624_Wrapulence.png" alt=""
        title="" height="511" width="511"> <br>
      <img src="images/20200603_cn.png" alt="" title="" height="511"
        width="511"> <img src="images/20200624_ColorNoise.png" alt="cn
        after today's slight contrast increase" title="cn after today's
        slight contrast increase" height="511" width="511"> </div>
    <div class="post" id="20200622"> <a href="#20200622" class="date">June








































































































































































































































































































































































































































































































        22, 2020</a>
      <h1>Degenerate “two point” spaces</h1>
      <p>Several texture operators have two <em>Vec2</em> point
        parameters to define a local 2d coordinate space (scale,
        rotation, translation). Originally <em>Gradation</em> and <em>Grating</em>
        used this approach. Then (starting <a href="#20200531">May 31</a>)
        the noise textures were changed to use this same “two point”
        approach. </p>
      <p>However, if the two points happen to be identical, the space
        will have zero scale and an undefined orientation. Generally
        this would lead to an arbitrary result, often a blend fraction
        of zero, so passing through the first texture parameter and
        ignoring the other. In the cases of <em>Gradation</em> and <em>Grating</em>
        I originally decided to define the result as an blend of the two
        inputs (a uniform blend fraction of 0.5). Today I cleaned up the
        handling of this in <em>Gradation</em> and <em>Grating</em>,
        then changed the base <em>Noise</em> operator to follow the
        same convention for all noise textures. In each pair below, the
        first texture has a space defined by two identical points (<code>p0</code>),








































































































































































































































































































































































































































































































        and the second texture has a space defined by two distinct
        points (<code>p1</code> and <code>p2</code>).</p>
      <pre>p0 = Vec2(0, 0)
p1 = Vec2(0.1, 0.1)
p2 = Vec2(-0.1, -0.1)
black = Uniform(0)
cyan = Uniform(0, 1, 1)
magenta = Uniform(1, 0, 1)

Gradation(p0, cyan, p0, magenta)
Gradation(p1, cyan, p2, magenta)</pre>
      <img src="images/20200622_degenerate_0.png" alt="Gradation(p0,
        cyan, p0, magenta)" title="Gradation(p0, cyan, p0, magenta)"
        height="511" width="511"> <img
        src="images/20200622_degenerate_1.png" alt="Gradation(p1, cyan,
        p2, magenta)" title="Gradation(p1, cyan, p2, magenta)"
        height="511" width="511">
      <pre>Grating(p0, cyan, p0, magenta, 0.5, 0.5)
Grating(p1, cyan, p2, magenta, 0.5, 0.5)</pre>
      <img src="images/20200622_degenerate_4.png" alt="Grating(p0, cyan,
        p0, magenta, 0.5, 0.5)" title="Grating(p0, cyan, p0, magenta,
        0.5, 0.5)" height="511" width="511"> <img
        src="images/20200622_degenerate_5.png" alt="Grating(p1, cyan,
        p2, magenta, 0.5, 0.5)" title="Grating(p1, cyan, p2, magenta,
        0.5, 0.5)" height="511" width="511">
      <pre>grate_c = Grating(Vec2(), cyan, Vec2(0.1, 0), black, 0.2, 0.5)
grate_m = Grating(Vec2(), magenta, Vec2(0.1, 0), black, 0.2, 0.5)<br>
Gradation(p0, grate_c, p0, grate_m)
Gradation(p1, grate_c, p2, grate_m)</pre>
      <img src="images/20200622_degenerate_2.png" alt="Gradation(p0,
        grate_c, p0, grate_m)" title="Gradation(p0, grate_c, p0,
        grate_m)" height="511" width="511"> <img
        src="images/20200622_degenerate_3.png" alt="Gradation(p1,
        grate_c, p2, grate_m)" title="Gradation(p1, grate_c, p2,
        grate_m)" height="511" width="511">
      <pre>Turbulence(p0, p0, grate_c, grate_m)
Turbulence(p1, p2, grate_c, grate_m)</pre>
      <img src="images/20200622_degenerate_6.png" alt="Turbulence(p0,
        p0, grate_c, grate_m)" title="Turbulence(p0, p0, grate_c,
        grate_m)" height="511" width="511"> <img
        src="images/20200622_degenerate_7.png" alt="Turbulence(p1, p2,
        grate_c, grate_m)" title="Turbulence(p1, p2, grate_c, grate_m)"
        height="511" width="511"> </div>
    <div class="post" id="20200620"> <a href="#20200620" class="date">June








































































































































































































































































































































































































































































































        20, 2020</a>
      <h1><em>CotsMap</em> operator</h1>
      <p>After rewriting and testing the underlying code from Jarek
        Rossignac, these are example textures made with the new <em>CotsMap</em>
        operator. Its parameters are four <em>Vec2</em>s and an input <em>Texture</em>.
        The <em>Vec2</em>s are the four control points: positions to
        where the corners of the input unit square will be deformed. For
        example, this COTS map provides a sharp non-linear contraction
        in the lower left with a slight radial curvature:</p>
      <pre>black = Uniform(0)
yellow = Uniform(1, 1, 0)
by_noise = Wrapulence(Vec2(2, 3), Vec2(2.5, 3.5), black, yellow)<br><br>CotsMap(Vec2(-0.4, -0.5), Vec2(-0.5, -0.4), Vec2(+0.2, +0.8), Vec2(+0.8, -0.2), by_noise)</pre>
      <img src="images/20200620_noise0.png" alt="by_noise"
        title="by_noise" height="511" width="511"> <img
        src="images/20200620_noise1.png" alt="CotsMap(a, b, c, d,
        by_noise)" title="CotsMap(a, b, c, d, by_noise)" height="511"
        width="511">
      <p>Here a gentle curvature in two directions:</p>
      <pre>gray = Uniform(0.5)
blue = Uniform(0, 0, 1)
green = Uniform(0, 1, 0)
grate0 = Grating(Vec2(), blue, Vec2(0.1, 0), gray, 1, 0.5)
grate1 = Grating(Vec2(), green, Vec2(0, 0.1), gray, 1, 0.5)
bg_grid = Add(grate0, grate1)
<br>CotsMap(Vec2(+0.1, -0.6), Vec2(+0.7, +0.2), Vec2(-0.5, +0.9), Vec2(-0.8, -0.5), bg_grid)</pre>
      <img src="images/20200620_grid0.png" alt="bg_grid" title="bg_grid"
        height="511" width="511"> <img src="images/20200620_grid1.png"
        alt="CotsMap(a, b, c, d, bg_grid)" title="CotsMap(a, b, c, d,
        bg_grid)" height="511" width="511">
      <p>Here are several variations on two nested <em>CotsMap</em>
        operators. Generally COTS transforms impart log spiral curvature
        along the two orthogonal axes of the input texture. Nesting them
        allows more complex kinds of curvature.</p>
      <pre>magenta = Uniform(1, 0, 1)<br>mg_los = LotsOfSpots(0.8, 0.05, 0.05, 0.01, 0.02, magenta, gray)<br>
CotsMap(Vec2(+0.1, -0.2), Vec2(+0.7, +0.8), Vec2(-0.3, +0.4), Vec2(-0.5, -0.6),
        CotsMap(Vec2(-0.4, -0.5), Vec2(-0.5, -0.4), Vec2(+0.2, +0.8), Vec2(+0.8, -0.2),
                mg_los))</pre>
      <img src="images/20200620_los0.png" alt="mg_los" title="mg_los"
        height="511" width="511"> <img src="images/20200620_los1.png"
        alt="CotsMap(a, b, c, d, CotsMap(e, f, g, h, mg_los)) 1"
        title="CotsMap(a, b, c, d, CotsMap(e, f, g, h, mg_los)) 1"
        height="511" width="511">
      <p>And two more variations on this theme:</p>
      <pre>CotsMap(Vec2(+0.2, +0.1), Vec2(+0.4, +0.7), Vec2(+0.5, +0.6), Vec2(+0.7, +0.4),
        CotsMap(Vec2(-0.7, -0.2), Vec2(-0.5, -0.4), Vec2(-0.6, -0.3), Vec2(-0.1, -0.6),
                mg_los))<br><br>CotsMap(Vec2(+0.6, +0.0), Vec2(+0.2, +0.8), Vec2(+0.7, +0.4), Vec2(+0.8, +0.6),
        CotsMap(Vec2(-0.6, -0.2), Vec2(-0.3, -0.3), Vec2(-0.2, -0.5), Vec2(-0.7, -0.6),
                mg_los))</pre>
      <img src="images/20200620_los2.png" alt="CotsMap(a, b, c, d,
        CotsMap(e, f, g, h, mg_los)) 2" title="CotsMap(a, b, c, d,
        CotsMap(e, f, g, h, mg_los)) 2" height="511" width="511"> <img
        src="images/20200620_los3.png" alt="CotsMap(a, b, c, d,
        CotsMap(e, f, g, h, mg_los)) 3" title="CotsMap(a, b, c, d,
        CotsMap(e, f, g, h, mg_los)) 3" height="511" width="511"> </div>
    <div class="post" id="20200617"> <a href="#20200617" class="date">June








































































































































































































































































































































































































































































































        17, 2020</a>
      <h1>Prototype COTS maps</h1>
      <p>Shortly after adding <em>MobiusTransform</em> on <a
          href="#20200128">January 28</a>, I saw a listing of papers in
        the next issue of <strong>ACM Transactions on Graphics</strong>.
        One paper seemed related to the Möbius transformation: <a
          href="https://doi.org/10.1145/3267346">Corner-operated
          Tran-similar (COTS) Maps, Patterns, and Lattices</a> by <a
          href="https://www.cc.gatech.edu/%7Ejarek/">Jarek Rossignac</a>.
        He patiently answered my questions and told me sample code was
        forthcoming (now <a
          href="https://github.com/jarek-rossignac/COTS">here</a>, in
        the form of an interactive demo).</p>
      <p>Jarek begins his paper saying “The planar COTS map proposed
        here takes the unit square to a region R bounded by four
        log-spiral edges. It is Corner-operated (controlled by the four
        corners of R) and Tran-similar (it maps translations to
        similarities)...” But as it turns out, more generally, these
        COTS transformations define a mapping from the real plane ℝ²
        onto itself. This is exactly the property needed by a TexSyn
        “warping” operator. COTS maps are described as 2d “curvilinear
        rectangles” bounded by four log-spiral edges which can be
        specified, and directly manipulated, using four control points
        at the “corners.” COTS maps have several other useful
        properties, which are described in Jarek's paper and interactive
        demo. This is Figure 1 of the paper, “Examples of the 4DoF
        distortion-space of the COTS map”:</p>
      <img src="images/20200617_COTS_fig_1.jpg" alt="Figure 1 of
        “Corner-operated Tran-similar (COTS) Maps, Patterns, and
        Lattices”" title="Figure 1 of “Corner-operated Tran-similar
        (COTS) Maps, Patterns, and Lattices”" height="582" width="1000">
      <p>The demo code was written in the <a
          href="https://processing.org/">Processing</a> language. I took
        a portion of it, converting it to c++, and changing it to use
        TexSyn's <em>Vec2</em> class in place of its own. I defined a
        prototype <em>CotsMap</em> texture operator, initially just to
        test the geometric code, by showing which points are inside the
        rectangle. This shows the first test, an image of the unit
        square under a COTS transform defined by the four highlighted
        corners:</p>
      <img src="images/20200617_COTS_in_or_out.png" alt="image of unit
        square transformed by COTS map" title="image of unit square
        transformed by COTS map" height="511" width="511">
      <p>Then I defined the “local space” of the unit square is
        procedurally colored using HSV so hue varies across <em>x</em>,
        saturation varies along <em>y</em>, and value is 0.5:</p>
      <img src="images/20200617_COTS_UV_to_HS_inside.png" alt="Unit
        square with UV converted to HS of HSV color space" title="Unit
        square with UV converted to HS of HSV color space" height="511"
        width="511">
      <p>This is the same COTS transform, without clipping to the unit
        square. Here each (per pixel) sample point is passed through the
        inverse COTS map, which defines a pattern everywhere on the
        texture plane:</p>
      <img src="images/20200617_COTS_UV_to_HS.png" alt="Like previous,
        without clipping to unit square." title="Like previous, without
        clipping to unit square." height="511" width="511">
      <p>Two more versions of the same COTS maps, now operating in the
        normal TexSyn fashion, warping a procedurally defined <em>Texture</em>:
        here a pair of crossed <em>Gratings</em> and a <em>Furbulence</em>
        noise pattern. With this choice of corners (shown as four
        colored rings) the <em>CotsMap</em> texture operator is
        behaving rather like the <em>Wrap</em> operator ( <a
          href="#20200115">January 15</a>):</p>
      <img src="images/20200617_COTS_grid.png" alt="COTS transformation
        of grid texture." title="COTS transformation of grid texture."
        height="511" width="511"> <img
        src="images/20200617_COTS_noise.png" alt="COTS transformation of
        noise texture." title="COTS transformation of noise texture."
        height="511" width="511"> </div>
    <div class="post" id="20200614"> <a href="#20200614" class="date">June








































































































































































































































































































































































































































































































        14, 2020</a>
      <h1>Refactor <code>interpolate</code> — floating point versus
        math</h1>
      <p>“Nothing Is Easy” — Jethro Tull</p>
      <p>Following on from <a href="#20200610">June 10</a>, I was going
        to finish switching textures from using raw <code>interpolate()</code>
        to the new <code>Texture::interpolatePointOnTextures()</code>
        utility. I started with <em>SoftMatte</em>, I duplicated it
        then changed this “<em>SoftMatte2</em>” to use the new utility,
        a trivial change. I made a little test case and rendered it.
        Everything looked fine. Then “just to be sure” (famous last
        words) I compared the new and old version with <code>Texture::diff()</code>
        to ensure they produced exactly the same texture. They did not.
      </p>
      <p>I measured super tiny mismatches (e.g., RGB component
        differences of 5.96046e-08) in about 0.1% of the pixels. Here is
        the code, and a version of <code>Texture::diff()</code>
        modified to show a binary map of mismatched pixels (since the
        actual brightness differences were too small to be seen):</p>
      <pre>p1 = Vec2(0, 0)<br>p2 = Vec2(0.1, 0.2)<br>red = Uniform(1, 0, 0)
blue = Uniform(0, 0, 1)
green = Uniform(0, 1, 0)
white = Uniform(1, 1, 1)
black = Uniform(0, 0, 0)
noise1 = Noise(p1, p2, red, blue)
noise2 = Brownian(p1, p2, blue, green)<br>grate = Grating(p1, white, p2, black, 0.5, 0.5)
<br>Texture::diff(SoftMatte(grate, noise1, noise2),
              SoftMatte2(grate, noise1, noise2)</pre>
      <img src="images/20200611_a_few_bad_pixels.png"
        alt="Texture::diff(SoftMatte(grate, noise1, noise2),
        SoftMatte2(grate, noise1, noise2)"
        title="Texture::diff(SoftMatte(grate, noise1, noise2),
        SoftMatte2(grate, noise1, noise2)" height="333" width="999">
      <p>While aimlessly searching for the cause of this, I noticed it
        was the <code> alpha==1</code> case where it was happening, so
        substituted a uniform <code>white</code> texture for <code>grate</code>.
        This increased the frequency of mismatched pixels to about 0.7%
        (which seemed to roughly follow the most blue parts of the <code>noise2</code>
        texture):</p>
      <pre>Texture::diff(SoftMatte(white, noise1, noise2),
              SoftMatte2(white, noise1, noise2))</pre>
      <img src="images/20200613_show_mismatches.png"
        alt="Texture::diff(SoftMatte(white, noise1, noise2),
        SoftMatte2(white, noise1, noise2))"
        title="Texture::diff(SoftMatte(white, noise1, noise2),
        SoftMatte2(white, noise1, noise2))" height="333" width="999">
      <p>On a hunch I Googled “5.96046e-08” and decided this was
        probably an issue specifically related to floating point
        representation. I tried rewriting (“refactoring” in the original
        mathematical sense) the body of the basic generic interpolation
        utility (a c++ “template”):</p>
      <pre><span class="comment">// Before, with one multiply:</span>
return x0 + ((x1 - x0) * alpha);

<span class="comment">// After, with two multiplies:</span>
return (x0 * (1 - alpha)) + (x1 * alpha);</pre>
      <p>Algebraically these are equivalent, but have “different
        rounding properties” when used with floating point arithmetic,
        as Robert Bridson explained when I asked about this. With this
        change, <code>interpolate()</code> and <code>Texture::interpolatePointOnTextures()</code>
        now match exactly, as verified by <code>Texture::diff()</code>.
        The upshot is, I think, that the new code was fine. Instead the
        problem was in the basic <code>interpolate()</code> which long
        predates TexSyn's entire codebase, perhaps by decades. The
        original motivation for the “one multiply” version of linear
        interpolation was to “save time” because “multiplies are
        expensive” — which today are long-outdated rules of thumb.</p>
    </div>
    <div class="post" id="20200610"> <a href="#20200610" class="date">June








































































































































































































































































































































































































































































































        10, 2020</a>
      <h1>Optimization (of questionable value) for inherent matting</h1>
      <p>In the earlier (2008-2010) version of this library, the <em>SoftMatte</em>
        operator performed all interpolation between textures. It had a
        special optimization for when the “alpha” interpolation
        parameter was 0 or 1. In those cases only one of the two
        textures is relevant to the result, so the irrelevant texture
        can be completely ignored. That is, we can skip computing the
        color of the irrelevant texture for a given pixel. This matters
        primarily for the slow convolution-based textures: <em>Blur</em>,
        <em>EdgeEnhance</em>, and <em>EdgeDetect</em>,</p>
      <p>If we assume that alpha ranges uniformly over [0, 1] it would
        only rarely take on those exact values. (And in fact, in TexSyn,
        alpha can be anywhere on the range of floating point numbers:
        [-∞, ∞].) However the pattern generators used by <em>Spot</em>,
        <em>Gradation</em>, and <em>Grating</em> (plus, to a lesser
        extent, the noise textures) will “often” produce values of 0 or
        1. For example <em>Spot</em> is 1 inside its inner radius and 0
        outside its outer radius. <em>Gradation</em> is 0 or 1
        everywhere except in the transition band between two
        half-planes.</p>
      <p>In my simple handmade examples in this document, there tend to
        be lots of pixels where two textures are being interpolated with
        an alpha of 0 or 1. I suspect it will be much less so “in
        production” when genetic programming is breeding textures to
        blend in with photographs of the natural world. However since it
        is easy to implement, I added this supposed optimization. <code>Texture::interpolatePointOnTextures()</code>—a









































































































































































































































































































































































































































































































        new utility to be called from specific overloads of <code>Texture::getColor()</code>—takes









































































































































































































































































































































































































































































































        an alpha, a position, and two texture references. I updated <em>Spot</em>,
        <em>Gradation</em>, and <em>Grating</em> to use this utility. I
        should later do the same for the noise textures.</p>
      <p>I also decided to roll back the “gamma correction and waveform
        generators” experiment proposed on <a href="#20200518">May 18</a>
        (and <a href="#20200510a">May 10</a>). I don't think it
        reliably does what I hoped it would (because it de-gamma-s the
        waveform, not the full color result) and it just seem counter to
        the approach: “work in linear space, apply output gamma only at
        very end of process.”</p>
      <p>As to the best result that this optimization might lead to,
        consider this <code>grid</code> texture, the sum of two
        gratings:</p>
      <pre>b1 = Uniform(Color(0, 0, 1.0))
b2 = Uniform(Color(0, 0, 0.2))
g1 = Uniform(Color(0, 0.7, 0))
g2 = Uniform(Color(0, 0.05, 0))
blues = Grating(Vec2(), b1, Vec2(-0.03, 0.09), b2, 0.2, 0.2)
greens = Grating(Vec2(), g1, Vec2(0.03, 0.09), g2, 0.2, 0.2)
grid = Add(blues, greens)</pre>
      <img src="images/20200610_background.png" alt="" title=""
        height="511" width="511">
      <p>Here is the <em>Blur</em> of <code>grid</code> with a kernel
        width of 0.1:</p>
      <pre>blurred = Blur(0.1, grid)</pre>
      <img src="images/20200610_blurred.png" alt="" title=""
        height="511" width="511">
      <p>And here is a small <em>Spot</em> with the <code>blurred</code>
        texture inside and <code>grid</code> outside. Before today's
        change, for each pixel rendered—that is, for each call to that <em>Spot</em>'s









































































































































































































































































































































































































































































































        <code>getColor()</code>—calls were made to <code>getColor()</code>
        for both <code>blurred</code> and <code>grid</code>. For all
        the pixels outside the spot, the color returned from <code>blurred</code>
        is multiplied by a weight of zero. With today's fix, the
        expensive <em>Blur</em> is only called where its weight is
        non-zero: inside a circle of radius 0.2 around the origin. </p>
      <p>The before and after textures are identical, but the new one
        was about <strong>17 times faster</strong> than the old one. As
        described above, the key ratio is not that 1:17 but rather how
        often that speed-up would actually occur in practice.</p>
      <pre>Spot(Vec2(), 0.15, blurred, 0.2, grid)</pre>
      <img src="images/20200610_Spot.png" alt="" title="" height="511"
        width="511">
      <p><strong>Side note:</strong> I had been procrastinating about
        making this change for a while, then was reminded of it by a
        post on LinkedIn by <a
          href="https://www.linkedin.com/in/franknielsen/">Frank Nielsen</a>.
        It was sort of a retroactive “poster” for his 1998 paper <a
href="http://www.sonycsl.co.jp/person/nielsen/PT/groupingquerying/n-grouping.ps">Grouping









































































































































































































































































































































































































































































































          and Querying: A Paradigm to Get Output-Sensitive Algorithms</a>.
        Seeing that made me realize that the rendering improvement above
        was a form of “output-sensitive algorithm” — depending not on
        the number of pixels rendered, but on the number of them that
        would actually appear in the output.<br>
        &nbsp;</p>
    </div>
    <div class="post" id="20200607"> <a href="#20200607" class="date">June








































































































































































































































































































































































































































































































        7, 2020</a>
      <h1>48 textures</h1>
      <p>During the merging, rearranging, bug fixing, etc., I made a
        tool to verify that all expected <em>Texture</em> types were
        defined and operating as intended. I had not been keeping track,
        but it turns out there are now 48 types of <em>Texture</em>s
        defined. There are thumbnail images for each, and below that,
        the actual expressions used to generate them. The bug was
        introduced while making the noise textures use the “two point”
        characterization. It incorrectly applied the noise pattern
        transformation (from two-point) to the input textures instead of
        just the noise pattern.</p>
      <img src="images/20200607_48_textures.png" alt="48 Textures"
        title="48 Textures" height="700" width="700">
      <pre><span class="comment">// These are the TexSyn expressions for those 48 textures:</span>
Uniform(0.5)
Spot(p1, 0.1, t1, 0.2, t2)
Gradation(p1, t1, p2, t2)
Grating(p1, t1, p3, t2, 1, 0.5)
SoftMatte(t1, t2, t3)
Add(t1, t2)
Subtract(t1, t2)
Multiply(t1, t2)
Max(t1, t2)
Min(t1, t2)
AbsDiff(t1, t2)
Noise(p1, p2, t1, t2)
Brownian(p1, p2, t1, t2)
Turbulence(p1, p2, t1, t2)
Furbulence(p1, p2, t1, t2)
Wrapulence(p1, p2, t1, t2)
MultiNoise(p1, p2, t1, t2, 0.5)
ColorNoise(p1, p2, 0.5)
BrightnessToHue(0.5, t1)
Wrap(2, p1, p2, t1)
StretchSpot(5, 1, p1, t1)
Stretch(Vec2(2, 3), p2, t1)
SliceGrating(p3, p2, t1)
SliceToRadial(p3, p2, t1)
SliceShear(p3, p2, t1, Vec2(0.4, 0.1), p1, t2)
Colorize(Vec2(1, 0.2), p1, t2, t3)
MobiusTransform(p3, p1, Vec2(0.4, 0.1), p2, t1)
Scale(0.5, t1)
Rotate(0.5, t1)
Translate(p1, t1)
Blur(0.2, t1)
SoftThreshold(0, 1, t1)
EdgeDetect(0.1, t1)
EdgeEnhance(0.1, 1, t1)
AdjustHue(0.25, t1)
AdjustSaturation(0.5, t1)
AdjustBrightness(0.5, t1)
Twist(10, 2, p1, t1)
BrightnessWrap(0.4, 0.6, t3)
Mirror(p3, p2, t1)
Ring(9, p3, p1, t1)
Row(Vec2(0.1, 0.1), p1, t1)
Shader(Vec3(1, 1, 1), 0.2, t1, t3)
LotsOfSpots(0.8, 0.1, 0.4, 0.05, 0.01, t1, t2)
ColoredSpots(0.8, 0.1, 0.4, 0.05, 0.01, t1, t2)
LotsOfButtons(0.8, 0.1, 0.4, 0.05, 0.01, p1, t1, 1, t2)
Gamma(0.5, t3)
RgbBox(0.2, 1, 0, 0.2, 0.2, 1, t1)

<span class="comment">// Based on these variables:</span>
p1 = Vec2(-0.1, 0)
p2 = Vec2(0.1, 0)
p3 = Vec2(0.4, 0.6)
black = Uniform(Color(0, 0, 0))
white = Uniform(Color(1, 1, 1))
red = Uniform(Color(1, 0, 0))
cyan = Uniform(Color(0, 1, 1))
t1 = Grating(Vec2(0, 0.2), white, Vec2(0, 0), cyan, 0.1, 0.5)
t2 = Grating(Vec2(0.1, 0), black, Vec2(0, 0), red, 0.1, 0.5)
t3 = ColorNoise(p1, p3, 0.2)
</pre>
    </div>
    <div class="post" id="20200605"> <a href="#20200605" class="date">June








































































































































































































































































































































































































































































































        5, 2020</a>
      <h1>Not-so-grand unification of <em>Generator</em> and <em>Operator</em></h1>
      <p>As mentioned on <a href="#20191219">December 19</a>, what I
        later called “inherent matting” (<a href="#20200514">May 14</a>)
        means that almost all <em>Texture</em> classes now have at
        least one <em>Texture</em> among their input parameters.
        (Previously, a <em>Generator</em>, like <em>Spot</em>, had
        parameters of type float, position, and color—but not texture.
        Now instead of an inside and outside <em>Color</em>, its
        parameters now include an inside and and outside <em>Texture</em>.)








































































































































































































































































































































































































































































































        In fact the only Texture classes that now do not take a <em>Texture</em>
        parameter are <em>Uniform</em> and <em>ColorNoise</em>.<br>
        <br>
        As a result, I don't think it makes sense to retain the source
        file <code>Generator.h</code> nor its category-defining base
        class <em>Generator</em>. (Which class was never used for
        anything.) So today I merged the content of <code>Generator.h</code>
        into <code>Operator.h</code>, plus some related cleanup. One
        downside is that <code>Operator.h</code>, which was already
        inconveniently long, is now even longer.</p>
    </div>
    <div class="post" id="20200603"> <a href="#20200603" class="date">June








































































































































































































































































































































































































































































































        3, 2020</a>
      <h1>Experimental <em>RgbBox</em> color remapping</h1>
      <p>I had considered exposing the <code>Color::clipToUnitRGB()</code>
        as a texture operator. It is normally used at the end of the
        texture synthesis pipeline, just before output gamma correction.
        In TexSyn, RGB color components are allowed to take on any
        floating point value, <code>Color::clipToUnitRGB()</code> clips
        them to be non-negative, and clips positive RGB triplets, as if
        they were a 3d ray, against the three “bright” faces of the unit
        color cube. I speculated this sort of clipping might provide
        some additional utility in defining textures.</p>
      <p>I decided to try a more generalized version of this as <em>RgbBox</em>.
        It maps the unit RGB cube to a axis-aligned “box” within the
        unit cube. Its parameters are the min and max bounds of the box
        for each of red, green, and blue — plus an input texture. It
        first does the <code>clipToUnitRGB()</code> then linearly
        “remaps” the range of each component from the interval [0, 1] to
        [min, max]. These examples each use this <code>cn</code>
        texture, similar to the final example <a href="#20200602">yesterday</a>.</p>
      <pre>p1 = Vec2 p1(2, 3)
p2 = p1 + Vec2(-1, 3).normalize() * 0.3
cn = ColorNoise(p1, p2, 0.6)</pre>
      <img src="images/20200603_cn.png" alt="ColorNoise(p1, p2, 0.6)"
        title="ColorNoise(p1, p2, 0.6)" height="511" width="511">
      <pre><span class="comment">// Here red is on[0, 0.3], green on [0.3, 0.6], and blue [0.6, 0.9].</span>
RgbBox(0.0, 0.3,  0.3, 0.6,  0.6, 0.9,  cn)</pre>
      <img src="images/20200603_RgbBox_1.png" alt="RgbBox(0.0, 0.3, 0.3,
        0.6, 0.6, 0.9, cn)" title="RgbBox(0.0, 0.3, 0.3, 0.6, 0.6, 0.9,
        cn)" height="511" width="511">
      <pre><span class="comment">// Here red is pushed up, green is midrange, and blue unchanged.</span>
RgbBox(0.5, 1.0,  0.2, 0.6,  0.0, 1.0,  cn)</pre>
      <img src="images/20200603_RgbBox_2.png" alt="RgbBox(0.5, 1.0, 0.2,
        0.6, 0.0, 1.0, cn)" title="RgbBox(0.5, 1.0, 0.2, 0.6, 0.0, 1.0,
        cn)" height="511" width="511">
      <pre><span class="comment">// Here red is pushed up, green even more so, and blue pushed down.</span>
RgbBox(0.5, 1.0, 0.7, 1.0, 0.0, 0.2, cn)</pre>
      <img src="images/20200603_RgbBox_3.png" alt="RgbBox(0.5, 1.0, 0.7,
        1.0, 0.0, 0.2, cn)" title="RgbBox(0.5, 1.0, 0.7, 1.0, 0.0, 0.2,
        cn)" height="511" width="511">
      <pre><span class="comment">// The floor of red is pushed up (to 0.2) while green and blue are squished below it.</span>
RgbBox(0.2, 1.0,  0.0, 0.2,  0.0, 0.2,  cn)</pre>
      <img src="images/20200603_RgbBox_4.png" alt="RgbBox(0.2, 1.0, 0.0,
        0.2, 0.0, 0.2, cn)" title="RgbBox(0.2, 1.0, 0.0, 0.2, 0.0, 0.2,
        cn)" height="511" width="511"> </div>
    <div class="post" id="20200602"> <a href="#20200602" class="date">June








































































































































































































































































































































































































































































































        2, 2020</a>
      <h1>New “two point” specification for other noise-based textures</h1>
      <p>Following on from <a href="#20200531">May 31</a>, I refactored
        the new “two point” version of <em>Noise</em> to make it easy
        to customize which scalar noise pattern to use. Then I derived
        the other noise patterns from that, leading to less code
        duplication overall. First, here are the other four noise
        patterns—<em>Brownian</em>, <em>Turbulence</em>, <em>
          Furbulence</em>, and <em>Wrapulence</em>—each shown for two
        pairs of points.</p>
      <pre>p1 = Vec2(10, 3)
p2 = p1 + Vec2(-1, 3).normalize() * 0.4
p3 = Vec2(-1, 4)
p4 = p3 + Vec2(4, 1).normalize() * 0.2
red = Uniform(1, 0, 0)
dark_blue = Uniform (0, 0, 0.1)
<br>Brownian(p1, p2, red, dark_blue)
Brownian(p3, p4, red, dark_blue)</pre>
      <img src="images/20200602_Brownian_1.png" alt="Brownian(p1, p2,
        red, dark_blue)" title="Brownian(p1, p2, red, dark_blue)"
        height="511" width="511"> <img
        src="images/20200602_Brownian_2.png" alt="Brownian(p3, p4, red,
        dark_blue)" title="Brownian(p3, p4, red, dark_blue)"
        height="511" width="511">
      <pre>Turbulence(p1, p2, red, dark_blue)
Turbulence(p3, p4, red, dark_blue)</pre>
      <img src="images/20200602_Furbulence_1.png" alt="Turbulence(p1,
        p2, red, dark_blue)" title="Turbulence(p1, p2, red, dark_blue)"
        height="511" width="511"> <img
        src="images/20200602_Furbulence_2.png" alt="Turbulence(p3, p4,
        red, dark_blue)" title="Turbulence(p3, p4, red, dark_blue)"
        height="511" width="511">
      <pre>Furbulence(p1, p2, red, dark_blue)
Furbulence(p3, p4, red, dark_blue)</pre>
      <img src="images/20200602_Turbulence_1.png" alt="Furbulence(p1,
        p2, red, dark_blue)" title="Furbulence(p1, p2, red, dark_blue)"
        height="511" width="511"> <img
        src="images/20200602_Turbulence_2.png" alt="Furbulence(p3, p4,
        red, dark_blue)" title="Furbulence(p3, p4, red, dark_blue)"
        height="511" width="511">
      <pre>Wrapulence2(p1, p2, red, dark_blue)
Wrapulence2(p3, p4, red, dark_blue)</pre>
      <img src="images/20200602_Wrapulence_1.png" alt="Wrapulence2(p1,
        p2, red, dark_blue)" title="Wrapulence2(p1, p2, red, dark_blue)"
        height="511" width="511"> <img
        src="images/20200602_Wrapulence_2.png" alt="Wrapulence2(p3, p4,
        red, dark_blue)" title="Wrapulence2(p3, p4, red, dark_blue)"
        height="511" width="511">
      <p>In addition <em>MultiNoise</em> was updated to “two points”,
        which look identical to the textures above. </p>
      <p>Finally two <em>ColorNoise</em> with a third parameter of 0.6
        which selects a “disaligned” <em>Furbulence</em> for each
        primary.</p>
      <pre>ColorNoise(p1, p2, 0.6)
ColorNoise(p3, p4, 0.6)</pre>
      <img src="images/20200602_ColorNoise_1.png" alt="ColorNoise(p1,
        p2, 0.6)" title="ColorNoise(p1, p2, 0.6)" height="511"
        width="511"> <img src="images/20200602_ColorNoise_2.png"
        alt="ColorNoise(p3, p4, 0.6)" title="ColorNoise(p3, p4, 0.6)"
        height="511" width="511"> </div>
    <div class="post" id="20200531"> <a href="#20200531" class="date">May








































































































































































































































































































































































































































































































        31, 2020</a>
      <h1>New “two point” specification for <em>Noise</em></h1>
      <p>When the Noise texture was first described on <a
          href="#20200103">January 3</a>, it included this design note:
        <span class="designnote">I don't like that the noise pattern is
          always “axis aligned” in this formulation. I could add a
          rotation angle. But I am leaning toward changing to a
          specification with two points (<em>Vec2</em>s)—like used in <em>Gradation</em>—or








































































































































































































































































































































































































































































































          a point and a basis vector to specify a whole transform:
          translation, rotation, and scale.</span> </p>
      <p>That change has now been made. I was worried back then about
        making a non-backward-compatible change. But over the last month
        I have developed a workable scheme for backward compatibility.
        (So the old specification still works after the new one was
        added.) In the new style, the parameters to <em>Noise</em> are:
        two points and two textures. The two points define a full
        transformation of the texture plane including translation,
        rotation, and scale. It operates like “two finger” gestures on a
        touch screen (drag, pinch, twist). Below are two identical
        textures, one defined by the old specification (with center
        point and scale) and one by the new specification with two
        points. In the latter, the origin point is indicated with a red
        spot, the other point is green. (Note the magenta “bulb” near
        the red point and the green point nearby. The noise is
        transformed to keep the two points aligned with the
        corresponding parts of the texture, as in the other examples
        below.)</p>
      <pre><span class="comment">// Full texture expression shown at end of this post.</span>
s = 0.4
Noise(s, Vec2(0, 0), gray, magenta)
Noise(Vec2(0, 0), Vec2(s, 0), gray, magenta)</pre>
      <img src="images/20200531_old_Noise.png" alt="Noise(s, Vec2(0, 0),
        gray, magenta)" title="Noise(s, Vec2(0, 0), gray, magenta)"
        height="511" width="511"> <img
        src="images/20200531_new_Noise_1.png" alt="Noise(Vec2(0, 0),
        Vec2(s, 0), gray, magenta)" title="Noise(Vec2(0, 0), Vec2(s, 0),
        gray, magenta)" height="511" width="511">
      <p>The <em>Noise</em> pattern's transform is based on the line
        segment between the two points. It can be thought of as
        corresponding to the unit basis vector of the texture's <strong>x</strong>
        axis. The texture center moves to the first point. The texture
        rotates to align with the tangent vector of the line segment.
        The texture scales with the length of the segment. In these
        examples, both points are within the unit radius rendered extent
        of the texture. They are in fact allowed to be anywhere on the
        texture plane. In the textures below, the two points are
        symmetric about the origin. In the second one they have each
        moved to 1/4 of the previous distance. This changes the scale
        while position and orientation stay fixed.</p>
      <pre>Noise(Vec2(0.38, 0.13), Vec2(-0.38, -0.13), gray, magenta)
Noise(Vec2(0.09, 0.03), Vec2(-0.09, -0.03), gray, magenta)</pre>
      <img src="images/20200531_new_Noise_2.png" alt="Noise(Vec2(0.38,
        0.13), Vec2(-0.38, -0.13), gray, magenta)"
        title="Noise(Vec2(0.38, 0.13), Vec2(-0.38, -0.13), gray,
        magenta)" height="511" width="511"> <img
        src="images/20200531_new_Noise_3.png" alt="Noise(Vec2(0.09,
        0.03), Vec2(-0.09, -0.03), gray, magenta)"
        title="Noise(Vec2(0.09, 0.03), Vec2(-0.09, -0.03), gray,
        magenta)" height="511" width="511">
      <p>Two more examples:</p>
      <pre>Noise(Vec2(-0.3, -0.8), Vec2(0.2, 0.8), gray, magenta)<br>Noise(Vec2(-0.3, 0.7), Vec2(-0.25, 0.68), gray, magenta)</pre>
      <img src="images/20200531_new_Noise_4.png" alt="Noise(Vec2(-0.3,
        -0.8), Vec2(0.2, 0.8), gray, magenta)" title="Noise(Vec2(-0.3,
        -0.8), Vec2(0.2, 0.8), gray, magenta)" height="511" width="511">
      <img src="images/20200531_new_Noise_5.png" alt="Noise(Vec2(-0.3,
        0.7), Vec2(-0.25, 0.68), gray, magenta)" title="Noise(Vec2(-0.3,
        0.7), Vec2(-0.25, 0.68), gray, magenta)" height="511"
        width="511">
      <p>...</p>
      <pre><span class="comment">// The textures above were defined by this expression. (No Spots for first one.)</span>
Spot(p1, spot_ir, red, spot_or,
     Spot(p2, spot_ir, green, spot_or,
          SoftMatte(SoftThreshold(0.50,
                                  0.53,
                                  <em><strong>Noise(p1, p2, black, white)</strong></em>),
                    gray,
                    magenta)))</pre>
    </div>
    <div class="post" id="20200526"> <a href="#20200526" class="date">May








































































































































































































































































































































































































































































































        26, 2020</a>
      <h1>Prototype anti-aliasing</h1>
      <p>Describing <em>MobiusTransform</em> on <a href="#20200128">January








































































































































































































































































































































































































































































































          28</a>, I point out the aliasing (“under-sampling confetti”)
        in areas where the transform strongly compresses its input
        texture. In a transformation like this, spatial frequencies in
        the input can be shifted up arbitrarily high by geometric
        compression. The point sampling used by TexSyn's rendering will
        inevitably fail in this case. Since then, <em>Blur</em> was
        implemented with a stochastic sampling approach using a jittered
        grid of subsamples. I realized the same approach, even the same
        code, could be re-purposed to approximate area sampling with a
        jittered grid of point samples within each pixel. Shown below is
        the fourth example from <a href="#20200128">January 28</a>
        (updated for changes in Grating and gamma correction) then
        renderings of the same texture at 5x5, 7x7, and 10x10
        subsampling per pixel.</p>
      <p>While this additional sampling helps significantly, the test
        case below is pretty hard to get correct, and indeed, there is
        still some low amplitude confetti noise in the region of high
        contraction. Also note that this is a global setting, so cannot
        be used “only where needed.” Absolutely no adaptive detection of
        aliasing artifacts is supported.</p>
      <pre>float f = 1.0
float d = 0.1  <span class="comment">// Changed from 0.3 to 0.1 to compensate for May 6 gamma change.</span>
full_green = Uniform(Color(f, 0, 0))
dark_green = Uniform(Color(d, 0, 0))
full_red = Uniform(Color(0, f, 0))
dark_red =Uniform(Color(0, d, 0))
red_stripes = Grating(Vec2(0, 0), full_red, Vec2(0.1, 0.1), dark_red, 0.3, 0.5)
green_stripes = Grating(Vec2(0, 0), full_green, Vec2(-0.1, 0.1), dark_green, 0.3, 0.5)
plaid = Add(red_stripes, green_stripes);

MobiusTransform(Vec2( 0.246782, -0.338772),
                Vec2(-1.73553,  -0.969689),
                Vec2( 0.535863, -1.75309),
                Vec2(-0.201844, -0.394775),
                plaid)</pre>
      <img src="images/20200526_mt_ss_1.png" alt="MobiusTransform,
        subsamples=1" title="MobiusTransform, subsamples=1" height="511"
        width="511">
      <p>5x5 jittered grid, 25 subsamples per pixel.</p>
      <img src="images/20200526_mt_ss_25.png" alt="MobiusTransform,
        subsamples=25 (5x5)" title="MobiusTransform, subsamples=25
        (5x5)" height="511" width="511">
      <p>7x7 jittered grid, 49 subsamples per pixel.</p>
      <img src="images/20200526_mt_ss_49.png" alt="MobiusTransform,
        subsamples=49 (7x7)" title="MobiusTransform, subsamples=49
        (7x7)" height="511" width="511">
      <p>10x10 jittered grid, 100 subsamples per pixel.</p>
      <img src="images/20200526_mt_ss_100.png" alt="MobiusTransform,
        subsamples=100 (10x10)" title="MobiusTransform, subsamples=100
        (10x10)" height="511" width="511">
      <p><strong>Update the next morning</strong>: I was curious about
        that remaining “confetti noise” in the previous image. Will that
        persist regardless of how many point samples are averaged
        together? I decided to crank it up to see if I could get the
        noise to fade away. (I have plenty of time: I'm retired, and
        we're in the middle of a pandemic. But I'm being melodramatic,
        the render below took about 5 minutes.) The formerly “confetti”
        area to the right of center is now an apparently flat field of
        yellow. There are eight little “moiré rosettes” arranged in a
        circle around the flat color area. I am not certain if I am
        looking at actual features of this <code>plaid</code> pattern
        as seen through this <em>MobiusTransform</em>, or if they are
        just a deeper level of aliasing artifacts.</p>
      <p>150x150 jittered grid, 22500 subsamples per pixel.</p>
      <img src="images/20200527_mt_ss_22500.png" alt="MobiusTransform,
        subsamples=22500 (150x150)" title="MobiusTransform,
        subsamples=22500 (150x150)" height="511" width="511"> </div>
    <div class="post" id="20200525"> <a href="#20200525" class="date">May








































































































































































































































































































































































































































































































        25, 2020</a>
      <h1>D'oh! Switch back to row-by-row parallelism</h1>
      <p>On <a href="#20200330">March 30</a> I mentioned discovering
        that my multi-threaded texture rendering code essentially
        duplicated a feature already provided in OpenCV: <code><a
href="https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a952ef1a85d70a510240cb645a90efc0d">cv::Mat::forEach()</a></code>.
        It is kind of embarrassing to code from scratch something you
        could have just called from a library. My dignity was partially
        preserved because my version was about 11% faster than the
        similar code from OpenCV. I suspect the difference may be that
        my code created a thread for each <strong>row</strong> of the
        image, while <code>cv::Mat::forEach()</code> makes a thread for
        each <strong>pixel</strong>. It may turn out that the overhead
        for running a new thread, and/or waiting for mutex locks, is in
        the same ballpark as rendering one pixel of a TexSyn texture. If
        so, a thread per pixel may be “excessive parallelism.”</p>
      <p>In any case, I write today because I noticed that the <code>#ifdef</code>
        I used to compare those two implementations, was still there,
        and still set to use the slightly slower OpenCV version. I fixed
        that and tried some tests. Recall that <em>Blur</em> uses a
        stochastic sparse convolution. The good news is that textures
        2-4 all take the same amount of time, unrelated to kernel size.
        The bad news is that results become noisier as kernel size
        increases. This can be adjusted by increasing <code>Blur::sqrt_of_subsample_count</code>
        from its default value of 11, but that is a global setting, and
        so not a convenient solution. (I increased it today to 30 to
        slow down rendering to better measure performance numbers.)</p>
      <pre>grating = Grating grating(Vec2(), yellow, Vec2(0.2, 0), blue, 0.01, 0.5)
Blur(0.1, grating)</pre>
      <img src="images/20200525_grating.png" alt="grating"
        title="grating" height="511" width="511"> <img
        src="images/20200525_Blur_0_1.png" alt="Blur(0.1, grating)"
        title="Blur(0.1, grating)" height="511" width="511">
      <pre>Blur(0.2, grating)
Blur(0.4, grating)</pre>
      <img src="images/20200525_Blur_0_2.png" alt="Blur(0.2, grating)"
        title="Blur(0.2, grating)" height="511" width="511"> <img
        src="images/20200525_Blur_0_4.png" alt="Blur(0.4, grating)"
        title="Blur(0.4, grating)" height="511" width="511"> </div>
    <div class="post" id="20200524"> <a href="#20200524" class="date">May








































































































































































































































































































































































































































































































        24, 2020</a>
      <h1>Inherent matting for other noise textures</h1>
      <p>Following on from <a href="#20200520">May 20</a>, after some
        intervening improvements on the noise utilities, I upgraded <em>Brownian</em>,
        <em>Turbulence</em>, <em>Furbulence</em>, <em>Wrapulence</em>,
        and <em>MultiNoise</em> to use input parameters that are
        textures rather than colors. Here are examples of <em>Noise</em>,
        <em>Brownian</em>, <em>Turbulence</em>, <em>Furbulence</em>,
        and <em>Wrapulence</em>. I tested <em>MultiNoise</em> but its
        output is identical to the others for appropriate parameters —
        its final parameter selects between the noise types below.</p>
      <pre>Noise(0.3, Vec2(), black_red, white_cyan)
Noise(0.3, Vec2(), yellow, red)</pre>
      <img src="images/20200524_Noise_stripes.png" alt="" title=""
        height="511" width="511"> <img
        src="images/20200524_Noise_y_r.png" alt="" title="" height="511"
        width="511">
      <pre>Brownian(0.3, Vec2(), black_red, white_cyan)
Brownian(0.3, Vec2(), yellow, red)</pre>
      <img src="images/20200524_Brownian_stripes.png" alt="Brownian(0.3,
        Vec2(), black_red, white_cyan)" title="Brownian(0.3, Vec2(),
        black_red, white_cyan)" height="511" width="511"> <img
        src="images/20200524_Brownian_y_r.png" alt="Brownian(0.3,
        Vec2(), yellow, red)" title="Brownian(0.3, Vec2(), yellow, red)"
        height="511" width="511">
      <pre>Turbulence(0.3, Vec2(), black_red, white_cyan)
Turbulence(0.3, Vec2(), yellow, red)</pre>
      <img src="images/20200524_Turbulence_stripes.png"
        alt="Turbulence(0.3, Vec2(), black_red, white_cyan)"
        title="Turbulence(0.3, Vec2(), black_red, white_cyan)"
        height="511" width="511"> <img
        src="images/20200524_Turbulence_y_r.png" alt="Turbulence(0.3,
        Vec2(), yellow, red)" title="Turbulence(0.3, Vec2(), yellow,
        red)" height="511" width="511">
      <pre>Furbulence(0.3, Vec2(), black_red, white_cyan)
Furbulence(0.3, Vec2(), yellow, red)</pre>
      <img src="images/20200524_Furbulence_stripes.png"
        alt="Furbulence(0.3, Vec2(), black_red, white_cyan)"
        title="Furbulence(0.3, Vec2(), black_red, white_cyan)"
        height="511" width="511"> <img
        src="images/20200524_Furbulence_y_r.png" alt="Furbulence(0.3,
        Vec2(), yellow, red)" title="Furbulence(0.3, Vec2(), yellow,
        red)" height="511" width="511">
      <pre>Wrapulence(0.3, Vec2(), black_red, white_cyan)
Wrapulence(0.3, Vec2(), yellow, red)</pre>
      <img src="images/20200524_Wrapulence_stripes.png"
        alt="Wrapulence(0.3, Vec2(), black_red, white_cyan)"
        title="Wrapulence(0.3, Vec2(), black_red, white_cyan)"
        height="511" width="511"> <img
        src="images/20200524_Wrapulence_y_r.png" alt="Wrapulence(0.3,
        Vec2(), yellow, red)" title="Wrapulence(0.3, Vec2(), yellow,
        red)" height="511" width="511">
      <p>One more slightly less trivial example: a four color pattern. A
        simple Perlin <em>Noise</em> texture modulates two higher
        frequency textures: a yellow and red <em>Furbulence</em>, and a
        blue and cyan <em>Brownian</em>.</p>
      <pre>Noise(0.2, Vec2(1, 2),
      Furbulence(0.1, Vec2(3, -2), yellow, red),
      Brownian(0.1, Vec2(-1, 5), blue, cyan))</pre>
      <img src="images/20200524_less_trivial.png" alt="less trivial
        example" title="less trivial example" height="511" width="511">
    </div>
    <div class="post" id="20200523"> <a href="#20200523" class="date">May








































































































































































































































































































































































































































































































        23, 2020</a>
      <h1>Fixing noise range, again</h1>
      <p>Well <em>that</em> did not last long. I moved on to making the
        other noise textures conform to the new “inherent matting”
        approach. Then I recalled that all of the multi-octave fractal
        noise generators actually call <code>PerlinNoise::noise2d()</code>
        instead of <code>PerlinNoise::unitNoise2d()</code> which is
        where I made <a href="#20200522">yesterday</a>'s change. The
        problem was deeper, in the “raw” Perlin noise generator, whose
        output range was hand-waving-ly described as being on [-1, 1]
        but measured (over 10 million random <code>Vec2</code>s) to be
        closer to [-0.7, 0.7]. It now maps its internal result on [-0.7,
        0.7] to the nominal output range [-1, 1], as shown in the
        histogram below. Now unitNoise2d() just remaps [-1, 1] values
        onto [0, 1].</p>
      <img src="images/20200523_chart.png" alt="" title="" height="235"
        width="500"> </div>
    <div class="post" id="20200522"> <a href="#20200522" class="date">May








































































































































































































































































































































































































































































































        22, 2020</a>
      <h1>Belatedly adjusting the range of <em>Noise</em></h1>
      <p>I decided the issues I was wrestling with (<em>Noise</em>,
        inherent matting, gamma) were exacerbated by my poorly
        calibrated Perlin noise generator. Specifically, the utility
        function named <code>PerlinNoise::unitNoise2d()</code> promises
        that its output ranges over [0, 1]. But (as the first histogram
        from <a href="#20200521">yesterday</a> showed) its actual range
        looked more like [0.1, 0.85]. These “dead zones” at the top and
        bottom prevented good “inherent matting” — and in combination
        with gamma correction the bottom (dark) dead zone covered nearly
        half of the dynamic range. (I think I had originally assumed (in
        a linear color space) that GP evolution would easily adjust for
        these range artifacts. But now, with gamma correction, it seems
        more problematic.)</p>
      <p>So today I readjusted the output range of <code>PerlinNoise::unitNoise2d()</code>.
        Fortunately it already called the (strict) linear
        range-adjusting utility <code>remapIntervalClip()</code>, in an
        earlier attempt to re-range the raw <code>PerlinNoise::noise2d()</code>
        into [0, 1]. So all I had to do was pick better bounds than the
        previous [-1, 1]. These new values: [-0.75, 0.80], are
        completely <em>ad hoc</em>, based only on measuring what
        happens to come out of the current implementation my noise
        generator. I adjusted the bounds so that in the 100 bucket
        histograms, the top and bottom values were not zero and were not
        much bigger than the adjacent bucket. (That is: bucket 0 has a
        value near that of bucket 1, similarly 98 and 99.) Here are the
        new histograms, after adjusting the output range of <code>PerlinNoise::noise2d()</code>
        for both (the old, obsolete) linear gamma, and for the current
        gamma of 2.2. Compare these to the first two histograms from <a
          href="#20200521">May 21</a>.</p>
      <pre>setDefaultGamma(1)
Noise(0.1, Vec2(), black, white)
setDefaultGamma(2.2)
Noise(0.1, Vec2(), black, white)</pre>
      <img src="images/20200522_Noise_g_1_0.png" alt="Histogram for
        Noise luminance, gamma=1.0" title="Histogram for Noise
        luminance, gamma=1.0" height="229" width="500">
      &nbsp;&nbsp;&nbsp; <img src="images/20200522_Noise_g_2.2.png"
        alt="Histogram for Noise luminance, gamma=2.2" title="Histogram
        for Noise luminance, gamma=2.2" height="229" width="500">
      <p>Here is that grayscale <em>Noise</em> texture at linear gamma
        (just for comparison) and at now default 2.2 gamma.</p>
      <img src="images/20200522_Noise_g_1_0_gray.png" alt="Noise,
        gamma=1.0, grayscale" title="Noise, gamma=1.0, grayscale"
        height="511" width="511"> <img
        src="images/20200522_Noise_g_2_2_gray.png" alt="Noise,
        gamma=2.2, grayscale" title="Noise, gamma=2.2, grayscale"
        height="511" width="511">
      <p>And here are the latter (gamma = 2.2) with the input textures
        as the two striped “inherent matting” test patterns (from <a
          href="#20200514">May 14</a>) and two Uniform textures in
        yellow and red:</p>
      <pre>Noise(0.1, Vec2(), black_red, white_cyan)
Noise(0.1, Vec2(), yellow, red)</pre>
      <img src="images/20200522_Noise_g_2_2_im.png" alt="Noise,
        gamma=2.2, inherent matting" title="Noise, gamma=2.2, inherent
        matting" height="511" width="511"> <img
        src="images/20200522_Noise_g_2_2_colors.png" alt="Noise,
        gamma=2.2, colors" title="Noise, gamma=2.2, colors" height="511"
        width="511"> </div>
    <div class="post" id="20200521"> <a href="#20200521" class="date">May








































































































































































































































































































































































































































































































        21, 2020</a>
      <h1><em>Noise</em> and gamma</h1>
      <p>As mentioned <a href="#20200520">yesterday</a>, I needed to
        measure the luminance distribution of my Perlin noise
        generators. I did a lot of measurement back in <a
          href="#20200103">January</a> but now realize I made the
        mistake of characterizing the luminance distribution only in
        terms of max and min. I was probably thrown off by a few
        outliers, a rare very bright or very dark point in the noise.
        This exaggerated the typical range of the noise generator. Today
        I made a histogram with 100 integer “buckets” . During rendering
        I convert pixel luminance to a index on [0, 99] and increment
        the corresponding bucket. </p>
      <p>Here is how it looked, before <a href="#20200506">May 6</a>,
        with linear gamma. The distribution (see histogram below) is
        centered, but misses at least 10% at both dark and light ends of
        the (sRGB) color space. For “inherent matting” we want to see
        one input texture in the bright areas of noise, and the <strong>other</strong>
        input texture in dark areas of noise. If it never gets below 10%
        or above 90% we will always see a mixture of the input textures.
      </p>
      <pre><span class="comment">// All made with these definitions:</span>
Noise(0.1, Vec2(), white, black)<br>Noise(0.1, Vec2(), white_cyan, black_red)</pre>
      <img src="images/20200521_Noise_1_0.png" alt="Noise grayscale
        gamma=1.0" title="Noise grayscale gamma=1.0" height="511"
        width="511"> <img src="images/20200521_Noise_1_0_im.png"
        alt="Noise matting gamma=1.0" title="Noise matting gamma=1.0"
        height="511" width="511"> <br>
      <img src="images/20200521_Chart_g_1_0.png" alt="Histogram of Noise
        grayscale gamma=1.0" title="Histogram of Noise grayscale
        gamma=1.0" height="229" width="500">
      <p>Below is how it has been since <a href="#20200506">May 6</a> ,
        with gamma of 2.2 applied at the end. I interpret the histogram
        shifting to the right (bright) side as the intended effect of
        gamma correction, pushing up the mid-range values. The result of
        this for “inherent matting” is that one input texture is visible
        in most places, with occasional place were we can see a bit of
        the other input texture.</p>
      <img src="images/20200521_Noise_2_2.png" alt="Noise grayscale
        gamma=2.2" title="Noise grayscale gamma=2.2" height="511"
        width="511"> <img src="images/20200521_Noise_2_2_im.png"
        alt="Noise matting gamma=2.2" title="Noise matting gamma=2.2"
        height="511" width="511"> <br>
      <img src="images/20200521_Chart_g_2_2.png" alt="Histogram of Noise
        grayscale gamma=2.2" title="Histogram of Noise grayscale
        gamma=2.2" height="229" width="500">
      <p>Below is with gamma of 2.2 and a “de-gamma” applied to the
        scalar noise amplitude. This is like the previous one, but
        worse.</p>
      <img src="images/20200521_Noise_2_2_dg.png" alt="Noise grayscale
        gamma=2.2 with de-gamma" title="Noise grayscale gamma=2.2 with
        de-gamma" height="511" width="511"> <img
        src="images/20200521_Noise2_2_2_im.png" alt="Noise matting
        gamma=2.2 with de-gamma" title="Noise matting gamma=2.2 with
        de-gamma" height="511" width="511"> <br>
      <img src="images/20200521_Chart_g_2_2_dg.png" alt="Histogram of
        Noise grayscale gamma=2.2 with de-gamma" title="Histogram of
        Noise grayscale gamma=2.2 with de-gamma" height="229"
        width="500">
      <p> &nbsp; </p>
    </div>
    <div class="post" id="20200520"> <a href="#20200520" class="date">May








































































































































































































































































































































































































































































































        20, 2020</a>
      <h1><em>Noise</em>, inherent matting, and disappointment...</h1>
      <p>I began to convert the various generators based on Perlin noise
        to support recent design changes: to accept general <em>Texture</em>
        references instead of <em>Color</em>s, and to “de-gamma” their
        waveform, which is to say, their basic scalar noise amplitude.
        The first results, here with <em>Noise</em>, were
        disappointing. Here are two versions of <em>Noise</em>, first
        changed to take textures as input (“inherent matting” using the
        same two textures introduced on <a href="#20200514">May 14</a>),








































































































































































































































































































































































































































































































        and then the same with “de-gamma” of noise amplitude:</p>
      <pre>Noise(0.2, Vec2(), white_cyan, black_red)</pre>
      <img src="images/20200520_inherent_matting.png" alt="Noise with
        inherent matting" title="Noise with inherent matting"
        height="511" width="511"> <img
        src="images/20200520_inherent_matting_gamma.png" alt="Noise with
        inherent matting and de-gamma" title="Noise with inherent
        matting and de-gamma" height="511" width="511">
      <p>I find these disappointing because they are so mushy and
        indistinct. Almost the entire texture is a mixture of the two
        input textures. I am trying to decide is this is a real problem,
        or just bad expectations. I think in my mind I was picturing
        something a bit more like this:</p>
      <pre><span class="comment">// What I was hoping to see:</span>
SoftMatte(SoftThreshold(0.2, 0.6, Noise(0.2, Vec2(-5, 3), white, black)),
          white_cyan,
          black_red)</pre>
      <img src="images/20200520_in_my_mind.png" alt="" title=""
        height="511" width="511"> </div>
    <p>I need to look at the distribution of values coming out of the
      basic noise utilities and see if they are what I expect. Yes, I
      know, I should have done this back on <a href="#20200103">January
        3</a>.</p>
    <div class="post" id="20200518"> <a href="#20200518" class="date">May








































































































































































































































































































































































































































































































        18, 2020</a>
      <h1>Gamma correction and waveform generators</h1>
      <p>On <a href="#20200510a">May 10</a> I discuss gamma adjusting
        the <em>Gradient</em> waveform. The goal there was to make sure
        the “top/bright/1” part of the periodic waveform appear as wide
        as the “bottom/dark/0” part of the waveform. I felt unsettled
        about the design issue since there are other places in TexSyn
        that similarly define a scalar/monochrome “matte” texture, then
        use that to <em>SoftMatte</em> between two other textures.
        These experiments use <em>Spot</em> and <em>Gradation</em>.
        The noise textures are in this same category. </p>
      <p>The images below are from <code>Texture::diff()</code> showing
        two given textures and their <em>AbsDiff</em>. The first image
        is the old version, second has de-gamma applied to the <em>Spot</em>
        or <em>Gradation</em> waveform before the interpolation
        happens.</p>
      <p>For the case of Spot, the difference is obvious: in the second
        image the bright central area extends further out along the
        radius. It is less clear which is “better.” Unsurprisingly the
        third “diff image” shows that the two images differ very little
        in the bright center or in the dark outer ring. Almost all of
        the affect of gamma correction is in the mid-tones, leading to
        the “doughnut shaped” difference image.</p>
      <pre>black = Uniform(Color(0, 0, 0))
white = Uniform(Color(1, 1, 1))
<br>Texture::diff(Spot (Vec2(), 0.1, white, 1.0, black),
              Spot2(Vec2(), 0.1, white, 1.0, black))</pre>
      <img src="images/20200518_Spot_diff.png" alt="Spot
        Texture::diff()" title="Spot Texture::diff()" height="333"
        width="999">
      <p>For the <em>Gradation</em> comparison below, I think the new
        version (second image) is better. The mid-point of the gray
        scale transition seems to fall closer to the geometric center.</p>
      <pre>Texture::diff(Gradation (Vec2(0.6, 0.6), black, Vec2(-0.6, -0.6), white),
              Gradation2(Vec2(0.6, 0.6), black, Vec2(-0.6, -0.6), white))</pre>
      <img src="images/20200518_Gradation_diff.png" alt="Gradation
        Texture::diff()" title="Gradation Texture::diff()" height="333"
        width="999">
      <p><strong>So:</strong> <em>Grating</em> and <em>Gradation</em>
        seem better with “de-gamma-ed” waveform. I will keep this new
        version. I will plan to make the same change to the various
        Perlin noise based operators, as I convert them to “inherent
        matting.” To keep things more consistent, I will re-render
        yesterday's textures to use this new gamma approach.<br>
        &nbsp;</p>
    </div>
    <div class="post" id="20200517"> <a href="#20200517" class="date">May








































































































































































































































































































































































































































































































        17, 2020</a>
      <h1>Inherent matting for <em>Spot</em>, <em>Gradient</em>, and <em>Grating</em></h1>
      <p>Continuing with the “inherent matting” theme from <a
          href="#20200514">May 14</a>: three basic <em>Generators</em>
        are changed to take two <em>Texture</em> references where they
        used to allow only <em>Color</em> values. For example, Spot
        previously took an <code>inside_color</code> and an <code>outside_color</code>,
        now it takes an <code>inside_texture</code> and an <code>outside_texture</code>:</p>
      <pre><span class="comment">// Define Spot with white_cyan/black_red as its input parameters.</span>
Spot(Vec2(0, 0), 0.2, white_cyan, 0.9, black_red)</pre>
      <img src="images/20200517_Spot.png" alt="Spot with Texture inputs"
        title="Spot with Texture inputs" height="511" width="511">
      <pre><span class="comment">// </span><span class="comment"><span class="comment">Similarly, a diagonal </span></span><span class="comment"><span class="comment">Gradation using white_cyan/black_red</span>.</span><br>Gradation(Vec2(0.7, -0.7), white_cyan, Vec2(-0.7, 0.7), black_red)</pre>
      <img src="images/20200517_Gradation.png" alt="Gradation with
        Texture inputs" title="Gradation with Texture inputs"
        height="511" width="511">
      <pre><span class="comment">// A </span><span class="comment">Grating using white_cyan/black_red, 20% square-wave with 30% duty-cycle.</span>
Grating(Vec2(0.1, -0.2), white_cyan, Vec2(-0.1, 0.2), black_red, 0.2, 0.3)</pre>
      <img src="images/20200517_Grating.png" alt="Grating with Texture
        inputs" title="Grating with Texture inputs" height="511"
        width="511">
      <pre><span class="comment">// Interesting mistake, first thing I typed in, Grating happens to align with "black_red".</span>
Grating(Vec2(0.1, 0), white_cyan, Vec2(-0.1, 0), black_red, 0.2, 0.5)
</pre>
      <img src="images/20200517_Grating_oops.png" alt="" title=""
        height="511" width="511">
      <pre><span class="comment">// Define "white_cyan"/"black_red" as before, using Uniform texture in place of Color.</span>
black = Uniform(Color(0, 0, 0))
white = Uniform(Color(1, 1, 1))
red = Uniform(Color(1, 0, 0))
cyan = Uniform(Color(0, 1, 1))
white_cyan = Grating(Vec2(0, 0.2), white, Vec2(0, 0), cyan, 0.1, 0.5)
black_red = Grating(Vec2(0.1, 0), black, Vec2(0, 0), red, 0.1, 0.5)
</pre>
    </div>
    <div class="post" id="20200515"> <a href="#20200515" class="date">May








































































































































































































































































































































































































































































































        15, 2020</a>
      <h1>Backward compatibility: our little secret<span
          style="font-family: monospace;"></span><code></code></h1>
      <p>Several recent changes involved retrofitting new features
        (margins between spots, inherent matting, duty cycle) into
        existing texture classes. These required adding or changing
        parameters to texture constructors. My goal has been to, where
        possible, provide backward compatibility for the old version, so
        that old examples (below, in this file) will continue to work.
        This was done using the C++ feature allowing one class
        constructor to invoke another. So for example, consider <a
          href="#20200513">Margins between spots</a>, which added a new
        <code>margin</code> parameter in the middle of the old parameter
        list. I added a new “rewriting constructor” with the old
        parameters that merely forwards to the new constructor,
        inserting the 0 default for <code>margin</code> in its new
        place.</p>
      <p>But it was slightly more complicated for <a href="#20200514">Inherent









































































































































































































































































































































































































































































































          matting</a> where <em>Color</em> parameters were replaced
        with <em>Texture</em> references. The solution—to be used
        solely for this backward compatibility constructors—is a new
        function <code>Texture::disposableUniform(color)</code>. This
        returns a const reference to a new instance of the tiny <em>Uniform</em>
        color texture. These need to persist as long as the main
        texture,&nbsp; There are various ways to do this, I chose to use
        the “leak” form of memory management. (A handful of these tiny
        objects might get allocated during testing, they are not freed
        until the end of the run.) All code related to this idea is
        marked with a comment containing the string <code>BACKWARD_COMPATIBILITY</code>.
        These objects should only be created in the context of building
        and testing TexSyn.</p>
      <p class="designnote">(Design note: The <code>disposableUniform()</code>
        function also prints a warning when it is called. Perhaps that
        can be made a fatal error in “production mode” if such a thing
        were to exist in the future.)<br>
        &nbsp;</p>
    </div>
    <div class="post" id="20200514"> <a href="#20200514" class="date">May








































































































































































































































































































































































































































































































        14, 2020</a>
      <h1>Inherent matting</h1>
      <p>Since <a href="#20191219">December 19, 2019</a> this note has
        been on my TexSyn to-do list:</p>
      <p style="margin-left: 40px;">While writing <a
          href="https://cwreynolds.github.io/TexSyn/#20191219">Designing
          for genetic programming</a>, I wondered: instead of giving <em>Generators</em>
        two <em>Colors</em>, what if we gave them two <em>Texture</em>s?








































































































































































































































































































































































































































































































        This effectively turns each <em>Generator</em> into a <em>SoftMatte</em>.
        This might be more “GP friendly” overall. This would mean all
        current <em>Generators</em> (except for <em>Uniform</em>?)
        would become <em>Operators</em>.</p>
      <p>I am not quite ready to make that change, but I am creeping up
        on it, in the context of <em>LotsOfSpots</em> and friends. To
        unpack that note: <em>Texture</em> is TexSyn's base class for
        representing its model of textures. On top of <em>Texture</em>
        are two “category classes” <em>Generator</em> and <em>Operator</em>.
        These (as in the 2010-ish version of this library) serve no
        purpose other than to be the base class for two disjoint sets of
        concrete texture types. So for example, <em>Spot</em>'s base
        class is <em>Generator</em>, as are the other <em>Texture</em>
        types whose parameters do <strong>not</strong> include <em>Texture</em>s.








































































































































































































































































































































































































































































































        Conversely, <em>Operator</em> is the base class for (e.g.) <em>SoftMatte</em>
        or <em>Add</em>, those that have one or more <em>Texture</em>s
        among their parameters.</p>
      <p>In the old library, <em>Generator</em>s like <em>Spot</em>
        produced a monochrome signal to be manipulated by other
        operators. It would be as if LotsOfSpots always used white and
        black as its color parameters:</p>
      <pre>los = LotsOfSpots(0.9, 0.1, 0.4, 0.08, 0.03, Color(1, 1, 1), Color(0, 0, 0))</pre>
      <img src="images/20200514_LotsOfSpots_b_and_w.png" alt="los =
        LotsOfSpots(0.9, 0.1, 0.4, 0.08, 0.03, w, b)" title="los =
        LotsOfSpots(0.9, 0.1, 0.4, 0.08, 0.03, w, b)" height="511"
        width="511">
      <p>You (or GP evolution) could take that result then tint it, and
        add it to some other texture, etc. Or you could use <em>SoftMatte</em>
        to get the effect of current <em>LotsOfSpots</em>, where the
        basic texture is used to modulate between two colors, here
        represented as <em>Uniform</em> textures:</p>
      <pre>SoftMatte(los, Uniform(Color(0.1, 0.1, 0.1)), Uniform(Color(0.1, 0.6, 0.1)))</pre>
      <img src="images/20200514_SoftMatte_of_Uniforms.png"
        alt="SoftMatte(los, gray, green)" title="SoftMatte(los, gray,
        green)" height="511" width="511">
      <p>Of course, this more general case allows using the <code>los</code>
        pattern to combine arbitrary textures, not just <em>Uniform</em>s:</p>
      <pre>black_red  = Grating(Vec2(0.1, 0), Color(0, 0, 0), Vec2(0, 0), Color(1, 0, 0), 0.1, 0.5)
white_cyan = Grating(Vec2(0, 0.2), Color(1, 1, 1), Vec2(0, 0), Color(0, 1, 1), 0.1, 0.5)<br>SoftMatte(los, black_red, white_cyan)</pre>
      <img src="images/20200514_striped_spots_new.png" alt="" title=""
        height="511" width="511">
      <p>The textures above are simple to create with the current
        version of <em>LotsOfSpots</em>. It is easy because it is a
        pure <em>Generator</em> with no <em>Texture</em> parameters.
        The same is not true of its sibling classes <em>ColoredSpots</em>
        and <em>LotsOfButtons</em>. All three have a <code>background_color</code>
        parameter, with no convenient way to insert a background texture
        “behind” the spots. Say we wanted to define a texture somewhere
        between the texture below and the <em>LotsOfButtons</em>
        examples on <a href="#20200406c">April 6</a>. This texture
        would have cyan and white stripes in the background, with a red
        spiral “button” on each spot. This is not impossible but a bit
        awkward. You would need a <code>los</code> matte as in the
        first texture above, then you would need a <em>LotsOfButtons</em>
        element (“<code>lob</code>”) as on April 6, but carefully
        constructed to exactly match the matte, then you could SoftMatte
        <code>lob</code> over the cyan-white grating, using <code>los</code>
        as the matte.</p>
      <p>I have convinced myself this requires too much troubling code
        nesting “context” (in the sense described on <a
          href="#20191219">December 19, 2019</a> and in my to-do note
        quoted at the top of this post). So I decided to change <em>LotsOfSpots</em>,
        <em>ColoredSpots</em>, and <em>LotsOfButtons</em>. to take a <code>background_<strong>texture</strong></code>
        parameter instead of a <code>background_<strong>color</strong></code>
        parameter. For the same reason, I think <em>LotsOfSpots</em>'s
        <code>spot_<strong>color</strong></code> parameter should become
        <code>spot_<strong>texture</strong></code>. This implements <strong>inherent








































































































































































































































































































































































































































































































          matting</strong> for the three “spots” operators. I strongly
        suspect I will go on to make a corresponding change to the other
        <em>Generators</em>, but that wait for later.</p>
      <p><strong>So after this change:</strong> here is <em>LotsOfSpots</em>
        with the same “spot geometry” parameters, but its last two
        parameters are now textures rather than colors. Instead of
        making a monochrome matte and using it with <em>SoftMatte</em>,
        we can now directly combine the two input textures (in this case
        two <em>Grating</em>s) according to the collection of spots. I
        reversed the roles of the two gratings just so this image is not
        identical to the previous one. Before this change, the last two
        parameters of <em>LotsOfSpots</em> were <code>spot_color</code>
        and <code>background_color</code>, now they are <code>spot_texture</code>
        and <code>background_texture</code>.</p>
      <pre>LotsOfSpots(0.9, 0.1, 0.4, 0.08, 0.03, white_cyan, black_red)</pre>
      <img src="images/20200514_striped_spots.png" alt="LotsOfSpots with
        spot_texture and background_texture" title="LotsOfSpots with
        spot_texture and background_texture" height="511" width="511">
      <p>Similarly, after this change, the last parameter to <em>LotsOfButtons</em>
        is a background texture instead of a background color. Here is
        the <code>twist</code> texture (as described on <a
          href="#20200406c">April 6</a>) used as the <code>button_texture</code>
        and <code>white_cyan</code> used as the <code>background_texture</code>.
        Note that the spots containing black and red spirals are in the
        same distribution as the ones containing black and red stripes
        above:</p>
      <pre>LotsOfButtons(0.9, 0.1, 0.4, 0.08, 0.03, Vec2(), twist, 1, white_cyan)</pre>
      <img src="images/20200514_LotsOfButtons.png" alt="LotsOfButtons
        with background_texture" title="LotsOfButtons with
        background_texture" height="511" width="511">
      <p>Finally the new <em>ColoredSpots</em> with its new <code>background_texture</code>
        parameter. The spot colors come from a yellow-green <em>Noise</em>
        texture. They are “inherently matted” over a blue-gray <em>Noise</em>
        background texture:</p>
      <pre>ColoredSpots(0.6, 0.05, 0.1, 0.03, 0.01,
             Noise(0.30, Vec2(-3,2), Color(1, 1, 0), Color(0, 1, 0)),
             Noise(0.05, Vec2(3, 7), Color(0, 0, 1), Color(0.4, 0.4, 0.7)))</pre>
      <img src="images/20200514_ColoredSpots.png" alt="ColoredSpots with
        background_texture" title="ColoredSpots with background_texture"
        height="511" width="511"> </div>
    <div class="post" id="20200513"> <a href="#20200513" class="date">May








































































































































































































































































































































































































































































































        13, 2020</a>
      <h1>Margins between spots</h1>
      <p>While prototyping <em>LotsOfSpots</em> (and its
        base-class-sharing friends <em>ColoredSpots</em> and <em>LotsOfButtons</em>)
        I originally intended to add a parameter for how much empty
        “margin” to leave between spots. At the time, I forgot to
        provide a parameter for this. So now when I finally get back to
        adding that functionality, I have a “legacy API” issue. Again,
        not a huge problem for software that currently has a user
        community of exactly one. But I have tried to provide backward
        compatibility, and generally want the older examples (below, in
        this document) to produce the same result. So in addition to
        adding the <code>margin</code> parameter and functionality, I
        also provided stubs for the old versions without <code>margin</code>,
        so those should continue to work.</p>
      <p>The “margin” is used to establish an invisible boundary around
        each spot. Its “radius for placement” is its “radius for
        display” plus <code>margin</code>. The three texture operators
        (<em>LotsOfSpots</em>, <em>ColoredSpots</em>, and <em>LotsOfButtons</em>)
        had four parameters in common used by the shared base class. Now
        there are five. The new margin parameter is now the <strong>fifth</strong>
        parameter to each.</p>
      <p>Five sample textures are shown below. The first three being
        identical calls to <em>LotsOfSpots</em> except for the new <code>margin</code>
        parameter which has values of: <strong>0</strong>, <strong>0.02</strong>,
        and <strong>0.04</strong>. The last two textures are calls to <em>ColoredSpots</em>
        and <em>LotsOfButtons</em> with the same first five basic spot
        parameters as the third <em>LotsOfSpots</em> texture. As a
        result, the last three textures have identical arrangements of
        spots, with different renderings.</p>
      <pre>yellow = Color c1(0.7, 0.7, 0)
red = Color(0.9, 0, 0)

<span class="comment">// LotsOfSpots with margin=0 (fifth parameter).</span>
LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0, yellow, red)</pre>
      <img src="images/20200513_LOS_margin_0.png" alt="margin=0
        LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0, yellow, red)"
        title="margin=0 LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0, yellow,
        red)" height="511" width="511">
      <pre><span class="comment">// LotsOfSpots with margin=0.02 (fifth parameter).</span>
LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0.02, yellow, red)</pre>
      <img src="images/20200513_LOS_margin_0_02.png" alt="margin=0.02
        LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0.02, yellow, red)"
        title="margin=0.02 LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0.02,
        yellow, red)" height="511" width="511">
      <pre><span class="comment">// LotsOfSpots with margin=0.04 (fifth parameter).</span>
LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0.04, yellow, red)</pre>
      <img src="images/20200513_LOS_margin_0_04.png" alt="margin=0.04
        LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0.04, yellow, red)"
        title="margin=0.04 LotsOfSpots(0.8, 0.1, 0.3, 0.01, 0.04,
        yellow, red)" height="511" width="511">
      <pre><span class="comment">// ColoredSpots with margin=0.04, same first five parameters, so same spot placement.</span>
noise = Noise(0.03, Vec2(), yellow, red)
ColoredSpots(0.8, 0.1, 0.3, 0.01, 0.04, noise, red)</pre>
      <img src="images/20200513_CS_margin_0_04.png"
        alt="ColoredSpots(0.8, 0.1, 0.3, 0.01, 0.04, noise, red)"
        title="ColoredSpots(0.8, 0.1, 0.3, 0.01, 0.04, noise, red)"
        height="511" width="511">
      <pre><span class="comment">// </span><span class="comment">LotsOfButtons with margin=0.04, same first five parameters, so same spot placement.</span>
LotsOfButtons(0.8, 0.1, 0.3, 0.01, 0.04, Vec2(), noise, 1, red)</pre>
      <img src="images/20200513_LOB_margin_0_04.png"
        alt="LotsOfButtons(0.8, 0.1, 0.3, 0.01, 0.04, Vec2(), noise, 1,
        red)" title="LotsOfButtons(0.8, 0.1, 0.3, 0.01, 0.04, Vec2(),
        noise, 1, red)" height="511" width="511"> </div>
    <div class="post" id="20200510b"> <a href="#20200510b" class="date">May








































































































































































































































































































































































































































































































        10, 2020</a>
      <h1><em>Gradient</em> with “duty cycle” — asymmetrical stripes</h1>
      <p>This adds a parameter to <em>Grating</em> to control the
        relative width of the two “sub-stripes” of a <em>Grating</em>'s
        waveform. In signal analysis this is sometimes called <a
          href="https://en.wikipedia.org/wiki/Duty_cycle">duty cycle</a>.
        (This term is borrowed from hardware design, where it means the
        difference between say a motor meant to run continuously versus
        one that runs for short periods of time.) Below are three pairs,
        for three values of <code>duty_cycle</code>: 0.2, 0.5, and 0.8.
        A <code>duty_cycle</code> of 0.5 is the symmetrical version—as
        <em>Grating</em> was previously defined—the sub-stripes (here
        green and black) are approximately the same width.</p>
      <p><code>duty_cycle</code> is 0.2 — <code>softness</code> of 1.0
        and 0.2:</p>
      <img src="images/20200510_new_soft_0_2.png" alt="Grating
        duty_cycle=0.2, softness=1" title="Grating duty_cycle=0.2,
        softness=1" height="511" width="511"> <img
        src="images/20200510_new_hard_0_2.png" alt="Grating
        duty_cycle=0.2, softness=0.2" title="Grating duty_cycle=0.2,
        softness=0.2" height="511" width="511">
      <p><code>duty_cycle</code> is 0.5 — <code>softness</code> of 1.0
        and 0.2:</p>
      <img src="images/20200510_new_soft_0_5.png" alt="Grating
        duty_cycle=0.5, softness=1" title="Grating duty_cycle=0.5,
        softness=1" height="511" width="511"> <img
        src="images/20200510_new_hard_0_5.png" alt="Grating
        duty_cycle=0.5, softness=0.2" title="Grating duty_cycle=0.5,
        softness=0.2" height="511" width="511">
      <p><code>duty_cycle</code> is 0.8 — <code>softness</code> of 1.0
        and 0.2:</p>
      <img src="images/20200510_new_soft_0_8.png" alt="Grating
        duty_cycle=0.8, softness=1" title="Grating duty_cycle=0.8,
        softness=1" height="511" width="511"> <img
        src="images/20200510_new_hard_0_8.png" alt="Grating
        duty_cycle=0.8, softness=0.2" title="Grating duty_cycle=0.8,
        softness=0.2" height="511" width="511">
      <p>I implemented this by “remapping” the cross-stripe parameter
        before it goes into the waveform generator (<code>Grating::softSquareWave(Grating::dutyCycle(f))</code>).









































































































































































































































































































































































































































































































        My first attempt was based on a piecewise linear remapping
        composed of two ramps. That was OK but I worried about <a
          href="https://en.wikipedia.org/wiki/Mach_bands">Mach bands</a>
        where the ramps joined. So I looked into continuous versions of
        the remapping. These all “looked wrong.” Perhaps the issue was
        that they were not symmetrical across each sub-stripe and indeed
        the transitions were wider on one side of a sub-stripe than the
        other. I decided to live with any supposed Mach band artifacts
        and get on with my life. The textures above use the piecewise
        linear remapping.</p>
      <pre><span class="comment">// Grating with duty_cycle of 0.8 (so wide white, thin blue).</span>
grating = Grating(Vec2(), Color(1, 1, 1), Vec2(0.04, 0.04), Color(0, 0, 1), 0.4, 0.8);
<span class="comment">// A bit of smooth Perlin noise.</span>
noise = Noise n(0.3, Vec2(2, 7), Color(), Color(1, 1, 1));
<span class="comment">// Noise colorized with stripes to form a "coutour map".</span>
Colorize(Vec2(1, 0), Vec2(), grating,  noise))</pre>
      <img src="images/20200511_Grating.png" alt="Grating,
        duty_cycle=0.8 (wide white, thin blue)" title="Grating,
        duty_cycle=0.8 (wide white, thin blue)" height="511" width="511">
      <img src="images/20200511_Colorize.png" alt="Colorize Noise with
        Grating" title="Colorize Noise with Grating" height="511"
        width="511"> </div>
    <div class="post" id="20200510a"> <a href="#20200510a" class="date">May








































































































































































































































































































































































































































































































        10, 2020</a>
      <h1>Gamma adjust <em>Gradient</em> waveform</h1>
      <p>I mentioned this issue on <a href="#20200505">May 5</a>, <em>Gradient</em>
        stripes that had been symmetrical before the gamma changes are
        now asymmetrical. This added confusion during work on making
        stripes intentionally asymmetrical (see next post, above). My
        previous theory was this had to do with the colors involved. But
        I looked at black and white <em>Gratings</em> (or here, black
        and green) and felt the asymmetry was not acceptable. So I added
        a pre-de-gamma to the basic 1d waveform of a <em>Grating</em>.
        That waveform can range between a pure cosine wave or a square
        wave depending on the <em>Grating</em> parameter <code>softness</code>.
        Here are four textures: the previous and new versions, at
        softness values of 1.0 and 0.2. Note in the old soft version the
        green stripes appear much wider than the black stripes:</p>
      <img src="images/20200510_old_soft.png" alt="previous Grating,
        softness=1" title="previous Grating, softness=1" height="511"
        width="511"> <img src="images/20200510_old_hard.png"
        alt="previous Grating, softness=0.2" title="previous Grating,
        softness=0.2" height="511" width="511">
      <p>And this is the new gamma-adjusted version. The hard-edge <em>Grating</em>s
        are nearly the same. The transition regions in the soft-edged
        versions are different. In the new version, the green and black
        stripes are closer in width for both soft and hard edge
        versions:</p>
      <img src="images/20200510_new_soft_0_5.png" alt="new Grating,
        softness=1" title="new Grating, softness=1" height="511"
        width="511"> <img src="images/20200510_new_hard_0_5.png"
        alt="new Grating, softness=0.2" title="new Grating,
        softness=0.2" height="511" width="511"> </div>
    <div class="post" id="20200507"> <a href="#20200507" class="date">May








































































































































































































































































































































































































































































































        7, 2020</a>
      <h1>Moving forward with new gamma</h1>
      <p>I made another commit today which cleaned out all the gamma
        related experiments from the last week or so. There were several
        temporary mode switches which conditionally used one approach to
        gamma handling or another. Now there is just one path left with
        no conditionalization: internally TexSyn assumes linear RGB
        space as before. Gamma correction is done exactly once (with <em>γ</em>=1/2.2,








































































































































































































































































































































































































































































































        for sRGB) “on the way out” as image data is passed to OpenCV.
        Just as a reality check, without any special modes set, no
        tricks up my sleeves, this is a default render of <code>two_spots</code>,
        the test case inspired by <a
href="https://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/#colour-blending">Novak's









































































































































































































































































































































































































































































































          article</a>, now free of “dark rings”:</p>
      <pre>SoftMatte(Translate(Vec2(0.5, 0), spot),
          SoftMatte(Translate(Vec2(-0.5, 0), spot), green, red),
          magenta)</pre>
      <img src="images/20200507_two_spots.png" alt="two_spots with new
        gamma handling" title="two_spots with new gamma handling"
        height="511" width="511">
      <p>As noted yesterday, due to the change in gamma handling,
        running unmodified code from earlier examples (below, before
        yesterday) will now produce different results. However TexSyn
        will be used with a genetic programming optimizer, which can
        easily adapt to nonlinearities such as gamma correction. This
        comes down to changing a few of the numeric constant in a TexSyn
        expression. Here is one case in point. This rendered image is
        from <a href="#20200407">April 7</a> (let's call this version
        1):</p>
      <img src="images/20200408_LotsOfButtons_random_rotate.png"
        alt="LotsOfButtons version 1 (April 7)" title="LotsOfButtons
        version 1 (April 7)" height="511" width="511">
      <p>Running that same code <strong>today</strong> produces this
        result, brighter and less saturated (let's call this version 2):</p>
      <img src="images/20200507_LOB_original_code.png"
        alt="LotsOfButtons version 2" title="LotsOfButtons version 2"
        height="511" width="511">
      <p>On the other hand, with just a change of numeric constants in
        the original TexSyn expression (see code details below) it is
        possible <strong>today</strong> to closely reproduce the look
        of the earlier example (let's call this version 3):</p>
      <img src="images/20200507_LOB_tweaked_code.png" alt="LotsOfButtons
        version 3" title="LotsOfButtons version 3" height="511"
        width="511">
      <p>For details on the original texture, see <a href="#20200407">April








































































































































































































































































































































































































































































































          7</a>. Here is basically the same code, with some RGB values
        that had been written as inline constants, pulled out as <code>float</code>
        variables <code>m</code>, <code>n</code>, and <code>o</code>:</p>
      <pre><span class="comment">// For "version 1" and "version 2", parameters from April 7:</span>
m = 0.1
n = 0.3
o = 0.3

<span class="comment">// For "version 3" hand tweaked today until they looked close:</span>
m = 0.005
n = 0.25
o = 0.07

<span class="comment">// Definition of "sixths" used for each button:</span>
p1 = Vec2(-0.01, 0);
p2 = Vec2(+0.01, 0);
a3 = 2 * pi / 3
sixths = Translate(Vec2(0.5, 0.5),
                   Add(Gradation(p1.rotate(a3 * 2), Color(m, m, n),
                                 p2.rotate(a3 * 2), Color(n, n, n)),
                       Add(Gradation(p1.rotate(a3), Color(m, n, m),
                                     p2.rotate(a3), Color(n, n, n)),
                           Gradation(p1,            Color(n, m, m),
                                     p2,            Color(n, n, n)))))

<span class="comment">// Overall texture:</span>
LotsOfButtons(0.8, 0.04, 0.4, 0.02, Vec2(0.5, 0.5), sixths, 1, Color(o, o, o))</pre>
    </div>
    <div class="post" id="20200506"> <a href="#20200506" class="date">May








































































































































































































































































































































































































































































































        6, 2020</a>
      <h1><span style="background-color: red;"> Gamma correction changed
          incompatibly!</span></h1>
      <p>“The deed is done!” The incompatible change discussed on <a
          href="#20200504">May 4</a> and <a href="#20200505">May 5</a>
        has now been made in this <a
href="https://github.com/cwreynolds/TexSyn/commit/a0d8390b60e15bf923e015d29bd84f30d4e411ed">git









































































































































































































































































































































































































































































































          commit</a>. TexSyn operates internally as it did before <a
          href="#20200502">May 2</a>. It assumes all color values are in
        <strong>linear</strong> RGB space, as opposed to <strong>nonlinear</strong>
        (<em>γ</em>=2.2) sRGB space. So it does <strong>not</strong>
        “de-gamma” then “re-gamma” color values around operations like
        interpolation and low pass filtering, as described on <a
          href="#20200502">May 2</a>. It <strong>does,</strong>
        however, now apply a gamma correction (of <em>γ</em>=1/2.2, aka
        “re-gamma”) at the very end of the texture synthesis pipeline.
        Henceforth, Texsyn does its synthesis computation in linear RGB
        space, then at the end applies an sRGB display gamma before
        passing pixel color data on to OpenCV for output to display or
        file.</p>
      <p>This means at all texture samples below from before this
        change, if re-rendered, will now appear different. They will
        generally be brighter and less saturated. On the plus side,
        certain color rendering “bugs” will be fixed, and TexSyn will be
        on a more sound basis in <a
          href="https://en.wikipedia.org/wiki/Colorimetry">colorimetry</a>.
        As a software guy, it pains me to make a change that is not
        backward-compatible, even if this library has exactly one user.
        I will make notes about this in the <a href="#top">Introduction</a>
        at the top of this document and in the entry for <a
          href="#20191215">December 15, 2019</a> at the bottom.</p>
    </div>
    <div class="post" id="20200505"> <a href="#20200505" class="date">May








































































































































































































































































































































































































































































































        5, 2020</a>
      <h1>Live in linear RGB space, gamma correct at end</h1>
      <p>Say I did re-re-refactor to follow the strategy I discussed <a
          href="#20200504">yesterday</a>: assume TexSyn normally
        operates in a linear (<em>γ</em>=1) RGB color space, then do a
        single gamma correction (<em>γ</em>=1/2.2 for sRGB) at the end
        of the texture synthesis pipeline. Today's topic is: what does
        that look like? These are pairs of textures, as they would have
        looked before <a href="#20200502">May 2</a>, and how they would
        look under this new design. Let's start with a simple sinusoidal
        <em>Grating</em>, colored in red and green. First the “before <a
          href="#20200502">May 2</a>” version, then the proposed new
        “after” version: </p>
      <pre>red = Color(1, 0, 0)
green = Color(0, 1, 0)

Grating(Vec2(-0.2, 0), red, Vec2(0.2, 0), green, 1)</pre>
      <img src="images/20200505_lin_gam_a.png" alt="Grating(Vec2(-0.2,
        0), red, Vec2(0.2, 0), green, 1) before"
        title="Grating(Vec2(-0.2, 0), red, Vec2(0.2, 0), green, 1)
        before" height="511" width="511"> <img
        src="images/20200505_lin_gam_b.png" alt="Grating(Vec2(-0.2, 0),
        red, Vec2(0.2, 0), green, 1) after" title="Grating(Vec2(-0.2,
        0), red, Vec2(0.2, 0), green, 1) after" height="511" width="511">
      <p>Two things jump out: the are undesirable “dark transitions” in
        the <u>before</u> version. But in the <u>after</u> version,
        the green stripes seem much wider. Why is that? Recall that the
        only difference between them is gamma correction at the end. OK,
        let's try reversing the role of red and green:</p>
      <pre>Grating(Vec2(-0.2, 0), green, Vec2(0.2, 0), red, 1)</pre>
      <img src="images/20200505_lin_gam_c.png" alt="Grating(Vec2(-0.2,
        0), green, Vec2(0.2, 0), red, 1) before"
        title="Grating(Vec2(-0.2, 0), green, Vec2(0.2, 0), red, 1)
        before" height="511" width="511"> <img
        src="images/20200505_lin_gam_d.png" alt="Grating(Vec2(-0.2, 0),
        green, Vec2(0.2, 0), red, 1) after" title="Grating(Vec2(-0.2,
        0), green, Vec2(0.2, 0), red, 1) after" height="511" width="511">
      <p>So the green stripes still appear wider in the <u>after</u>
        version, regardless of whether they are assigned to the white or
        black part of the <em>Grating</em>. This suggests to me that it
        it unrelated to the way <em>Grating</em> works, or the affects
        of gamma. Perhaps is due to the fact that green is the
        “brightest” of the three primaries (red is in the middle and
        blue is the least bright). Perhaps the colors intermediate
        between red and green, which in the <u>before</u> texture
        appear too dark, are dominated by the brighter green color in
        the gamma corrected <u>after</u> texture. </p>
      <p>To test this hypothesis, I picked two different saturated
        colors that are very close in brightness. As measured by <code>Color::luminance()</code>,
        <code>cyan</code> is 0.7874 and <code>orange</code> is 0.7848:</p>
      <pre>cyan = Color(0, 1, 1)
orange = Color(1, 0.8, 0)
Grating(Vec2(0, -0.2), cyan, Vec2(0, 0.2), orange, 1)</pre>
      <img src="images/20200505_lin_gam_e.png" alt="Grating(Vec2(0,
        -0.2), cyan, Vec2(0, 0.2), orange, 1) before"
        title="Grating(Vec2(0, -0.2), cyan, Vec2(0, 0.2), orange, 1)
        before" height="511" width="511"> <img
        src="images/20200505_lin_gam_f.png" alt="Grating(Vec2(0, -0.2),
        cyan, Vec2(0, 0.2), orange, 1) after" title="Grating(Vec2(0,
        -0.2), cyan, Vec2(0, 0.2), orange, 1) after" height="511"
        width="511">
      <p>In this case, the width of the stripes in the after texture
        seem approximately the same. (Note in the <u>before</u>
        texture, the intermediate colors between stripes appear a bit
        green.) Here is a sample of Brownian noise colored with red and
        green:</p>
      <pre>Brownian(0.2, Vec2(), red, green)</pre>
      <img src="images/20200505_lin_gam_g.png" alt="" title=""
        height="511" width="511"> <img
        src="images/20200505_lin_gam_h.png" alt="" title="" height="511"
        width="511">
      <p>The two <em>Brownian</em> textures get to the gist of the
        subjective “which is more correct” question. In the absolute, I
        would be hard pressed to say one or the other of these is more
        correct. The <u>before</u> version seems to have more contrast
        or “definition” but that is probably due to the dark transitions
        between red and green.The <u>after</u> version feels more
        “flat” probably due to the absence of those dark transitions. It
        seems to be dominated by green over red, analogous to the <em>Grating</em>s
        above. But I cannot convince myself that there is anything
        “wrong” with the <u>after</u> texture. The next example is a
        simple <em>ColorNoise</em> using a basic Perlin 2d noise for
        each primary color:</p>
      <pre>ColorNoise(0.15, Vec2(3, -5), 0)</pre>
      <img src="images/20200505_lin_gam_i.png" alt="ColorNoise(0.15,
        Vec2(3, -5), 0) before" title="ColorNoise(0.15, Vec2(3, -5), 0)
        before" height="511" width="511"> <img
        src="images/20200505_lin_gam_j.png" alt="ColorNoise(0.15,
        Vec2(3, -5), 0) after" title="ColorNoise(0.15, Vec2(3, -5), 0)
        after" height="511" width="511">
      <p>As in the previous case of <em>Brownian</em> noise, is there
        any basis to argue either of these is more correct? The <u>before</u>
        version is darker and more saturated. The <u>after</u> version
        is brighter and less saturated. Over the last few months I have
        come to expect the <u>before</u> version. But the very first
        time I saw it, I am sure my reaction was “yeah, I guess that is
        what it should look like” and would probably have had the same
        reaction to either of these textures. That is all to say, I do
        not object to the <u>after</u> texture.</p>
      <p>Finally, here is a <em>SoftMatte</em> of the vertical
        red-green <em>Grating</em> and the horizontal cyan-orange <em>Grating</em>
        seen above. The matte texture (first parameter) is a soft edged
        <em>Spot</em>, white on black, centered at the origin:</p>
      <pre>white = Color(1, 1, 1)<br>black = Color(0, 0, 0)

SoftMatte(Spot(Vec2(), 0.2, white, 1, black),
          Grating(Vec2(-0.2, 0), red, Vec2(0.2, 0), green, 1),
          Grating(Vec2(0, -0.2), cyan, Vec2(0, 0.2), orange, 1))</pre>
      <img src="images/20200505_lin_gam_k.png" alt="SoftMatte before"
        title="SoftMatte before" height="511" width="511"> <img
        src="images/20200505_lin_gam_l.png" alt="SoftMatte after"
        title="SoftMatte after" height="511" width="511">
      <p>In this case, I am quite confident that the <u>before</u>
        texture is significantly “worse” than the <u>after</u> texture.
        In the <u>before</u> texture, incorrect darkening in areas of
        transition serve to strongly break up what ought to be smooth
        stripes — the red stripes fade in and out confusingly where they
        cross the cyan stripes — the green stripes seem to occlude the
        orange stripes where they should be blending. In contrast the <u>after</u>
        texture looks “smooth”, “natural”, and “sensible” to me.</p>
    </div>
    <div class="post" id="20200504"> <a href="#20200504" class="date">May








































































































































































































































































































































































































































































































        4, 2020</a>
      <h1>Gamma angst: shouldn’t TexSyn use linear RGB?</h1>
      <br>
      <img src="images/20200501_two_spot_before.png" alt="Red and
        magenta Spots, SoftMatte-d on green, with gamma-related dark
        rings." title="Red and magenta Spots, SoftMatte-d on green, with
        gamma-related dark rings." height="511" width="511">
      <p>As mentioned on <a href="#20200502">May 2</a>, the texture
        above (based on <a
href="https://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/">Novak's









































































































































































































































































































































































































































































































          post</a>) sent me down a rabbit hole of gamma-related issues.
        The dark rings around the <em>Spot</em>s are a symptom of
        incorrect gamma handling. And in fact, the recent <em>Gamma</em>
        texture operator could be used to prototype a fix using TexSyn.
        The texture <code>Gamma(1/2.2, two_spots)</code> does not have
        the dark rings, and the transitions from <em>Spot</em> color to
        green background look smooth and natural.</p>
      <p>My first thought was to just go with that. Assume everything in
        TexSyn operates on colors in <strong>linear RGB space</strong>.
        Then add just a tiny bit of code to apply a gamma correction for
        display—using <em>γ</em>=1/2.2 which approximates sRGB—at the
        very end of the rendering pipeline as 8-bit RGB data is passed
        to OpenCV. That seemed ideal, one small change then everything
        is gamma correct. However, as shown in the top texture on <a
          href="#20200426">April 26</a>, this makes a significant change
        to the appearance of a texture. It generally becomes brighter
        and less saturated (more pastel) as the midrange values of RGB
        components are pushed up. This would make all previous images in
        this document become obsolete. I was reluctant to do that. It
        would require either rerendering all existing images (about 180
        today), or having having code-image incompatibly before or after
        a certain date.</p>
      <p>So I decided make several local “fixes” — especially to the
        overload of <code>interpolation()</code> for <code>Color</code>
        and the low pass filter in <em>Blur</em>. This was more code,
        but less disruption to TexSyn backward compatibility. Most
        textures would look nearly the same after this change. The
        effect to assume that TexSyn operates in RGB space with a gamma
        of 2.2 (or 1/2.2, hard to know which is which).</p>
      <p>But now I am having “designer's remorse.” It sure makes a <strong>lot</strong>
        more sense for TexSyn to operate in linear space. It is not just
        <em>SoftMatte</em> and <em>Blur</em> that ought to have special
        casing in the current situation. Any texture operation that
        modifies or combines color values—as opposed to just moving them
        geometrically—naturally assume it works in a linear color space.
        Examples include: <em>Add</em>, <em>Subtract</em>, <em>Multiply</em>,
        <em>AdjustBrightness</em>... Similarly texture generators
        implicitly assume they create signals in a linear space. For
        example <em>Gradient</em> generates a cosine wave. If we assume
        this is at gamma 2.2, then when we gamma correct into linear
        space, the former cosine wave now has asymmetrical peaks and
        valleys (<a
href="https://www.wolframalpha.com/input/?i=plot+%28%28cos+%28x%29+%2B+1%29+%2F+2%29%5E2.2">source</a>):</p>
      <img src="images/20200504_sin_gamma.png" alt="" title=""
        height="150" width="221">
      <p>Some of the traditional reasons to work with images in
        nonlinear “gamma space” it that images from real cameras in the
        real world came that way. But in TexSyn, all images are
        synthetic and non-photographic. There is usually also concern
        about memory for storing images, issues like avoiding floating
        point representation, and packing the most information into
        small fixed point (often 8 bit) values. But in TexSyn, color is
        always represented in floating point, and images are generated
        procedurally, not stored. Or at least not stored until the end
        of the synthesis process when they are written into a frame
        buffer (graphics card) or stored in an external image file.</p>
      <p>Finally, I have tried to support the idea that TexSyn colors
        have unrestricted <code>float</code> RGB values. So for
        example, large negative component values were allowed. Colors
        are not clipped until the end of the pipeline, when exporting to
        OpenCV for display or file output. So I would prefer to avoid
        gamma correcting (exponentiating) intermediate results—say in <code>interpolate()</code>—which









































































































































































































































































































































































































































































































        effectively clips to positive values. Better to do that at the
        end of the pipeline also, like the existing clip to positive
        unit RGB color cube.</p>
    </div>
    <div class="post" id="20200502"> <a href="#20200502" class="date">May








































































































































































































































































































































































































































































































        2, 2020</a>
      <h1>Interpolate and blur colors in RGB space with linear gamma.</h1>
      <p><strong>Or: “I finally understand that bug from 1982.”</strong></p>
      <p>While working on the <em>Gamma</em> texture operator (<a
          href="#20200426">April 26</a>) I was reading background
        material on the topic of gamma correction. It comes up a lot in
        computer graphics and digital image processing. In the back of
        my mind I knew gamma was important for correct and efficient
        image representation, and as a tool for adjusting the
        tone/contrast of an image. But it is also key to getting image
        blending and anti-aliasing to work correctly. I was looking at
        John Novak's 2016 <a
href="https://blog.johnnovak.net/2016/09/21/what-every-coder-should-know-about-gamma/">What









































































































































































































































































































































































































































































































          Every Coder Should Know About Gamma</a> and was struck by the
        section on color blending, particularly Figure 8:</p>
      <img src="images/20200502_Novak_bad.png" alt="Figure 8b from John
        Novak's 2016 “What Every Coder Should Know About Gamma”"
        title="Figure 8b from John Novak's 2016 “What Every Coder Should
        Know About Gamma”" class="novak_pad" height="227" width="300"> <img
        src="images/20200502_Novak_good.png" alt="Figure 8b from John
        Novak's 2016 “What Every Coder Should Know About Gamma”"
        title="Figure 8a from John Novak's 2016 “What Every Coder Should
        Know About Gamma”" class="novak_pad" height="227" width="300">
      <p>I recreated a bit of it in the first texture below. There are
        red and magenta spots composited with <em>SoftMatte</em> onto a
        green background<em></em>. The two spots have an obviously
        incorrect dark ring around them. I had been seeing—and willfully
        ignoring—this artifact in TexSyn. But seeing this test, my mind
        went back to my early days at SGD, the graphics “division” of
        Symbolics Inc. Around 1982 my first big project was an
        interactive “paint” system for graphics and illustration. I
        noticed these annoying dark boundaries when certain color pairs
        were blended together. I had several theories for what caused
        the problem but none of them was <strong>gamma</strong>.
        Novak's Figure 8 grabbed my attention because it clearly showed
        the problem, and showed how it should look when the problem is
        solved.</p>
      <p>The <em>Spot</em> texture generator uses a sinusoidal
        transition from the outside color (at 0) to the inside color (at
        1). That blend factor “alpha” is then used for a simple linear
        interpolation between the colors. But “linear interpolation”
        assumes a linear space. Color values as displayed on the screen
        assume a gamma of about 2.2. (Of course it is more complicated,
        the color space definition is: <a
href="https://en.wikipedia.org/wiki/Gamma_correction#Microsoft_Windows,_Mac,_sRGB_and_TV/video_standard_gammas">sRGB</a>.)









































































































































































































































































































































































































































































































        In order to perform linear interpolation of color values
        requires the RGB components be “de-gamma-ed” into linear space
        before interpolation, and “re-gamma-ed” afterward. TexSyn's <code>interpolate()</code>
        function had been a simple C++ template compatible with all
        types. Now there is a special case for interpolation of <code>Color</code>
        values. Here is the original failing case and the improved
        version which performs color interpolation in linear (gamma=1)
        space.</p>
      <img src="images/20200501_two_spot_before.png" alt="" title=""
        height="511" width="511"> <img
        src="images/20200501_two_spot_after.png" alt="" title=""
        height="511" width="511">
      <pre><span class="comment">// Texture above made from three Uniform textures of pure red, green and magenta:</span>
red = Uniform(Color(1, 0, 0))
green = Uniform(Color(0, 1, 0))
magenta = Uniform(Color(1, 0, 1))<br><span class="comment">// A Spot, at origin, white within inner radius 0.1, blends to black at outer radius 0.5:</span><br>spot = Spot(Vec2(), 0.1, Color(1, 1, 1), 0.5, Color(0, 0, 0));<br><span class="comment">// The spot, shifted to left and right, to matte in red and magenta onto green:</span><br>two_spots = SoftMatte(Translate(Vec2(0.5, 0), spot),<br>                     &nbsp;SoftMatte(Translate(Vec2(-0.5, 0), spot), green, red),
                      magenta)</pre>
      <p>This is the second texture above with inner and outer radii for
        the two <em> Spots</em> drawn in to help visualize where the
        color gradients fall:</p>
      <img src="images/20200501_two_spot_after_marked.png" alt=""
        title="" height="511" width="511">
      <p>The <em>SoftMatte</em> operator defines a texture that (for
        any given location) uses the luminance of it first texture
        parameter (called “matte”, here one of the two <em>Spot</em>s)
        to control the blend of the other two input textures. It now
        performs a “de-gamma” on the “matte” texture so that all three
        values involved in the interpolation are in linear gamma RGB
        space.</p>
      <p>Taken together, these changes (to <code>interpolate()</code>
        for <code>Color</code> and the <em>SoftMatte</em> texture
        operator) handle most of the blending gamma errors in the
        current library. Most operators rely on the same <code>interpolate()</code>
        function. For example, here is a <em>LotsOfSpots</em> before
        and after this change, for the problematic red and green color
        combination:</p>
      <pre>LotsOfSpots(0.9, 0.02, 0.5, 0.1, Color(1, 0, 0), Color(0, 1, 0))</pre>
      <img src="images/20200501_LotsOfSpots_before.png" alt="" title=""
        height="511" width="511"> <img
        src="images/20200501_LotsOfSpots_after.png" alt="" title=""
        height="511" width="511">
      <p>Another place where color computations require correct handling
        of gamma is in the <em>Blur</em> low pass filter. (The <em>EdgeDetect</em>
        and <em>EdgeEnhance</em> operators (see <a href="#20200301">March








































































































































































































































































































































































































































































































          1</a>) are based on <em>Blur</em> and so inherit its
        functionality.) Shown below is a test pattern <code>grating</code>,
        and two applications of <em>Blur</em>, before and after this
        gamma change. As in the case of <em>SoftMatte</em> and <code>interpolate()</code>,
        the change is basically to “de-gamma” the color values before
        the convolution calculation, then to “re-gamma” the resulting
        filtered value. (The Novak page cited above also links to this
        page, by the late Elle Stone, which has good example images of
        blending and blurring: <a
href="https://ninedegreesbelow.com/photography/linear-gamma-blur-normal-blend.html">Linear









































































































































































































































































































































































































































































































          Gamma vs Higher Gamma RGB Color Spaces: Gaussian Blur and
          Normal Blend Mode</a>.)</p>
      <pre><span class="comment">// Define "grating". apply Blur with kernel width of 0.1</span>
grating = Grating(Vec2(), Color(1, 1, 0), Vec2(0.2, 0), Color(0, 0, 1), 0.01);
Blur(0.1, grating)
</pre>
      <img src="images/20200501_Grating.png" alt="" title=""
        height="511" width="511"> <br>
      <img src="images/20200501_Blur_Grating_before.png" alt="" title=""
        height="511" width="511"> <br>
      <img src="images/20200501_Blur_Grating_after.png" alt="" title=""
        height="511" width="511">
      <p>In the examples above, <code>grating</code> has 10 pairs of
        yellow-blue stripes, so 20 individual stripes across its
        rendered diameter of 2 units. Each (say) yellow stripe has a
        width of 0.1. We then apply <em>Blur</em> to it with a kernel
        width of 0.1. So the circular “support” of the filter kernel of
        a pixel centered on a yellow stripe will all be yellow. It
        follows that a pixel centered on a (say) yellow stripe should
        have a color identical to the corresponding place on the
        original <code>grating</code> texture. (Conversely a pixel on
        the boundary between yellow and blue will result in a blend of
        half yellow and half blue.) This comparison can be seen in the
        image below. A horizontal slice through the three textures above
        are juxtaposed so they can be more carefully compared. Following
        the center line of a yellow stripe, it appears to be the same
        color in all three, as intended. Similarly for blue. What
        differs is the luminance of the blurred transition between
        yellow and blue. With the old code (stripe 2 of 3 below) the
        transitions are too dark. (This recalls the dark rings around
        the red and magenta spots in the first example above.) With the
        new code (stripe 3 of 3 below) the transitions are brighter and
        seem to better represent a transition from yellow to blue. Note
        that interpolating between complementary color pairs (like
        yellow↔︎blue, magenta↔︎green, or cyan↔︎red) requires traversing
        a main diagonal of the RGB color cube between opposite corners.
        This causes the center of the interpolation path to pass through
        the monochrome axis, the fourth main cube diagonal between black
        and white. We see these grayish midrange colors here between
        yellow and blue stripes, and in the first example above, between
        a magenta spot and the green background.</p>
      <img src="images/20200502_compare%20blurs.png" alt="" title=""
        height="125" width="511"> </div>
    <div class="post" id="20200426"> <a href="#20200426" class="date">April








































































































































































































































































































































































































































































































        26, 2020</a>
      <h1><em>Gamma</em></h1>
      <p><em>Gamma</em> provides the <a
          href="https://en.wikipedia.org/wiki/Gamma_correction">standard
          operation used in digital image processing</a>: exponentiating
        the RGB components by a given parameter gamma (<em>γ</em>).
        Values other than 1 cause a nonlinear contrast mapping. When
        gamma is less than 1, mid-range colors will be brighter. When
        gamma is greater than 1, mid-range colors will be darker. Values
        near white (luma ~1) and near black (luma ~0) stay about the
        same.</p>
      <p>Note that <em>Gamma</em> and <em>AdjustBrightness</em> (see <a
          href="#20200304">March 4</a>) are similar. Both have one float
        parameter and change a texture's contrast. <em>AdjustBrightness</em>
        performs a linear scale of a texture's brightness, so changes
        its contrast. Gamma makes a nonlinear contrast change. Normally
        gamma correction is used on color values within the positive
        unit RGB cube. In TexSyn colors are three arbitrary floats. </p>
      <p>Shown below are five applications of <em>Gamma</em> to the
        same <em>ColorNoise</em> texture <code>cn</code>. The
        parameters are <strong>0.5</strong>, <strong>0.66</strong>, <strong>1</strong>,
        <strong>1.5</strong>, <strong>2</strong>. I tested that as
        intended, <code>Gamma(1, cn)</code> is identical to <code>cn</code>.</p>
      <pre>cn = ColorNoise(0.3, Vec2(5, 3), 0.6)<br>Gamma(0.5, cn)</pre>
      <img src="images/20200426_Gamma_0_50.png" alt="Gamma(0.5, cn)"
        title="Gamma(0.5, cn)" height="511" width="511"> <br>
      <pre>Gamma(0.66, cn)</pre>
      <img src="images/20200426_Gamma_0_66.png" alt="Gamma(0.66, cn)"
        title="Gamma(0.66, cn)" height="511" width="511"> <br>
      <pre>Gamma(1, cn)  <span class="comment">// verified by Texture::diff() to be identical to "cn" </span></pre>
      <img src="images/20200426_Gamma_1_00.png" alt="Gamma(1, cn)"
        title="Gamma(1, cn)" height="511" width="511"> <br>
      <pre>Gamma(1.5, cn)</pre>
      <img src="images/20200426_Gamma_1_50.png" alt="Gamma(1.5, cn)"
        title="Gamma(1.5, cn)" height="511" width="511"> <br>
      <pre>Gamma(2, cn)</pre>
      <img src="images/20200426_Gamma_2_00.png" alt="Gamma(2, cn)"
        title="Gamma(2, cn)" height="511" width="511"> <br>
    </div>
    <div class="post" id="20200421"> <a href="#20200421" class="date">April








































































































































































































































































































































































































































































































        21, 2020</a>
      <h1>Even speedier <em>LotsOfSpots</em></h1>
      <p>In the previous post, I clocked recent acceleration due to
        algorithmic and parallelism as producing a speed-up of 17x. I
        tried one more time today and got it to <strong>22 times faster</strong>
        than the April 7 version, as measured on the same test suite of
        six textures shown in the previous post.</p>
      <p>Part of this was due to what I think of as <a
          href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl's
          law</a>, or at least the underlying principle. On an ideally
        parallel workload, execution time is inversely proportional to
        the number of parallel processors applied to the task. But
        normally there is some overhead that cannot be made parallel.
        That places a ceiling on the speed-up. (E.G. if 5% of the task
        cannot be made parallel, it will never run more that 20 times
        faster regardless of the number of processors applied to the
        task.) In concrete terms, the function <code>LotsOfSpotsBase::adjustOverlappingSpots()</code>
        was originally an O(<em>n²</em>) algorithm. Then using the <code>DiskOccupancyGrid</code>
        spatial data structure reduced that to O(<em>n</em>). Then I
        parallelized the part for deciding if each disk overlapped
        another, and if so, computing the required move. Finally today I
        parallelized the other part: erasing each disk from the grid,
        moving it, and reinserting it into the grid. This required
        making <code>DiskOccupancyGrid</code> thread-safe.</p>
      <p>I also looked at the number of threads used. In rendering I use
        a thread per scanline (row of pixels), so the renderings on this
        page use about 500 threads. In my prototype “disk
        de-overlapping” code I had arbitrarily selected 40 threads. I
        tested other values. It kept getting faster as I reduced the
        number of threads, until it hit a minimum at 8 threads, then got
        slower as I used fewer threads. Coincidentally, there are 8
        hardware “hyperthreads” on the 4 cores of my laptop's CPU. I
        don't know if if this is purely coincidental, but in case not,
        my thread count is now set to the value returned by <code>std::thread::hardware_concurrency()</code>.
      </p>
    </div>
    <div class="post" id="20200419"> <a href="#20200419" class="date">April








































































































































































































































































































































































































































































































        19, 2020</a>
      <h1>Speed ups for <em>LotsOfSpots</em> and friends</h1>
      <p>As mentioned on <a href="#20200406">April 6</a> the compute
        speed for the base class for <em>LotsOfSpots</em>, etc. was
        much too slow. The algorithm to ensure the “spots” did not
        overlap was O(<em>n</em>²) with typical <em>n</em> being in the
        thousands (4264 spots in the first two example below). I made
        two main changes. First to use a spatial data structure to allow
        quickly finding “spots” near a given point in texture space (for
        rendering) and near another spot to determine if they overlap.
        The second change was to use multi-thread parallelism during the
        overlap removal. Averaged over multiple runs, using the test
        suite below (6 textures), the code now runs about <strong>17
          times faster</strong>. The images below show a sample <em>LotsOfSpots</em>,
        and then the same texture “zoomed out” (scaled down by 0.13) to
        show the entire tiling pattern. </p>
      <p>To help compare these pair of images: the tiles are 10x10
        texture space units, meaning they are 5 times wider than the
        circular renderings whose diameters are 2 units. The “close up”
        image corresponds to the very center region of the zoomed out
        image next to it. You can see the scale of the tiling pattern if
        you look closely at the zoomed out image. For example, look at
        the second image: on the right hand side just below center there
        is a “bare patch” with more magenta background and less cyan
        spots. If you look to the left, about two third of the way
        across, you can see the same bare patch repeated (see second
        image on <a href="#20200403">April 3</a> which highlights the
        central tile).</p>
      <pre><span class="comment">// ~70% of texture is covered by spots, radii range from 0.02 to 0.2</span>
LotsOfSpots(0.7, 0.02, 0.2, 0.01, c, m)
Scale(0.13, LotsOfSpots(0.7, 0.02, 0.2, 0.01, c, m))</pre>
      <img src="images/20200419_los1.png" alt="LotsOfSpots(0.7, 0.02,
        0.2, 0.01, c, m)" title="LotsOfSpots(0.7, 0.02, 0.2, 0.01, c,
        m)" height="511" width="511"> <img
        src="images/20200419_los1_scaled.png" alt="Scale(0.13,
        LotsOfSpots(0.7, 0.02, 0.2, 0.01, c, m))" title="Scale(0.13,
        LotsOfSpots(0.7, 0.02, 0.2, 0.01, c, m))" height="511"
        width="511">
      <pre><span class="comment">// ~80% of texture is covered by spots, radii range from 0.02 to 0.4</span>
LotsOfSpots(0.8, 0.02, 0.4, 0.02, c, m)
Scale(0.13, LotsOfSpots(0.8, 0.02, 0.4, 0.02, c, m))</pre>
      <img src="images/20200419_los2.png" alt="LotsOfSpots(0.8, 0.02,
        0.4, 0.02, c, m)" title="LotsOfSpots(0.8, 0.02, 0.4, 0.02, c,
        m)" height="511" width="511"> <img
        src="images/20200419_los2_scaled.png" alt="Scale(0.13,
        LotsOfSpots(0.8, 0.02, 0.4, 0.02, c, m))" title="Scale(0.13,
        LotsOfSpots(0.8, 0.02, 0.4, 0.02, c, m))" height="511"
        width="511">
      <pre><span class="comment">// ~80% of texture is covered by spots, all radii are 0.2</span>
LotsOfSpots(0.8, 0.2, 0.2, 0.02, c, m)
Scale(0.13, LotsOfSpots(0.8, 0.2, 0.2, 0.02, c, m))</pre>
      <img src="images/20200419_los3.png" alt="LotsOfSpots(0.8, 0.2,
        0.2, 0.02, c, m)" title="LotsOfSpots(0.8, 0.2, 0.2, 0.02, c, m)"
        height="511" width="511"> <img
        src="images/20200419_los3_scaled.png" alt="Scale(0.13,
        LotsOfSpots(0.8, 0.2, 0.2, 0.02, c, m))" title="Scale(0.13,
        LotsOfSpots(0.8, 0.2, 0.2, 0.02, c, m))" height="511"
        width="511"> </div>
    <div class="post" id="20200407"> <a href="#20200407" class="date">April








































































































































































































































































































































































































































































































        7, 2020</a>
      <h1>Even more <em>LotsOfButtons</em></h1>
      <p>I had not tested <em>LotsOfButtons</em> with a non-zero <code>button_center</code>
        parameter. Sure enough, it did not work correctly. I made a new
        test case to use while fixing it. Here is contrived <code>button_texture</code>
        called “<code>sixths</code>” whose edges cross at <code>Vec2(0.5,








































































































































































































































































































































































































































































































          0.5)</code>:</p>
      <img src="images/20200407_sixths.png" alt="sixths" title="sixths"
        height="511" width="511">
      <p>Here is a <em>LotsOfButtons</em> using that <code>sixths</code>
        as its <code>button_texture</code>, and a <code>button_center</code>
        of <code>Vec2(0.5, 0.5)</code>. Now, after the bug fix, the
        six-fold input pattern is correctly positioned inside each spot:</p>
      <pre>LotsOfButtons(0.8, 0.04, 0.4, 0.02, Vec2(0.5, 0.5), sixths, 0, Color(0.3, 0.3, 0.3))</pre>
      <img src="images/20200407_LotsOfButtons_offset.png"
        alt="LotsOfButtons offset" title="LotsOfButtons offset"
        height="511" width="511">
      <p>The spiral pattern in yesterday's tests largely masked the
        issue of orientation. This highly directional texture makes it
        much more obvious that all the “buttons” are in the same
        orientation. I'm sure there are occasions when that could be
        helpful. Yet it feels oddly regular and mechanical next to the
        randomized positions and sizes of the spots. So I added code to
        randomly rotate each “button” as shown below. I added (yet
        another!) parameter to select between random-rotate or not. Like
        the <code>which</code> parameter to <em>MultiNoise</em> (see <a
          href="#20200112">January 12</a>) it is given as a floating
        point value—which is interpreted as a Boolean value via
        thresholding: (<code>button_random_rotate &gt; 0.5</code>)—allowing









































































































































































































































































































































































































































































































        it to be changed by mutation in the course of a Genetic
        Programming run. The operational principle being: “let evolution
        decide”!</p>
      <pre>LotsOfButtons(0.8, 0.04, 0.4, 0.02, Vec2(0.5, 0.5), sixths, 1, Color(0.3, 0.3, 0.3))</pre>
      <img src="images/20200408_LotsOfButtons_random_rotate.png"
        alt="LotsOfButtons random rotate" title="LotsOfButtons random
        rotate" height="511" width="511">
      <pre><span class="comment">// Just for completeness this is the "sixths" button_texture used above:</span>
p1 = Vec2(-0.01, 0);
p2 = Vec2(+0.01, 0);
a3 = 2 * pi / 3;<br>sixths = Translate(Vec2(0.5, 0.5),
                   Add(Gradation(p1.rotate(a3 * 2), Color(0.1, 0.1, 0.3),
                                 p2.rotate(a3 * 2), Color(0.3, 0.3, 0.3)),
                       Add(Gradation(p1.rotate(a3), Color(0.1, 0.3, 0.1),
                                     p2.rotate(a3), Color(0.3, 0.3, 0.3)),
                           Gradation(p1,            Color(0.3, 0.1, 0.1),
                                     p2,            Color(0.3, 0.3, 0.3)))))</pre>
    </div>
    <div class="post" id="20200406c"> <a href="#20200406c" class="date">April









































































































































































































































































































































































































































































































        6, 2020</a>
      <h1><em>LotsOfButtons</em></h1>
      <p><em>LotsOfButtons</em> is another variation on <em>LotsOfSpots</em>
        where the spots are filled with a circular portion of another
        texture. (I am still considering the use of “button” to mean a
        little portion of one texture inserted into another. Possible
        alternatives: thumbnail, icon, crop, stamp (as in rubber-stamp,
        pretty archaic), cameo, cookie...) </p>
      <p>Shown below are two examples of parameters to <em>LotsOfButtons</em>,
        and the texture used to fill the individual spots. The
        parameters to <em>LotsOfButtons</em> is similar to <em>ColoredSpots</em>
        with the addition of a <em>Vec2</em> position indicating the
        center of the input texture used for filling the individual
        spots. I am undecided on whether it makes more sense for the
        input <code>button_texture</code> should be scaled according to
        the radius of a spot.</p>
      <p>The fourth parameter to the <em>LotsOf</em>... operators—the <code>soft_edge_width</code>—had









































































































































































































































































































































































































































































































        been handled incorrectly and is now fixed. I was constraining it
        to be half the <code>min_radius</code>. First of all that
        should not have been half (radius-diameter confusion) and
        secondly the constraint should have been on a per-spot basis.
        That is: a large <code>soft_edge_width</code> (indicating very
        fuzzy-edged spots) should be used as given for large spots, and
        constrained only for individual spots whose radius was less than
        <code>soft_edge_width</code>.</p>
      <pre><span class="comment">// Parameters are: density, min_r, max_r, soft, button_center, button_texture, bg_color</span>
LotsOfButtons(0.79, 0.1, 0.6, 0.05, Vec2(), twist, gray2)</pre>
      <img src="images/20200406_LotsOfButtons_1.png"
        alt="LotsOfButtons_1" title="LotsOfButtons_1" height="511"
        width="511">
      <pre>LotsOfButtons(0.6, 0.05, 0.25, 0.025, Vec2(), twist, gray2)</pre>
      <img src="images/20200406_LotsOfButtons_2.png"
        alt="LotsOfButtons_2" title="LotsOfButtons_2" height="511"
        width="511">
      <pre><span class="comment">// this is the input texture whose center is inserted into each spot above.</span>
red = Color(1, 0, 0);
gray2 = Color(0.2, 0.2, 0.2);
gray3 = Color(0.3, 0.3, 0.3);
twist = Twist(7, 9, Vec2(),
              SliceToRadial(Vec2(0, 0.318), Vec2(),
                            Grating(Vec2(0, -0.1), red,
                                    Vec2(0, +0.1), gray3, 0.3)))</pre>
      <img src="images/20200406_Twist.png" alt="Twist" title="Twist"
        height="511" width="511"> </div>
    <div class="post" id="20200406b"> <a href="#20200406b" class="date">April









































































































































































































































































































































































































































































































        6, 2020</a>
      <h1><em>ColoredSpots</em></h1>
      <p><em>ColoredSpots</em> is based on an experiment from <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20100209">February









































































































































































































































































































































































































































































































          9, 2010</a>. It uses the same geometric model as <em>LotsOfSpots</em>,
        then each spot's color is taken from a given <code>color_texture</code>.
        Shown below is a <em>ColoredSpots</em> texture and the <em>ColorNoise</em>
        from which its spot colors are taken. </p>
      <p>To do this I refactored <em>LotsOfSpots</em> into a base class
        <em>LotsOfSpotsBase</em>, then moved the two color version into
        a derived class (now called <em>LotsOfSpots</em>) which
        inherits from the base class. Then I added a second derived
        class <em>ColoredSpots</em> also inheriting from <em>LotsOfSpotsBase.</em></p>
      <pre>cn = AdjustSaturation(1.2, ColorNoise(2.5, Vec2(3, 4), 0.2))
Scale(0.2, ColoredSpots(0.5, 0.2, 0.3, 0.05, cn, gray50))
</pre>
      <img src="images/20200406_ColoredSpots.png" alt="" title=""
        height="511" width="511"> <img
        src="images/20200406_ColorNoise.png" alt="" title=""
        height="511" width="511"> </div>
    <div class="post" id="20200406"> <a href="#20200406" class="date">April








































































































































































































































































































































































































































































































        6, 2020</a>
      <h1>Preliminary post-prototype <em>LotsOfSpots</em></h1>
      <p>So far the <em>LotsOfSpots</em> prototype had been black and
        white for simplicity. I finally got around to adding color
        parameters for spots and background. I'm planning some other
        varients on the basic concepts. I also noticed that—without the
        <code>Scale(0.2, ...)</code> I'd been wrapping them in—spot
        parameters that seem “reasonable” lead to excessive
        initialization and rendering times. Worse than <em>Blur</em>
        before I sped it up. So I will need to improve the algorithmic
        performance of <em>LotsOfSpots</em>. For comparison, this
        texture, without the scale, has 4264 spots per 10x10 tile, which
        is a big <em>n</em> for O(<em>n</em>²) algorithms.</p>
      <pre>c = Color(0.0, 0.8, 1.0)
m = Color(0.8, 0.0, 0.8)
LotsOfSpots(0.7, 0.02, 0.2, 0.01, c, m)</pre>
      <img src="images/20200406_LotsOfSpots_with_color.png"
        alt="LotsOfSpots(0.7, 0.02, 0.2, 0.01, c, m)"
        title="LotsOfSpots(0.7, 0.02, 0.2, 0.01, c, m)" height="511"
        width="511"> </div>
    <div class="post" id="20200405"> <a href="#20200405" class="date">April








































































































































































































































































































































































































































































































        5, 2020</a>
      <h1>Choosing spots for <em>LotsOfSpots</em></h1>
      <p>I drilled down a bit into the generation of the “random” spots
        in the prototype <em>LotsOfSpots</em> constructor. Below are
        three samples of various settings of the parameters that control
        the placement of spots. Those parameters are: <code>spot_density</code>,
        <code>min_radius</code>, <code>max_radius</code>, and <code>soft_edge_width</code>.
        Some constraints are enforced for practicality: <code>density</code>
        is clipped to 1, <code>soft_edge_width</code> is clipped to
        half of <code>min_radius</code>. The random radii fall between
        the min and max parameters, the the selection is biased toward
        the smaller values. Each of these texture samples are shown
        scaled down by a factor of 0.2, which means the whole 10x10
        central tile is the bounding box of the circular texture. </p>
      <p>In this first sample, the density is 0.5 so the total area of
        white spots is about the same as the black background. The spot
        radii are in a narrow range [0.10, 0.15]. Each tile of the
        texture contains 1270 spots.</p>
      <pre>Scale(0.2, LotsOfSpots(0.5, 0.1, 0.15, 0.1))</pre>
      <img src="images/20200405_LotsOfSpots_d1.png" alt="Scale(0.2,
        LotsOfSpots(0.5, 0.1, 0.15, 0.1))" title="Scale(0.2,
        LotsOfSpots(0.5, 0.1, 0.15, 0.1))" height="511" width="511">
      <p>Below, the density is 70%, spot radii range from 0.1 to 1.0,
        producing a total of 170 spots.</p>
      <pre>Scale(0.2, LotsOfSpots(0.7, 0.1, 1.0, 0.1))</pre>
      <img src="images/20200405_LotsOfSpots_d2.png" alt="Scale(0.2,
        LotsOfSpots(0.7, 0.1, 1.0, 0.1))" title="Scale(0.2,
        LotsOfSpots(0.7, 0.1, 1.0, 0.1))" height="511" width="511">
      <p>Below, the density is 90% and the spot radii range from 0.1 to
        1.0, producing a total of 245 spots. First the code randomly
        distributes enough spots to meet the given density level. Then
        it tries to adjust the spot positions to avoid overlap. After
        200 failed adjustment attempts, it stops and gives up. That
        happened here, the spots were too crowded for it to find a
        non-intersecting arrangement in 200 tries. (They are “pretty
        well” distributed but some still overlap.) Letting it run for
        1000 tries only slightly improved the layout, so I decided the
        time taken was not justified.</p>
      <pre>Scale(0.2, LotsOfSpots(0.9, 0.1, 1.0, 0.1))</pre>
      <img src="images/20200405_LotsOfSpots_d3.png" alt="Scale(0.2,
        LotsOfSpots(0.9, 0.1, 1.0, 0.1))" title="Scale(0.2,
        LotsOfSpots(0.9, 0.1, 1.0, 0.1))" height="511" width="511"> </div>
    <div class="post" id="20200403"> <a href="#20200403" class="date">April








































































































































































































































































































































































































































































































        3, 2020</a>
      <h1>Experimental <em>LotsOfSpots</em></h1>
      <p>This is a new, updated version of <em>LotsOfSpots</em>,
        loosely based on the version from <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20100208"
          class="date">February 8, 2010</a>. As the original version
        aspired to be, this one is defined everywhere on the texture
        plane. It achieves this by the boring method of tiling a pattern
        of spots. The square tiling has a period of 10x10 and is
        centered at the origin. Since the typical <strong>TexSyn</strong>
        “rendering interval” is [-1, +1] in both X and Y, normally the
        cyclic nature of the spot pattern would normally be hidden “off
        screen” unless the texture was scaled down significantly or
        subjected to a nonlinear warp.</p>
      <p>The first texture sample shows a typical portion of <em>LotsOfSpots</em>.
        (This prototype is returning a grayscale image.) In this case
        the <code>density</code> of spots is ~70%, the radii are
        distributed between 0.1 and 0.9 (with smaller values preferred)
        and a soft transition zone width of 0.1. The second texture
        sample shows the result of zooming out—scaling by 0.13—to reveal
        the tile size. The non-center tiles are tinted blue.</p>
      <pre><span class="comment">// Parameters are: density, min_radius, max_radius, softness</span><br>LotsOfSpots(0.7, 0.1, 0.9, 0.1)</pre>
      <img src="images/20200403_LotsOfSpots.png" alt="" title=""
        height="511" width="511">
      <pre>Scale(0.13, LotsOfSpots(0.7, 0.1, 0.9, 0.1))</pre>
      <img src="images/20200403_LotsOfSpots_zoomed_out.png" alt=""
        title="" height="511" width="511">
      <p>One more random example, showing why it is important for
        textures to be defined across the entire texture plane. Here is
        the previous texture (including its blue tint outside the
        central tile) as seen through a <em>MobiusTransform</em>
        operator. This happens to be the same Möbius transformation that
        was the “third example” on the <a href="#20200128">January 28</a>
        example below. Because a texture operator can arbitrarily scale,
        stretch, or warp its input texture, it is impossible to
        anticipate what part of a texture will be sampled.</p>
      <pre>MobiusTransform(Vec2(-0.958788, 1.64993),
                Vec2(-1.54534, -0.593485),
                Vec2(1.29155, -0.931471),
                Vec2(0.768266, 0.24665),
                Translate(Vec2(-0.75, -0.75),
                          Scale(0.13,
                                LotsOfSpots(0.7, 0.1, 0.9, 0.1)))</pre>
      <img src="images/20200403_LotsOfSpots_Mobized.png" alt="" title=""
        height="511" width="511"> </div>
    <div class="post" id="20200401"> <a href="#20200401" class="date">April








































































































































































































































































































































































































































































































        1, 2020</a>
      <h1>Incremental Halton sequence</h1>
      <p>[<b>Update:</b> on June 26, 2021 I removed the code for this
        experiment, long unused, from TexSyn. For reference it is now
        available <a moz-do-not-send="true"
href="https://drive.google.com/file/d/1okb1OWmXEJOEE62t2weZnh6rvRw6N7Cy/view?usp=sharing">here</a>.]<br>
      </p>
      <p>As mentioned before, there was an earlier version of the <strong>TexSyn</strong>
        library, begun in 2008. It included some experimental operators
        (<em>LotsOfSpots</em> replaced by <em>SpotsInCircle</em>) for
        generating patterns of many spots, see <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20100208"
          class="date">February 8, 2010</a> and <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20100209"
          class="date">February 9, 2010</a>. It is easy to curate a
        finite collection of spots, hence the easier-to-implement <em>SpotsInCircle</em>
        operator. It would be much more convenient to have a
        distribution defined throughout the texture plane. This is the
        beauty of Perlin noise which is defined at every point in the
        space.</p>
      <p>I had a vague memory of a talk at SIGGRAPH by someone who
        worked on the remarkable game <a
          href="https://en.wikipedia.org/wiki/Spore_%282008_video_game%29">Spore</a>,
        describing the technique they used to distribute game objects,
        say trees in a forest. A search led me to the website of <a
          href="http://www.andrewwillmott.com/">Andrew Willmott</a> and
        the <a href="http://www.andrewwillmott.com/s2007">materials
          from the SIGGRAPH 2007 session</a> including “Fast Object
        Distribution” about his incremental Halton sequence. The
        materials include sample code called <code>IncrementalHalton.cpp</code>.
        This is the first 300 points generated by the sequence. Ignoring
        the Z coordinate, I drew a circle centered on the Halton point.
        The first 100 are large orange, then medium green, then small
        purple. </p>
      <p>I'm still learning about this. Generally the points seem “well
        distributed” over the unit square. Also informally, new points
        “tend to fall between” older points. As I understand it, the
        points are often “spaced out” but there is no lower bound on the
        space between any two points. So if you consider the points to
        be disks, there will occasionally be collisions (overlaps). For
        more background information, see this excellent “explorable
        explanation” by <a href="https://observablehq.com/@jrus">Jacob
          Rus</a> about the <a
          href="https://observablehq.com/@jrus/halton">Halton Sequence</a>.
      </p>
      <p>The incremental Halton sequence was developed to allow its use
        in an interactive game and its interactive editors for game
        worlds. While performance matters in <strong>TexSyn</strong>,
        that is not its main focus. I want to look at some other methods
        for constructing spot patterns, including those that might do
        some pre-calculation (Such as, for example, to adjust spacing
        between disks). </p>
      <img src="images/20200401_Halton.png" alt="incremental Halton
        sequence test" title="incremental Halton sequence test"
        height="511" width="511"> </div>
    <div class="post" id="20200330"> <a href="#20200330" class="date">March








































































































































































































































































































































































































































































































        30, 2020</a>
      <h1>RTFM and phew!</h1>
      <p>A <code>Texture</code> instance now has an initially empty <code>cv::Mat</code>
        to serve as a rasterization cache. Both <code>Texture::displayInWindow()</code>
        and <code>Texture::writeToFile()</code> are refactored to first
        call <code>Texture::rasterizeToImageCache()</code> to ensure a
        cached rendering is available before displaying it in a window
        or writing it to a file. The cache means that (for a given <code>size</code>,
        etc.) each Texture will be rendered at most one time.</p>
      <p>After doing that, I happened to be browsing through the <strong>OpenCV</strong>
        doc—as one does—and noticed <a
href="https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a952ef1a85d70a510240cb645a90efc0d"><code>cv::Mat::forEach()</code></a>
        which applies a given function to all image pixels using all
        available parallelism. That is nearly identical to what I did
        yesterday in lovingly hand crafted code. So: (a) remember to
        always RTFM, and (b) how does the speed of using this <code>forEach()</code>
        tool compare with the hand written code? </p>
      <p>It turns out that my code is slightly faster (about 11%) but it
        would have been significantly faster and easier to write the
        code using the <code>forEach()</code> tool provided by <strong>OpenCV</strong>.
        I am curious about why my code was faster, or said another way,
        if the <strong>OpenCV</strong> code could be made faster.</p>
    </div>
    <div class="post" id="20200329"> <a href="#20200329" class="date">March








































































































































































































































































































































































































































































































        29, 2020</a>
      <h1>Multi-threading for faster <em>Blur</em></h1>
      <p><code>Texture::displayInWindow()</code> does pixel-by-pixel
        rendering of textures for display. I refactored it to loop for
        each “row” from top to bottom, kicking off a new <code>std::thread</code>
        to calculate the pixels across the row. So for the images shown
        here, 511 threads get created, then the operating system
        schedules them in parallel onto the hardware CPU cores
        available. On my laptop (MacBook Pro (Mid 2014)) there are 8
        cores. (Or I guess four hardware cores—each of which supports
        two “hyperthreads”—so if feels like eight cores.) In any case,
        my Instruments app shows utilization at nearly 8 cores: around
        785.4% CPU usage. To see those numbers, I had to really crank up
        the rendering load. Shown below is a very smooth (almost no
        sampling noise) of the standard <em>Blur</em> benchmark. As
        described on <a href="#20200317">March 17</a>, I set <code>Blur::sqrt_of_subsample_count</code>
        to 60, meaning each <code>Blur::getColor()</code> sample
        performed 3600 (60x60) subsamples.</p>
      <p>Overall, <strong>the multi-threading version of Blur is about
          <u>five</u> times faster</strong> than the previous version,
        which itself was about twice as fast as the one before that.</p>
      <img src="images/20200329_Blur_60x60_subsamples.png" alt="Blur
        60x60 subsamples" title="Blur 60x60 subsamples" height="511"
        width="511">
      <p>We also do square textures!&nbsp; 😲 I expect to normally use
        disk-shaped renderings. There was half-hearted support in the
        code for rectangular images. In the ongoing refactoring of the
        rasterization code, I made it a little more official. Here is
        the same texture shown above, with 60x60 subsampling, and the
        new <code>disk</code> parameter set to <code>false</code> to
        change the rasterization mode.</p>
      <img src="images/20200329_Blur_60x60_square.png" alt="Blur 60x60
        square" title="Blur 60x60 square" height="511" width="511"> </div>
    <div class="post" id="20200326"> <a href="#20200326" class="date">March








































































































































































































































































































































































































































































































        26, 2020</a>
      <h1>Experiments with <em>Shade</em></h1>
      <p>[<b>Update:</b> on June 26, 2021 I removed the code for this
        experiment, long unused, from TexSyn. For reference it is now
        available <a moz-do-not-send="true"
href="https://drive.google.com/file/d/1okb1OWmXEJOEE62t2weZnh6rvRw6N7Cy/view?usp=sharing">here</a>.]<br>
      </p>
      <p>Here are some more examples of the experimental <em>Shade</em>
        operator. Three examples below show different bump maps combined
        with this <em>ColorNoise</em>:</p>
      <pre>color_noise = ColorNoise(0.6, Vec2(), 0.2)</pre>
      <img src="images/20200326_color_noise.png" alt="color_noise"
        title="color_noise" height="511" width="511">
      <p>Here the bump map is Brownian noise. The result looks a bit
        like shaded mountainous terrain.</p>
      <pre>brownian = Brownian(0.3, Vec2(9, -5), black, white)</pre>
      <img src="images/20200326_Brownian.png" alt="brownian"
        title="brownian" height="511" width="511">
      <pre>Shader(Vec3(1, 3, 6), 0.3, color_noise, brownian)  <span class="comment">// parameters are: to_light, ambient_level, color_texture, bump_texture</span></pre>
      <img src="images/20200326_Brownian_shaded.png" alt="Shader(Vec3(1,
        3, 6), 0.3, color_noise, brownian)" title="Shader(Vec3(1, 3, 6),
        0.3, color_noise, brownian)" height="511" width="511">
      <p>Here the bump map is basic Perlin noise. The result looks a bit
        like sand dunes in late afternoon.</p>
      <pre>noise = Noise(0.1, Vec2(9, -5), black, white)</pre>
      <img src="images/20200326_Noise.png" alt="noise" title="noise"
        height="511" width="511">
      <pre>Shader(Vec3(1, 3, 6), 0.3, color_noise, noise)</pre>
      <img src="images/20200326_Noise_shaded.png" alt="Shader(Vec3(1, 3,
        6), 0.3, color_noise, noise)" title="Shader(Vec3(1, 3, 6), 0.3,
        color_noise, noise)" height="511" width="511">
      <p>Here the bump map is a soft-edged Spot with a flat spot in the
        center and around the outside. The result looks a bit like
        wide-brimmed hat, seen from above, in oblique light.</p>
      <pre>spot = Spot(Vec2(), 0.3, white, 0.95, black)</pre>
      <img src="images/20200326_Spot.png" alt="spot" title="spot"
        height="511" width="511">
      <pre>Shader(Vec3(1, 3, 6), 0.3, color_noise, spot)</pre>
      <img src="images/20200326_Spot_shaded.png" alt="Shader(Vec3(1, 3,
        6), 0.3, color_noise, spot)" title="Shader(Vec3(1, 3, 6), 0.3,
        color_noise, spot)" height="511" width="511">
      <p>I am a bit surprised at how “shiny” these renders appear. This
        is a purely diffuse reflection model with no specular
        highlights. <em>Shade</em> applied to <em>Brownian</em>
        produces a very bumpy surface, but it looks a bit like wrinkled
        aluminum. (I thought perhaps it was related to RGB clipping when
        shading adds to ambient illumination. But I see it even when
        ambient is zero.) Here is <em>Shade</em> applied to <em>Brownian</em>
        (again) alongside a version smoothed with <em>Blur</em>:</p>
      <pre>Shader(Vec3(1, 3, 6), 0.3, color_noise, brownian)<br>Shader(Vec3(1, 3, 6), 0.3, color_noise, Blur(0.1, brownian))</pre>
      <img src="images/20200326_Brownian_shaded.png" alt="Shader(Vec3(1,
        3, 6), 0.3, color_noise, brownian)" title="Shader(Vec3(1, 3, 6),
        0.3, color_noise, brownian)" height="511" width="511"> <img
        src="images/20200326_Brownian_filtered.png" alt="Shader(Vec3(1,
        3, 6), 0.3, color_noise, brownian)" title="Shader(Vec3(1, 3, 6),
        0.3, color_noise, brownian)" height="511" width="511"> </div>
    <div class="post" id="20200325"> <a href="#20200325" class="date">March








































































































































































































































































































































































































































































































        25, 2020</a>
      <h1>That’s no moon...</h1>
      <p>[<b>Update:</b> on June 26, 2021 I removed the code for this
        experiment, long unused, from TexSyn. For reference it is now
        available <a moz-do-not-send="true"
href="https://drive.google.com/file/d/1okb1OWmXEJOEE62t2weZnh6rvRw6N7Cy/view?usp=sharing">here</a>.]<br>
      </p>
      <p>This <em>Shader</em> texture operator is just an experiment. I
        am not sure if it belongs in the <strong>TexSyn</strong>
        library. It is essentially a computer graphic “shader” that
        scales the brightness of a <u>color texture</u> according to
        another interpreted as a <u>bump texture</u>: a height field
        based on luminance. It scales the input color by Lambertian
        reflectance, based on a given light direction and the height
        field's surface normal. This is a test pattern whose height
        field is a hemisphere, shaded with a <code>to_light</code>
        vector of <code>Vec3(-2, 2, 4)</code>:</p>
      <img src="images/20200325_no_moon.png" alt="That’s no moon..."
        title="That’s no moon..." height="511" width="511">
      <pre>green_stripes = Grating(Vec2(), Color(0, 1, 0), Vec2(0, 0.1), Color(1,1,1), 0.2)
hemisphere = ShadedSphereTest(Vec3(0, 0, 1))
Shader(Vec3(-2, 2, 4), 0, green_stripes, hemisphere)
</pre>
      <img src="images/20200325_green_stripes.png" alt="green_stripes"
        title="green_stripes" height="511" width="511"> <br>
      <img src="images/20200325_hemisphere.png" alt="hemisphere"
        title="hemisphere" height="511" width="511"> <br>
      <img src="images/20200325_sphere_stripes.png" alt="Shader(Vec3(-2,
        2, 4), 0, green_stripes, hemisphere)" title="Shader(Vec3(-2, 2,
        4), 0, green_stripes, hemisphere)" height="511" width="511">
      <p>I am not sure if it makes sense to put 3d shading in a 2d
        texture synthesis library. I will give it more thought and try
        other experiments.</p>
    </div>
    <div class="post" id="20200323"> <a href="#20200323" class="date">March








































































































































































































































































































































































































































































































        23, 2020</a>
      <h1>Add “strength” parameter to <em>EdgeEnhance</em></h1>
      <p>In the original <a href="#20200301">March 1</a> description of
        <em>EdgeDetect</em> and <em>EdgeEnhance</em>, they were both
        shown as (like <em>Blur</em>) having two parameters: a <code>float








































































































































































































































































































































































































































































































          width</code> (the diameter of the convolution kernel) and an
        input texture. In the <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20090824">previous









































































































































































































































































































































































































































































































          version</a> of this library, <em>EdgeEnhance</em> had another
        parameter, a <code>float strength</code> that controlled how <u>how









































































































































































































































































































































































































































































































          much</u> edge enhancement there was. At first I thought that
        parameter might be unneeded, and now changed my mind. That is, <em>EdgeEnhance</em>
        is now defined as:</p>
      <pre>Add(input_texture, AdjustBrightness(strength, Subtract(input_texture, Blur(width, input_texture))))</pre>
      <p>Shown below are six combinations of filter <code>width</code>
        (big=0.2 or small=0.1) and enhancement <code>strength</code>
        (low=0.66, normal=1.0, high=1.33). Filter <code>width</code>
        controls how far across the texture the edge enhancement
        extends, while <code>strength</code> is its amplitude. At the
        end is the input texture <code>colors</code> and the code that
        defines it. Note how “flat” the input texture looks compared to
        the edge enhanced versions, how much more “pop” they have at the
        edges.</p>
      <pre>EdgeEnhance(0.2, 0.66, colors)  <span class="comment">// wide width and low strength.</span></pre>
      <img src="images/20200323_EdgeEnhance_02_066.png"
        alt="EdgeEnhance(0.2, 0.66, colors)" title="EdgeEnhance(0.2,
        0.66, colors)" height="511" width="511">
      <pre>EdgeEnhance(0.2, 1.00, colors)  <span class="comment">// </span><span class="comment">wide width and normal strength.</span></pre>
      <img src="images/20200323_EdgeEnhance_02_100.png"
        alt="EdgeEnhance(0.2, 1.00, colors)" title="EdgeEnhance(0.2,
        1.00, colors)" height="511" width="511">
      <pre>EdgeEnhance(0.2, 1.33, colors)  <span class="comment">// </span><span class="comment">wide width and high strength.</span></pre>
      <img src="images/20200323_EdgeEnhance_02_133.png"
        alt="EdgeEnhance(0.2, 1.33, colors)" title="EdgeEnhance(0.2,
        1.33, colors)" height="511" width="511">
      <pre>EdgeEnhance(0.1, 0.66, colors)  <span class="comment">// narrow</span><span class="comment"> width and low strength.</span></pre>
      <img src="images/20200323_EdgeEnhance_01_066.png" alt="" title=""
        height="511" width="511">
      <pre>EdgeEnhance(0.1, 1.00, colors)  <span class="comment">// </span><span class="comment">narrow</span><span class="comment"> width and normal strength.</span></pre>
      <img src="images/20200323_EdgeEnhance_01_100.png"
        alt="EdgeEnhance(0.1, 1.00, colors)" title="EdgeEnhance(0.1,
        1.00, colors)" height="511" width="511">
      <pre>EdgeEnhance(0.1, 1.33, colors)  <span class="comment">// </span><span class="comment">narrow</span><span class="comment"> width and high strength.</span></pre>
      <img src="images/20200323_EdgeEnhance_01_133.png"
        alt="EdgeEnhance(0.1, 1.33, colors)" title="EdgeEnhance(0.1,
        1.33, colors)" height="511" width="511">
      <pre><span class="comment">// Definition of the "colors" texture used as input for the examples above.</span>
b = Color(0, 0, 0)
w = Color(1, 1, 1)
colors = SoftMatte(SoftThreshold(0.60, 0.64,
                                 Rotate(5, Noise(0.2, Vec2(-1, -4), b, w))),
                   SoftMatte(SoftThreshold(0.60, 0.64,
                                           Rotate(3, Noise(0.2, Vec2(+1, +3), b, w))),
                             SoftMatte(SoftThreshold(0.60, 0.64,
                                                     Rotate(1, Noise(0.2, Vec2(-2, +1), b, w))),
                                       Uniform(0.5),
                                       Uniform(Color(0.8, 0.8, 0))),
                             Uniform(Color(0.8, 0, 0.8))),
                   Uniform(Color(0, 0.8, 0.8)))</pre>
      <img src="images/20200323_colors.png" alt="colors" title="colors"
        height="511" width="511">
      <p>[Update on March 24] One more example of the strength
        parameter: I wanted to look at high values of <code>strength</code>.
        Here is a some <em>Wrapulance ColorNoise</em> that has been
        reduced in saturation and brightness, then operated on by <em>EdgeEnhance</em>
        with a strength of 5.</p>
      <pre>grayish_color_noise = AdjustBrightness(0.7, AdjustSaturation(0.3, ColorNoise(0.5, Vec2(5,7), 0.8)))</pre>
      <img src="images/20200324_grayish_color_noise.png"
        alt="grayish_color_noise" title="grayish_color_noise"
        height="511" width="511">
      <pre>EdgeEnhance(0.1, 5, grayish_color_noise)</pre>
      <img src="images/20200324_EdgeEnhance_01_5.png"
        alt="EdgeEnhance(0.1, 5, grayish_color_noise)"
        title="EdgeEnhance(0.1, 5, grayish_color_noise)" height="511"
        width="511"> </div>
    <div class="post" id="20200321"> <a href="#20200321" class="date">March








































































































































































































































































































































































































































































































        21, 2020</a>
      <h1><em>SliceShear</em></h1>
      <p>I wrote the first version of <em>SliceShear</em> on January 25
        but the two tangents were interacting when they were supposed to
        be independent. After several debugging failures I set it aside.
        Today I tried again and found the right solution. This is yet
        another operator based on the idea of a “slice” through texture
        space, a one dimensional texture. Here it is used to laterally
        shift or shear another texture. (The scalar luminance of the 1D
        texture is used as a relative translation.) The parameters to <em>SliceShear</em>
        are: a tangent, center, and texture to describe the “slice”, and
        a tangent, center to describe how the input texture is sheared.
        Three examples which will hopefully make that more clear are
        below. First, the <code>for_slice</code> texture from which the
        shear-controlling slice will be taken and the <code>to_shear</code>
        test pattern to which the <em>SliceShear</em> operator will be
        applied.</p>
      <pre><span class="comment">// for_slice: noise in one direction, square wave in other direction.</span><br>white = Color(1, 1, 1)
black = Color(0, 0, 0)
gray = Color::gray(0.3)
for_slice = Add(SliceGrating(Vec2(1, 0), Vec2(), Brownian(0.1, Vec2(), black, gray)),
                Grating(Vec2(), black, Vec2(0, 0.1), gray, 0.2))</pre>
      <img src="images/20200321_for_slice.png" alt="for_slice"
        title="for_slice" height="511" width="511">
      <p>This <code>to_shear</code> is the input texture that will be
        shifted by <em>SliceShear</em> according to a slice of <code>for_slice</code>.</p>
      <pre>to_shear = Grating(Vec2(), Color(1, 0.5, 0), Vec2(0, 0.25), Color(0, 0.5, 1), 0.4)
</pre>
      <img src="images/20200321_to_shear.png" alt="to_shear"
        title="to_shear" height="511" width="511">
      <p>Here a <u>horizontal</u> slice (<code>Vec2(1, 0)</code>)
        through the center of <code>for_slice</code> produces <u>noise</u>
        that controls <u>vertical</u> shear (<code>Vec2(0, 1)</code>)
        of <code>to_shear</code>.</p>
      <pre>SliceShear(Vec2(1, 0), Vec2(), for_slice, Vec2(0, 1), Vec2(), to_shear)</pre>
      <img src="images/20200321_SliceShear_1.png" alt="SliceShear_1"
        title="SliceShear_1" height="511" width="511">
      <p>Here a <u>vertical</u> slice (<code>Vec2(0, 1)</code>) through
        the center of <code>for_slice</code> produces <u>square waves</u>
        that controls <u>vertical shear</u> (<code>Vec2(0, 1)</code>)
        of <code>to_shear</code>.</p>
      <pre>SliceShear(Vec2(0, 1), Vec2(), for_slice, Vec2(0, 1), Vec2(), to_shear)</pre>
      <img src="images/20200321_SliceShear_2.png" alt="SliceShear_2"
        title="SliceShear_2" height="511" width="511">
      <p>Here a <u>horizontal</u> slice (<code>Vec2(1, 0)</code>)
        through the center of <code>for_slice</code> produces <u>noise</u>
        that controls <u>diagonal shear</u> (<code>Vec2(1, 1)</code>)
        of <code>to_shear</code>.</p>
      <pre>SliceShear(Vec2(1, 0), Vec2(), for_slice, Vec2(1, 1), Vec2(), to_shear)</pre>
      <img src="images/20200321_SliceShear_3.png" alt="SliceShear_3"
        title="SliceShear_3" height="511" width="511"> </div>
    <div class="post" id="20200319"> <a href="#20200319" class="date">March








































































































































































































































































































































































































































































































        19, 2020</a>
      <h1>Repeatable randomness</h1>
      <p>Describing the first version of <em>Blur</em> on <a
          href="#20200225">February 25</a> I said: “...this prototype
        stochastic filter is not repeatable/deterministic. The noise
        pattern could be different on a subsequent run...” Now that is
        fixed. I used the <code>Texture::diff()</code> utility to
        compare two “identical” textures made with <em>Blur</em>:<br>
      </p>
      <pre>Texture::diff(Blur(0.2, Grating(Vec2(), yellow, Vec2(0.2, 0.2), blue, 0.01)),
              Blur(0.2, Grating(Vec2(), yellow, Vec2(0.2, 0.2), blue, 0.01)))</pre>
      <p>This <code>Texture::diff</code> comparison is from before
        today's change. The magnitude of the errors resulting from
        non-repeatable random number was actually very small. Here I
        have scaled up by a factor of 10 the brightness of of the third
        image, which is the <em>AbsDiff</em> (absolute value of the
        difference) of the two input textures:</p>
      <img src="images/20200319_Blur_diff_before.png" alt="" title=""
        height="334" width="1000">
      <p>This <code>Texture::diff</code> comparison is after today's
        change. The third texture is exactly zero/black according to <code>Texture::diff</code>'s









































































































































































































































































































































































































































































































        logging:</p>
      <img src="images/20200319_Blur_diff_after.png" alt="" title=""
        height="334" width="1000">
      <p>When looking up a color at a position in texture space <code>Blur::getColor()</code>
        now hashes the <code>Vec2</code> position to generate a “seed”,
        which is used to initialize a local <code>RandomSequence</code>
        object, which is used to jiggle the subsamples. So any two calls
        to <code>Blur::getColor()</code> with an identical position
        (exact <code>Vec2</code> equality) will use an identical set of
        subsamples. Hashing utilities for <code>float</code> and <code>Vec2</code>
        were added yesterday. <code>RandomSequence</code> was added
        today. Still working on integrating it with existing utilities
        like <code>Vec2::randomUnitVector()</code>. </p>
    </div>
    <div class="post" id="20200317"> <a href="#20200317" class="date">March








































































































































































































































































































































































































































































































        17, 2020</a>
      <h1>Blurry McBlurface</h1>
      <p>After yesterday's entry, I reconsidered my life choices. The
        newer hash-table-based-subsample-reusing version of <em>Blur</em>
        was about twice as fast but had much more unappealing noise.
        Which is to say: uglier and <u>only</u> twice as fast. That got
        me wondering. What if, in the previous version of <em>Blur</em>,
        I just reduced the number of subsamples per <code>getColor()</code>sample








































































































































































































































































































































































































































































































        so it was twice as fast? How would that compare, in terms of
        image quality, with the new version? The previous version used a
        jiggled grid of 15x15 or 225 subsamples per sample. (The current
        code assumes the grid is square.) Near one half of that is an
        11x11 grid with 121 subsamples, or a 10x10 grid with 100
        subsamples. The first two textures below are (1) previous
        version of <em>Blur</em> but with 11x11 or 121 subsamples, and
        (2) the new version of <em>Blur</em> which randomly selects 1/4
        of the subsamples of a 30x30 grid or 225 samples. The first one
        looks significantly better to me (smoother, less noisy). These
        are both approximately twice as fast as the previous version of
        <em>Blur</em> at 15x15 or 225 subsamples. All of the textures
        below are made with <code>Blur(0.2, sample)</code>. </p>
      <img src="images/20200317_Blur_old_yb_02_11x11.png" alt="121
        (11x11) subsamples" title="121 (11x11) subsamples" height="511"
        width="511"> <img src="images/20200316_Blur_newer_yb_02.png"
        alt="225 (30x30 / 4) subsamples" title="225 (30x30 / 4)
        subsamples" height="511" width="511">
      <p>For comparison, here is (1) the previous version of <em>Blur</em>
        with 15x15 or 225 subsamples, and (2) the previous version of <em>Blur</em>
        with 10x10 or 100 subsamples. The latter begins to look too
        noisy to me, so I lean toward 11x11 as the default setting.</p>
      <img src="images/20200317_Blur_old_yb_02_15x15.png" alt="225
        (15x15) subsamples" title="225 (15x15) subsamples" height="511"
        width="511"> <img
        src="images/20200317_Blur_old_yb_02_10x10.png" alt="100 (10x10)
        subsamples" title="100 (10x10) subsamples" height="511"
        width="511">
      <p>As much as I like to use hash tables, I will move that new <code>Blur</code>
        version to the code graveyard. The previous version will remain.
        I added <code>Blur::sqrt_of_subsample_count</code> to allow
        setting the <em>n</em> of the <em>n</em>² subsample grid. This
        global parameter allows adjusting the speed/quality trade-off
        for a given application.</p>
      <p>In any case, the <em>Blur</em> operator and the ones based on
        it (<em>EdgeDetect</em> and <em>EdgeEnhance</em>) are by far
        the slowest ones in TexSyn. Despite the “monkey wrenches” of
        doing convolution on procedurally defined textures—there are
        bound to be ways to accelerate that computation on a GPU.
        (Perhaps using <strong>OpenCV</strong> as the implementation
        layer, as currently done for image display and image file I/O.)
        Next time the slow performance of Blur causes trouble, it might
        be time to bite that bullet.</p>
    </div>
    <div class="post" id="20200316"> <a href="#20200316" class="date">March








































































































































































































































































































































































































































































































        16, 2020</a>
      <h1>Back to <em>Blur</em></h1>
      <p>Over the last four days I have been evaluating different ways
        to implement the <em>Blur</em> texture operator (see <a
          href="#20200226">February 26</a>). This is very well-trod
        ground in signal/image processing and computer graphics.
        Blurring (low pass filtering) is accomplished by <em>convolving</em>
        a “bell shaped” <em>kernel</em> with the input image data.
        Analytically the kernel is usually a rotationally symmetric <em>Gaussian</em>
        distribution (<u>the</u> bell curve). Among other benefits, this
        makes the 2d convolution <em>separable</em>, allowing it to be
        computed as the product of two 1d convolutions (so O(<em>n</em>)
        rather than O(<em>n²</em>)). Modern graphics hardware (GPUs)
        allow lightning fast implementation of convolution-based low
        pass filtering. It is so fast and easy that my macOS display
        system uses blurring as a design element, giving some parts of
        the GUI a real time “frosted glass” look.</p>
      <p>The structure of <strong>TexSyn</strong> throws some monkey
        wrenches into this approach. It is fine to apply convolution to
        an image, even a large image, which is bounded, rectangular, and
        uniformly sampled.&nbsp; But <strong>TexSyn</strong> textures
        have infinite (well, floating point) range, so it is daunting to
        collect the infinite input image, let alone to produce the
        infinite output. Also, most “interesting” blur kernels span a
        narrow range (too small and the output looks unchanged—too large
        and the result looks like a uniform color). But in principle
        there is no natural bound on the size of a kernel. An infinite
        texture convolved with an unbounded kernel is—well, a lot of
        computation.</p>
      <p>Of course we never need an infinite texture. We sample some
        portion of it at some sampling frequency, then display it on the
        screen or write it to an image file. But the <em>Blur</em>
        operator never knows what part of the texture will be sampled,
        nor at what frequency, nor whether that sampling is at a unifrom
        frequency across the image. (Consider a <em>MobiusTransform</em>
        (<a href="#20200128">January 28</a>) or <em>Wrap</em> (<a
          href="#20200115">January 15</a>) operator applied to the
        result of a <em>Blur</em> operator. The <em>Blur</em> will
        receive some number of <code>getColor()</code> sampling
        requests, but cannot know anything about their bounds,
        distribution, or spacing.)</p>
      <p>The approach taken previously (<a href="#20200226">February 26</a>)
        was to implement each <code>getColor()</code> call with a point
        convolution based on distributed sampling of the input under the
        kernel. An earlier version used sample points uniformly
        distributed across the nonzero part of the kernel. Then to
        reduce noise, it was changed to use a grid of “cells” each of
        which was sampled by a point uniformly distributed across the
        cell. At the time, I selected a 15x15 sampling grid spanning the
        kernel. Note that this implies the cost per <code>getColor()</code>
        call is constant (O(<em>1</em>)). It will always use 225
        subsamples. This means large kernels are the same speed as small
        ones, but the big ones have more noise.</p>
      <p>But the time to compute <em>Blur</em> seemed too long. I
        looked for ways to accelerate it. For two <code>getcolor()</code>
        calls at nearby locations—say within the diameter of the kernel,
        like “adjacent pixels”—many of the subsample cells will overlap.
        I tried caching these, and reusing them from one <code>getColor()</code>
        call to the next. But then all the samples in the vicinity of a
        cell go the identical “random” sample, which caused obvious
        artifacts on the scale of the cells. (I tried both O(<em>log n</em>)
        <code>std::map</code> (red-black trees) and O(<em>1</em>) <code>std::unordered_map</code>
        (hash tables). The hash tables were noticeably faster.)</p>
      <p>The last tweak was to cache just the one “random” sample per
        cell, but to make the grid finer (twice as many subsamples in
        each direction, so 4 times more cells) then for each <code>getColor()</code>
        to select a different random 1/4 set of cells. This is done by
        skipping whole randomly-chosen rows and columns of grid. </p>
      <p>The old version is clearly smoother and less noisy. It looks
        much better. The new version is much noisier but runs more than
        twice as fast. (New execution time is about 45% of old, for
        rendering a 511x511 texture.) At the moment I cannot choose one
        over the other. I guess I will leave both versions in for now
        and make a global switch to choose between them.</p>
      <p>This <code>sample</code> is the input for the rest of the
        texture samples:</p>
      <pre>sample = Grating(Vec2(), yellow, Vec2(0.2, 0.2), blue, 0.01)</pre>
      <img src="images/20200314_grating_yb.png" alt="sample"
        title="sample" height="511" width="511">
      <pre>Blur(0.2, sample)  // Previous version of Blur.</pre>
      <img src="images/20200314_Blur_old_yb.png" alt="old Blur(0.2,
        sample)" title="old Blur(0.2, sample)" height="511" width="511">
      <pre>Blur(0.2, sample)  // New faster, noisier version of Blur.</pre>
      <img src="images/20200316_Blur_newer_yb_02.png" alt="new Blur(0.2,
        sample)" title="new Blur(0.2, sample)" height="511" width="511">
      <pre>Blur(0.5, sample)  // Previous version of Blur.</pre>
      <img src="images/20200316_Blur_old_yb_05.png" alt="old Blur(0.5,
        sample)" title="old Blur(0.5, sample)" height="511" width="511">
      <pre>Blur(0.5, sample)  // New faster, noisier version of Blur.</pre>
      <img src="images/20200316_Blur_newer_yb_05.png" alt="new Blur(0.5,
        sample)" title="new Blur(0.5, sample)" height="511" width="511">
      <pre>EdgeDetect(0.2, sample)  // Previous version of Blur.</pre>
      <img src="images/20200316_EdgeD_old_yb_02.png" alt="old
        EdgeDetect(0.2, sample)" title="old EdgeDetect(0.2, sample)"
        height="511" width="511">
      <pre>EdgeDetect(0.2, sample)  // New faster, noisier version of Blur.</pre>
      <img src="images/20200316_EdgeD_newer_yb_02.png" alt="new
        EdgeDetect(0.2, sample)" title="new EdgeDetect(0.2, sample)"
        height="511" width="511"> </div>
    <div class="post" id="20200312"> <a href="#20200312" class="date">March








































































































































































































































































































































































































































































































        12, 2020</a>
      <h1><em>Row</em></h1>
      <p>The <em>Row</em> operator tiles the texture plane by copying
        and translating a portion (a fixed width “stripe”) of an input
        texture. Its parameters are two <code>Vec2</code> values: <code>basis</code>
        and <code>center</code>. The <code>basis</code> vector spans
        from one boundary of a “stripe” to the other (the length of <code>basis</code>
        is the width of the stripe, and its orientation is normal to the
        stripe's “axis”). The center position adjust the “phase” of
        stripes, effectively sliding the input texture relative to the
        stripe. The stripe that is centered on <code>center</code> is a
        “fixed point” of the <em>Row</em> transform, it remains
        identical to the input texture. All these examples use the same
        <code>spots_and_bg</code> textures used in <a href="#20200311">yesterday's</a>
        post about <em>Ring</em>. In each case, the <code>center</code>
        is at the origin, at the middle of these texture samples, and
        between the two spots.</p>
      <pre>Row(Vec2(0.4, 0), Vec2(), spots_and_bg)</pre>
      <img src="images/20200312_row_1.png" alt="Row(Vec2(0.4, 0),
        Vec2(), spots_and_bg)" title="Row(Vec2(0.4, 0), Vec2(),
        spots_and_bg)" height="511" width="511">
      <pre>Row(Vec2(0.35, 0.35), Vec2(), spots_and_bg)</pre>
      <img src="images/20200312_row_2.png" alt="Row(Vec2(0.35, 0.35),
        Vec2(), spots_and_bg)" title="Row(Vec2(0.35, 0.35), Vec2(),
        spots_and_bg)" height="511" width="511">
      <pre>Row(Vec2(0, 0.6),  Vec2(), spots_and_bg)</pre>
      <img src="images/20200312_row_3.png" alt="Row(Vec2(0, 0.6),
        Vec2(), spots_and_bg)" title="Row(Vec2(0, 0.6), Vec2(),
        spots_and_bg)" height="511" width="511">
      <p>An array can be formed with two nested <code>Row</code>
        operations. But note this only works “correctly” when the two <code>basis</code>
        vectors are perpendicular. Admittedly, my definition of
        “correct” is at best idiosyncratic: that all cells of the array
        be identical (and correspond to the neighborhood of <code>center</code>
        on the input texture). In the previous version of this library I
        provided an Array operator which worked “correctly” for oblique
        arrays with non-orthogonal bases (see <a
          href="https://www.red3d.com/cwr/texsyn/diary.html#20090426">here</a>).









































































































































































































































































































































































































































































































        With the benefit of hindsight, I think <strong>TexSyn</strong>
        does not need an Array operator.</p>
      <pre>Row(Vec2(0.17, 0.52), Vec2(), Row(Vec2(-0.52, 0.17), Vec2(), spots_and_bg))</pre>
      <img src="images/20200312_row_4.png" alt="Row(Vec2(0.17, 0.52),
        Vec2(), Row(Vec2(-0.52, 0.17), Vec2(), spots_and_bg))"
        title="Row(Vec2(0.17, 0.52), Vec2(), Row(Vec2(-0.52, 0.17),
        Vec2(), spots_and_bg))" height="511" width="511"> </div>
    <div class="post" id="20200311"> <a href="#20200311" class="date">March








































































































































































































































































































































































































































































































        11, 2020</a>
      <h1><em>Ring</em></h1>
      <p>The <em>Ring</em> operator takes a pie-slice-shaped sector
        from an input texture, copying and rotating around a center to
        create the whole “pie.” The parameters to <em>Ring</em> are: <code>float








































































































































































































































































































































































































































































































          copies</code>—how many “pie pieces” there are, <code>Vec2
          basis</code>—the direction of the bisector of the sector to be
        copied, <code>Vec2 center</code>—around which the sectors are
        rotated, and the input texture. The ray from <code>center</code>,
        along the direction of <code>basis</code>, is a “fixed point”
        of the <em>Ring</em> operator: texture within the fixed sector
        is identical to the input texture. The three textures shown
        below are: </p>
      <ul>
        <li>the input texture (called <code>spots_and_bg</code> below)</li>
        <li>the result of applying <em>Ring</em> with 10 <code>copies</code>,
          vertical <code>basis</code>, and <code>center</code> near
          the bottom</li>
        <li>the result of applying <em>Ring</em> with 5 <code>copies</code>,
          the <code>basis</code> along the diagonal up and to the
          right, and the <code>center</code> in the lower left.</li>
      </ul>
      <p>Note that the “shadow” in the input texture is directly below
        the bright spot. In the second example, where the center of
        rotation is below the spots, the “shadows” in each sector point
        toward the center. In the third example, the shadows point in a
        “clockwise” orientation. The <code>copies</code> parameter is
        given as a floating point value, but is interpreted as a
        positive integer greater than zero (via <code>round</code>, <code>abs</code>,
        and <code>max</code>).</p>
      <img src="images/20200311_spots_and_bg.png" alt="spots_and_bg"
        title="spots_and_bg" height="511" width="511">
      <pre>Ring(10.1, Vec2(0, 1), Vec2(0, -0.9), spots_and_bg)</pre>
      <img src="images/20200311_Ring_1.png" alt="Ring(10.1, Vec2(0, 1),
        Vec2(0, -0.9), spots_and_bg)" title="Ring(10.1, Vec2(0, 1),
        Vec2(0, -0.9), spots_and_bg)" height="511" width="511">
      <pre>Ring(-4.8, Vec2(1, 1), Vec2(1, 1) * -0.35, spots_and_bg)</pre>
      <img src="images/20200311_Ring_2.png" alt="Ring(-4.8, Vec2(1, 1),
        Vec2(1, 1) * -0.35, spots_and_bg)" title="Ring(-4.8, Vec2(1, 1),
        Vec2(1, 1) * -0.35, spots_and_bg)" height="511" width="511">
      <p>Definition for <code>spots_and_bg</code> used above:</p>
      <pre><span class="comment">// Color noise reduced in brightness and saturation.</span>
bg = AdjustBrightness(0.7,
                      AdjustSaturation(0.1,
                                       ColorNoise(0.5, Vec2(-1, -3),  0.6)))
<span class="comment">// Matte an 80% gray spot over a "shadow", then that over the background.</span>
spots_and_bg = SoftMatte(Translate(Vec2(0, 0.1), spot),
                         SoftMatte(spot, bg, AdjustBrightness(0.8, bg)),
                         Uniform(Color::gray(0.8)))</pre>
    </div>
    <div class="post" id="20200310"> <a href="#20200310" class="date">March








































































































































































































































































































































































































































































































        10, 2020</a>
      <h1>Lambda combinations — wait, what?!</h1>
      <p>Oh, sorry, it looks like another digression into arcane c++,
        but this time from the perspective of Lisp and the lambda
        calculus. (As described in the 1941 book <a
href="https://books.google.com/books?hl=en&amp;lr=&amp;id=yWCYDwAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=%22calculi+of+lambda+conversion%22#v=onepage&amp;q=%22calculi%20of%20lambda%20conversion%22&amp;f=false">The









































































































































































































































































































































































































































































































          Calculi of Lambda-Conversion</a> by Alonzo Church, a
        contemporary and collaborator of Alan Turing.) I first
        programmed in Lisp around 1972 when I was an undergraduate. I
        loved it and it has stuck with me for the rest of my life. The
        “lambda functions” introduced into c++ in 2011 appear to be
        based on the same concept in the Lisp programing language (since
        1959) which is in turn based on the same concept in the lambda
        calculus (since 1941). You can see they are very similar:</p>
      <pre><span class="comment">;; A Lisp lambda expression, a function of two arguments, returning them as a pair:</span>
(lambda (x y) (cons x y))
<span class="comment">;; To apply it to parameters, we form a “lambda combination”, a list of three
;; objects: the function and a value for each of the two parameters:</span>
((lambda (x y) (cons x y)) 1 2)
</pre>
      <pre><span class="comment">// A c++ lambda function of two arguments, returning them as a pair:</span>
[](int x, int y){return std::make_pair(x, y);}
<span class="comment">// To apply it to parameters, we write the function followed by an argument list:</span>
[](int x, int y){return std::make_pair(x, y);}(1, 2);</pre>
      <p>The key thing here is that during the execution of the lambda
        function, the parameters values (<code>1</code> and <code>2</code>)
        are <strong>bound</strong> to the parameter names (<code>x</code>
        and <code>y</code>). On <a href="#20200305">March 5</a> I
        talked about how the “lifetime” of temporary c++ objects was
        inconveniently short. Defining a nested procedural texture
        caused the not-top-level operators to be automatically deleted
        at the end of the assignment statement. (Worth noting in this
        context that the lifetime of Lisp objects is “as long as you
        need them.” It uses garbage collection to know when you are
        really done with them.) I implemented a solution, using copying,
        but it felt too clumsy. In the end I wrote a utility to take the
        nested texture tree as a parameter, then perform the needed
        tasks, before returning and allowing the temporary object to be
        deleted.</p>
      <p>What I realized today is that “lambda binding” can be used to
        fix this issue in c++ the same way lambdas and macros are used
        in Lisp to form new language extensions (e.g. <em>let</em> used
        to bind local variables). See below a “lambda combination” in
        c++. It corresponds to the code used to create the three sample
        textures for the <a href="#20200309">March 9</a> entry about <em>Mirror</em>.
        Within the curly brackets <code>{}</code> are three calls to a
        display utility. (The three were described yesterday as “...the
        original input, the result of mirroring it about the Y axis, and
        the result of mirroring about the Y axis then the X axis...”)
        Above that is the function's parameter name list: a <em>Texture</em>
        reference named <code>test</code>. After the the curly brackets
        is the parameter value list that the function is applied to: a
        nested <em>Texture</em> tree of seven operators, generators,
        and some primitives like numbers, <em>Vec2</em>s, and <em>Color</em>s.








































































































































































































































































































































































































































































































        While this is definitely a “weird” c++ style, it is limited to
        the places where it is used (sample code to support this
        document) rather than being inside the definition of all
        operators.</p>
      <pre><span class="comment">// Lexical capture spec ("all, by reference")</span>.
[&amp;]
<span class="comment">// Parameter type and name list (here a Texture reference named "test").</span>
(const Texture&amp; test)
<span class="comment">// Body of lambda where parameter values are bound to parameter names.
// Texture tree is bound to "test" for lifetime of this block.</span>
{
    Texture::displayAndFile(test);
    Texture::displayAndFile(Mirror(Vec2(0, 1), Vec2(), test));
    Texture::displayAndFile(Mirror(Vec2(1, 0),
                                   Vec2(),
                                   Mirror(Vec2(0, 1), Vec2(), test)));
}
<span class="comment">// Parameter value list, a nested tree of various Texture objects.</span>
(Subtract(Uniform(Color(1, 1, 1)),
          Multiply(ColorNoise(0.5, Vec2(-1, -3), 0.6),
                   Colorize(Vec2(1, 0),
                            Vec2(),
                            Multiply(grating, grating),
                            Brownian(0.3, Vec2(-1, -3),
                                     Color(1, 1, 1),
                                     Color(0, 0, 0))))));</pre>
    </div>
    <div class="post" id="20200309"> <a href="#20200309" class="date">March








































































































































































































































































































































































































































































































        9, 2020</a>
      <h1><em>Mirror</em></h1>
      <p><em>Mirror</em> across an arbitrary line in texture space. The
        line is defined by a tangent and center point. Shown below three
        textures: the original input, the result of mirroring it about
        the Y axis, and the result of mirroring about the Y axis then
        the X axis.</p>
      <img src="images/20200309_test.png" alt="test" title="test"
        height="511" width="511">
      <pre>Mirror(Vec2(0, 1), Vec2(), test)</pre>
      <img src="images/20200309_Mirror_y.png" alt="Mirror y"
        title="Mirror y" height="511" width="511">
      <pre>Mirror(Vec2(1, 0), Vec2(), Mirror(Vec2(0, 1), Vec2(), test))</pre>
      <img src="images/20200309_Mirror_y_then_x.png" alt="Mirror y then
        x" title="Mirror y then x" height="511" width="511">
      <p>Just a side note about the <code>test</code> pattern above. I
        was wrapping presents and one of the gift wrapping papers had a
        pattern with curvy lines. I was thinking about how to do that in
        <strong>TexSyn</strong>. Here is an experiment in that
        direction, a <code>squiggles</code> texure:</p>
      <pre>// Vertical black and white sine wave grating.
grating = Grating(Vec2(0.1, 0), Color(1, 1, 1), Vec2(0.3, 0), Color(0, 0, 0), 1)
// Sharpen brightness peaks by squaring.
grating_squared = Multiply(grating, grating)
// Brownian noise also in shades of gray.
noise = Brownian(0.3, Vec2(-1, -3), Color(1, 1, 1), Color(0, 0, 0))
// Colorize the noise with sharpened sine wave to get the squiggles texture below.
<b>squiggles</b> = Colorize(Vec2(1, 0), Vec2(), grating_squared, noise)<br><br>// The test pattern above was: squiggles times color-noise subtracted from white:<br><strong>test</strong> = Subtract(Uniform(Color(1, 1, 1)),
                Multiply(ColorNoise(0.5, Vec2(-1, -3), 0.6),
                         squiggles))</pre>
      <img src="images/20200309_squiggles.png" alt="squiggles"
        title="squiggles" height="511" width="511"> </div>
    <div class="post" id="20200308"> <a href="#20200308" class="date">March








































































































































































































































































































































































































































































































        8, 2020</a>
      <h1><em>BrightnessWrap</em></h1>
      <p><em>BrightnessWrap</em>, analogous to <em>SoftThreshold</em>,
        takes two brightness thresholds and an input texture. The
        brightness of the input texture is “wrapped around” in the sense
        of modulus (fmod) the brightness interval between the two
        thresholds. Then that brightness interval is adjusted to cover
        the interval between black and white. (The adjustment to full
        range had not been done in the previous version of this library.
        I now think it makes more sense. But I need to think about it
        some more.) These operations on brightness happen in
        hue-saturation-value color space, so only brightness (value) is
        changed. The hue and saturation remain unchanged.</p>
      <pre>gray_noise = Brownian(0.3, Vec2(-1, -3), white, black)<br>BrightnessWrap(0.4, 0.6, gray_noise)</pre>
      <img src="images/20200308_gray_noise.png" alt="gray noise"
        title="gray noise" height="511" width="511"> <img
        src="images/20200308_gray_BrightnessWrap.png"
        alt="BrightnessWrap(0.4, 0.6, gray_noise)"
        title="BrightnessWrap(0.4, 0.6, gray_noise)" height="511"
        width="511">
      <pre>color_noise = ColorNoise(0.5, Vec2(-1, -3), 0.6)<br>BrightnessWrap(0.4, 0.6, color_noise)</pre>
      <img src="images/20200308_color_noise.png" alt="color noise"
        title="color noise" height="511" width="511"> <img
        src="images/20200308_color_BrightnessWrap.png"
        alt="BrightnessWrap(0.4, 0.6, color_noise)"
        title="BrightnessWrap(0.4, 0.6, color_noise)" height="511"
        width="511"> </div>
    <div class="post" id="20200306"> <a href="#20200306" class="date">March








































































































































































































































































































































































































































































































        6, 2020</a>
      <h1><em>Twist</em></h1>
      <p><em>Twist</em> an input texture around a given <code>center</code>.
        The twist has infinite extent but falls off as 1/r. This creates
        a spiral tightly curved near <code>center</code> and
        asymptotically approaching zero curvature for increasing radius.
        The <em>Twist</em> is parameterized by an <code>angle_scale</code>
        (bigger values mean more twisting) and a <code>radius_scale</code>
        which adjusts the rate of falloff (bigger values pull the
        twisting closer to <code>center</code>). For a given radius
        from center, the twist angle is: <code>angle = angle_scale /
          ((radius * radius_scale) + 1)</code></p>
      <pre>// radial pattern:
center = Vec2(0.9, 0);
radial = SliceToRadial(Vec2(0, 0.318),
                       center,
                       Grating(Vec2(0, -0.1), Color(0.3, 0.15, 0),
                               Vec2(0, +0.1), Color(0.9, 0.9, 0),
                               0.3));</pre>
      <img src="images/20200306_radial.png" alt="radial" title="radial"
        height="511" width="511"><br>
      <pre>Twist(1, 1, center, radial)</pre>
      <img src="images/20200306_Twist_1_1.png" alt="Twist(1,1,...)"
        title="Twist(1,1,...)" height="511" width="511"><br>
      <p>With a larger <code>radius_scale</code> the twisting is pushed
        toward the center. So further out, the radial features are
        nearly straight.</p>
      <pre>Twist(1, 9, center, radial)</pre>
      <img src="images/20200306_Twist_1_9.png" alt="Twist(1,9,...)"
        title="Twist(1,9,...)" height="511" width="511"><br>
      <p>Stronger <code>radius_scale</code> causes more twisting.</p>
      <pre>Twist(7, 1, center, radial)</pre>
      <img src="images/20200306_Twist_7_1.png" alt="Twist(7,1,...)"
        title="Twist(7,1,...)" height="511" width="511"><br>
      <p>Stronger <code>radius_scale</code> and more concentration near
        center.</p>
      <pre>Twist(7, 9, center, radial)</pre>
      <img src="images/20200306_Twist_7_9.png" alt="Twist(7,9,...)"
        title="Twist(7,9,...)" height="511" width="511"><br>
      <p>An interesting mistake. Here the <em>Twist</em> is applied to
        the same point near the right side, but the radial pattern was
        located at the center.</p>
      <img src="images/20200306_near_miss.png" alt="near miss"
        title="near miss" height="511" width="511"><br>
    </div>
    <div class="post" id="20200305"> <a href="#20200305" class="date">March








































































































































































































































































































































































































































































































        5, 2020</a>
      <h1>Infrastructure / c++ temporaries</h1>
      <p>No new texture synthesis results today. Just boring code stuff.
        If you do not care about arcane c++ this is not worth reading.</p>
      <p>I went on a bit of a wild goose chase. While testing <strong>TexSyn</strong>,
        and creating the sample textures for this document, I'd been
        bothered by an issue related to the “lifetime” of temporary
        values in c++ expressions. This led me deep into the c++ rabbit
        hole, stackoverflow, and the “<a
href="https://en.wikipedia.org/wiki/Curiously_recurring_template_pattern">curiously









































































































































































































































































































































































































































































































          recurring template pattern</a>” (which reminds me of what we
        called “mixins” in the Flavors OOPS in Lisp (specifically: Lisp
        Machine Lisp) back in the 1980s). I managed to “fix” the issue I
        was digging into, then decided the solution seemed too
        intrusive/heavyweight, and not worth keeping. I will describe it
        here, save the code in the graveyard, and move on. This was the
        example I was testing with—furry blue and green stripes—which I
        think of as being written like this, three nested constructors
        of <strong>TexSyn</strong> classes: </p>
      <pre>Multiply(Grating(Vec2(-0.1, -0.1), greenish, Vec2(0.1, 0.1), bluish, 0.5),
         Furbulence(0.2, Vec2(3, 4), gray10, white))</pre>
      <img src="images/20200305_fuzzy_blue_green_stripes.png" alt="furry
        blue and green stripes" title="furry blue and green stripes"
        height="511" width="511">
      <p>This is defined as the product of two generators: <em>Multiply</em>
        applied to a <em>Grating</em> and a <em>Furbulence</em>:</p>
      <img src="images/20200305_grating.png" alt="blue and green
        stripes" title="blue and green stripes" height="511" width="511">
      <img src="images/20200305_furbulence.png" alt="furriness"
        title="furriness" height="511" width="511">
      <p>The problem I kept running into is that I want to create these
        procedural textures by writing a nested expressions (as in the
        code block above), bind them to a variable in my program, then
        invoke “member functions” of the <em>Texture</em> class on them
        to: display on the screen, write to an image file, and
        occasionally other things. Specifically, if I write:</p>
      <pre>Multiply test(Grating(Vec2(-0.1, -0.1), greenish, Vec2(0.1, 0.1), bluish, 0.5),
              Furbulence(0.2, Vec2(3, 4), gray10, white);</pre>
      <p>It creates a <em>Multiply</em> instance named <code>test</code>
        which contains references to two other <em>Texture</em>
        instances. (Generally these procedural textures are “trees” with
        operations as branching nodes and generators as leaves.)
        However, written like this, c++ considers those calls to the
        constructors of <em>Grating</em> and <em>Furbulence </em>to
        be&nbsp; “temporaries.” By the time the execution reaches the
        semicolon at end of that expression, it assumes we are done with
        them—that their “lifetime” has expired—so it deletes them
        (freeing their resources, including the memory they occupy).
        When we get around to using the composed <em>Texture</em> tree,
        only the named, root node remains. And it now has dangling
        references to nonexistent objects. Trying to then render the
        texture to the screen results in a visit to the debugger. I had
        been working around this by making each operator a named top
        level object, avoiding any use of nested (unnamed, temporary)
        subobjects:</p>
      <pre>Grating grating(Vec2(-0.1, -0.1), greenish, Vec2(0.1, 0.1), bluish, 0.5);
Furbulence furbulence(0.2, Vec2(3, 4), gray10, white);
Multiply multiply(grating, furbulence);</pre>
      <p>This does not capture the composition of nested objects used in
        TexSyn. It seemed I wanted to extend the lifetime of nested <em>Texture</em>
        objects. I found various approaches to this but none seemed
        right. I decided that perhaps the most straightforward approach
        would be to <b>copy</b> the <em>Texture</em> parameters passed
        into my operators, so preserving their lifetime as long as I
        needed them. Copying them did not seem burdensome since these
        procedural textures are very small, typically less than 100
        bytes, just to store the parameters. But copying was complicated
        by the fact that all these operators and generators are
        subclasses of <em>Texture</em> and are passed as “polymorphic”
        references (essentially pointers) of type <code>Texture&amp;</code>.
        So for example, it was trivial to copy the <em>Grating</em>
        object passed into <em>Multiply</em>, but all <em>Multiply</em>
        knew was that it had been given a generic <em>Texture</em>, not
        that it was a <em>Grating</em>. Naturally, I am not the first
        programmer who found a need to copy class instances given only a
        pointer to the base class. This stackoverflow post—<a
href="https://stackoverflow.com/questions/5731217/how-to-copy-create-derived-class-instance-from-a-pointer-to-a-polymorphic-base-c/">How









































































































































































































































































































































































































































































































          to copy/create derived class instance from a pointer to a
          polymorphic base class?</a>—was right on point, and had a <a
href="https://stackoverflow.com/questions/5731217/how-to-copy-create-derived-class-instance-from-a-pointer-to-a-polymorphic-base-c/5731259#5731259">good









































































































































































































































































































































































































































































































          answer</a> by “ltjax” It was based on that “curiously
        recurring template pattern” mentioned above. I followed the
        recipe in that answer (modifying <em>Texture</em>, <em>Multiply</em>,
        <em>Grating</em>, and <em>Furbulence</em>) and presto, it
        worked! <strong>Yay!</strong> But it would mean rewriting the
        inheritance spec and the constructor of every <strong>TexSyn</strong>
        operator. And the benefit was probably limited to “hand written”
        texture code. To be sure everything I have done over the last
        couple of months has been in that category. But the intended
        eventual use of this library for GP-based texture optimization
        would probably not benefit.</p>
      <p>In the end, I backed up, thought about the specific problem I
        had and found a lightweight solution with a small code
        footprint. Rather than try to extend the “lifetime” of
        temporaries outside a given c++ statement I just moved
        everything I needed to do inside that statement. I refactored
        some utility functions of <em>Texture</em>. Now I can pass in
        an arbitrarily nested expression of <strong>TexSyn</strong>
        element constructors. Then, during its lifetime, the texture is
        rendered to the screen, and optionally encoded into a image
        file. That is much less trouble, though perhaps not quite as
        elegant, as where I was at midday when I was at “peak geek.”
        Sometime you just need to step back from the precipice.</p>
      <p>(The API change was to add <code>Texture::displayAndFile()</code>
        and <code>Texture::waitKey()</code>. They were built from
        refactored bits of <code>Texture::displayInWindow()</code> and
        <code>Texture::writeToFile()</code>.)</p>
    </div>
    <div class="post" id="20200304"> <a href="#20200304" class="date">March








































































































































































































































































































































































































































































































        4, 2020</a>
      <h1><em>AdjustBrightness</em></h1>
      <p>The <em>AdjustBrightness</em> operator multiplies all colors
        of a texture by a given scale factor. Here we see a <em>ColorNoise</em>
        texture (as on <a href="#20200303">March 3</a>) in the center
        and the result of adjusting it by factors of 0.5 and 2. </p>
      <p>The scaling happens in RGB space. Note that for screen display,
        and for writing to image files, colors are clipped to the faces
        of the unit RGB color cube. So some of the colors will have been
        scaled up outside the unit cube and then been clipped back in,
        leading to brightness artifacts. See also <em>Multiply</em>
        which forms the product of two textures. In the previous version
        of this library there was a <em>Tint</em> operator which
        multiplied a <em>Texture</em> by a <em>Color</em>.</p>
      <pre>cn = ColorNoise(1, Vec2(2, 3), 0.8)
AdjustBrightness(0.5, cn)
AdjustBrightness(2.0, cn)</pre>
      <img src="images/20200304_AdjustBrightness_down.png"
        alt="AdjustBrightness(0.5, cn)" title="AdjustBrightness(0.5,
        cn)" height="511" width="511"> <br>
      <img src="images/20200303_ColorNoise.png" alt="original cn"
        title="original cn" height="511" width="511"> <br>
      <img src="images/20200304_AdjustBrightness_up.png"
        alt="AdjustBrightness(2.0, cn)" title="AdjustBrightness(2.0,
        cn)" height="511" width="511"> </div>
    <div class="post" id="20200303"> <a href="#20200303" class="date">March








































































































































































































































































































































































































































































































        3, 2020</a>
      <h1><em>AdjustSaturation</em></h1>
      <p>Operating in HSV (hue, saturation, value) color space, the <em>AdjustSaturation</em>
        operator applies a scale factor to saturation, then clips it
        back into the range [0, 1]. The textures below show a sample of
        <em>Wrapulance ColorNoise</em> (second texture) and the result
        of applying <em>AdjustSaturation</em> with a factor of 0.3 and
        3.0:</p>
      <pre>cn = ColorNoise(1, Vec2(2, 3), 0.8)
AdjustSaturation(0.3, cn)
AdjustSaturation(3.0, cn)</pre>
      <img src="images/20200303_AdjustSaturation_down.png"
        alt="AdjustSaturation(0.3, cn)" title="AdjustSaturation(0.3,
        cn)" height="511" width="511"> <br>
      <img src="images/20200303_ColorNoise.png" alt="original cn"
        title="original cn" height="511" width="511"> <br>
      <img src="images/20200303_AdjustSaturation_up.png"
        alt="AdjustSaturation(3.0, cn)" title="AdjustSaturation(3.0,
        cn)" height="511" width="511"> </div>
    <div class="post" id="20200302"> <a href="#20200302" class="date">March








































































































































































































































































































































































































































































































        2, 2020</a>
      <h1><em>AdjustHue</em></h1>
      <p>Operating in HSV (hue, saturation, value) color space, the <em>AdjustHue</em>
        operator rotates hue by a given <code>offset</code>. In <strong>TexSyn</strong>,
        hue is defined on the interval [0, 1]. So hue and <code>offset</code>
        are added and the fractional part of the sum (“f-modulo”)
        becomes the new hue. In this example, <em>BrightnessToHue</em>
        assigns a “rainbow” pattern to a warped gray gradient. Then <em>AdjustHue</em>
        rotates the pattern halfway around (180° of phase).</p>
      <pre>grad = Gradation(Vec2(0, -1), Color(1, 1, 1), Vec2(0, 1), Color())
warp = MobiusTransform(Vec2(0.24665, 1.44486),
                       Vec2(-0.184825, 1.64791),
                       Vec2(0.391668, -1.24418),
                       Vec2(1.04597, -0.412046),
                       grad)
color1 = BrightnessToHue(0.7, warp)
color2 = AdjustHue(0.5, color1)</pre>
      <img src="images/20200302_color1.png" alt="color1" title="color1"
        height="511" width="511"> <img src="images/20200302_color2.png"
        alt="color2" title="color2" height="511" width="511"> </div>
    <div class="post" id="20200301"> <a href="#20200301" class="date">March








































































































































































































































































































































































































































































































        1, 2020</a>
      <h1><em>EdgeDetect</em> and <em>EdgeEnhance</em></h1>
      <p><em>Blur</em> is a low pass filter that removes high
        frequencies. Conversely, <em>EdgeDetect</em> is a high pass
        filter that removes low frequencies. <em>EdgeDetect</em> is
        built on top of <em>Blur</em> using the concept of <a
          href="https://en.wikipedia.org/wiki/Unsharp_masking">unsharp
          masking</a> — a texture is blurred then, subtracting the
        blurred version from the original leaves the high frequency edge
        texture. Adding the edges back to the original gives <em>EdgeEnhance</em>
        which emphasizes the edges of the original texture. Here is “<code>grays</code>”,









































































































































































































































































































































































































































































































        a thresholded noise pattern of dark and light grays, the result
        of applying <em>EdgeDetect</em> and the result of applying <em>EdgeEnhance</em>:</p>
      <pre>EdgeDetect(0.2, grays)<br>EdgeEnhance(0.2, grays)</pre>
      <img src="images/20200301_grays.png" alt="grays" title="grays"
        height="511" width="511"> <br>
      <img src="images/20200301_grays_edge_detect.png" alt="grays edge
        detect" title="grays edge detect" height="511" width="511"> <br>
      <img src="images/20200301_grays_edge_enhance.png" alt="grays edge
        enhance" title="grays edge enhance" height="511" width="511">
      <p>Similarly, here is a thresholded noise pattern of three colors,
        the result of applying <em>EdgeDetect</em> and the result of
        applying <em>EdgeEnhance</em>:</p>
      <pre>EdgeDetect(0.2, colors)<br>EdgeEnhance(0.2, colors)</pre>
      <img src="images/20200301_colors.png" alt="colors" title="colors"
        height="511" width="511"> <br>
      <img src="images/20200301_color_edge_detect.png" alt="color edge
        detect" title="color edge detect" height="511" width="511"> <br>
      <img src="images/20200301_color_edge_enhance.png" alt="color edge
        enhance" title="color edge enhance" height="511" width="511"> </div>
    <div class="post" id="20200228"> <a href="#20200228" class="date">February









































































































































































































































































































































































































































































































        28, 2020</a>
      <h1><em>SoftThreshold</em></h1>
      <p>The <em>SoftThreshold</em> operator remaps an interval of
        brightness to “full range.” The operator's parameters are two
        intensity level (generally between 0 and 1) and an input
        texture. As in the 2009 version, for a gray scale texture, this
        means that any part of the input darker than the lower
        brightness bound is mapped to black, and any part brighter than
        the upper brightness bound is mapped to white. Parts of the
        input that fall between the two bounds are remapped to range
        between black and white. In this example, parts of the gray <em>Noise</em>
        texture below brightness 0.2 become black, above 0.7 become
        white. “Mach bands” can be seen around the bright parts of the
        image that have been clipped to white (and less obviously in the
        blacks):</p>
      <pre>grays = Noise(0.4, Vec2(), Color(0, 0, 0), Color(1, 1, 1));
threshold_grays = SoftThreshold(0.2, 0.7, grays);</pre>
      <img src="images/20200228_grays.png" alt="grays" title="grays"
        height="511" width="511"> <img
        src="images/20200228_threshold_grays.png" alt="threshold grays"
        title="threshold grays" height="511" width="511">
      <p>In addition, this version of <em>SoftThreshold</em> handles
        colored input textures in an analogous fashion. (The earlier
        version always returned a gray scale result.) The input texture
        is converted to hue-saturation-value color space, the
        value(/brightness/intensity) is remapped and clipped (as
        described above) while the hue and saturation components remain
        unchanged. Here a <em>ColorNoise</em> is thresholded by its
        intensity. For example, note that near the center of the
        thresholded texture, there is a clipped patch of yellow and
        pink. These colors are as bright as they can be (in unit RGB
        space) while their hue and saturation vary as before.</p>
      <pre>colors = ColorNoise(0.6, Vec2(), 0)
threshold_colors = SoftThreshold(0.4, 0.75, colors)</pre>
      <img src="images/20200228_colors.png" alt="colors" title="colors"
        height="511" width="511"> <img
        src="images/20200228_threshold_colors.png" alt="threshold
        colors" title="threshold colors" height="511" width="511">
      <p>This was the original motivation for <em>SoftThreshold</em>,
        dividing a noise texture into black and white regions, like the
        pattern printed on the cover of “old school” <a
          href="https://www.google.com/search?q=composition+books&amp;tbm=isch">composition









































































































































































































































































































































































































































































































          books</a>.</p>
      <pre>SoftThreshold(0.5, 0.55, Brownian(0.08, Vec2(), Color(0, 0, 0), Color(1, 1, 1)))</pre>
      <img src="images/20200228_composition_book_cover.png"
        alt="composition book cover" title="composition book coverx"
        height="511" width="511">
      <p>Something analogous from a color noise.</p>
      <pre>SoftThreshold(0.55, 0.6, ColorNoise cbc(0.15, Vec2(), 0))</pre>
      <img src="images/20200228_color_book_cover.png" alt="color book
        cover" title="color book coverx" height="511" width="511"> </div>
    <div class="post" id="20200227"> <a href="#20200227" class="date">February









































































































































































































































































































































































































































































































        27, 2020</a>
      <h1><em>Colorize</em></h1>
      <p><em>Colorize</em> is another “slice” based operator. A bit like
        <em> BrightnessToHue</em> (see <a href="#20200113">January 13,
          2020</a>) it assigns colors to an input texture based on
        brightness/intensity/luminance. But the sequence of colors comes
        from a “slice” of another input texture. The parameters to <em>Colorize</em>
        are: a <em>Vec2</em> <code>slice_tangent</code> and <code>center</code>
        which define a “ray” in texture space, then a <code>texture_for_slice</code>
        from which the colors are read, and a <code>texture_to_color</code>
        whose luminance is mapped to colors along the ray on <code>texture_for_slice</code>.
        (In fact, the “ray” is actually a line, since in <strong>TexSyn</strong>
        color RGBs are bounded only by floating point range.) Here is a
        typical call to <em>Colorize</em> and the resulting texture:</p>
      <pre>Colorize(Vec2(0, 1), Vec2(), texture_for_slice, texture_to_color)</pre>
      <img src="images/20200227_colorized.png" alt="" title=""
        height="511" width="511">
      <p>Here are the two input textures, <code>texture_for_slice</code>
        is a <em>ColorNoise</em> which supplies the sequence of colors
        (from the origin, up along the vertical axis):</p>
      <pre>texture_for_slice = ColorNoise(0.1, Vec2(2, 2), 0.6)</pre>
      <img src="images/20200227_for_slice.png" alt="" title=""
        height="511" width="511">
      <p>The <code>texture_to_color</code> parameter is a black and
        white <em> Brownian</em> texture supplying the luminance which
        is mapped to the ray of colors. The <code>texture_to_color</code>
        texture need not be monochrome, but only its luminance matters
        to <em>Colorize</em>.</p>
      <pre>texture_to_color = Brownian(0.4, Vec2(), Color(0, 0, 0), Color(1, 1, 1))</pre>
      <img src="images/20200227_to_color.png" alt="" title=""
        height="511" width="511"> </div>
    <div class="post" id="20200226"> <a href="#20200226" class="date">February









































































































































































































































































































































































































































































































        26, 2020</a>
      <h1><em>Blur</em> with a “jiggled grid” of subsamples</h1>
      <p>To reduce the noise from stochastic sampling in <em>Blur</em>,
        I tried to reduce the variance between adjacent output samples.
        One way to do this is to regularize the “random” subsamples. One
        common approach is to construct a grid over the kernel, and take
        one subsample randomized inside each square grid cell. This
        keeps the sampling density more uniform (for example you don't
        just happen to get all samples be on the left side of the
        kernel) while retaining the sampling's stochastic nature.</p>
      <p> That part seems to work. It looks like the noise now has less
        magnitude. Unfortunately it is hard to tell for sure because
        something else is definitely different between before and
        after.The new grid-based version seems to have lower contrast
        than yesterday's random-position-on-kernel version. Worse I
        don't immediately see what causes this difference. Given that
        the kernel size is the width of a black and a white stripe, the
        lower contrast seems more plausible. Here is the improved(?)
        version with a 15x15 grid of subsamples:</p>
      <img src="images/20200226_vs_blur_15x15.png" alt="15x15 grid of
        subsamples" title="15x15 grid of subsamples" height="511"
        width="511">
      <p>While here is yesterday's version with 225 subsamples randomly
        distributed on the non-zero “support” of the circular kernel,
        which has more contrast (brighter “whites” and darker “blacks”
        than today's version): </p>
      <img src="images/20200226_vs_blur_225_ss.png" alt="225 random
        subsamples" title="225 random subsamples" height="511"
        width="511">
      <p>The new version is about 20% faster since some of the gridded
        subsamples fall outside the kernel and so are just skipped.</p>
      <p><span style="color:white;">February 27, 2020</span> — An
        addendum: I'm still puzzled by the difference in contrast of the
        two texture samples above. I repeated the first sample (“jiggled
        grid” of subsamples) using a kernel width of 0.1 instead of 0.2.
        Recall that each black or white stripe has a width of 0.1. So I
        expect blurring with a 0.1 kernel to produce full white along
        the center-line of (e.g.) the white strips (since the entire
        kernel is then within the white stripe) and a soft black↔︎white
        transition between the center-lines. That is indeed what I see
        here, which makes me somewhat more confident that the new
        jiggled-grid code is doing the right thing:</p>
      <img src="images/20200227_blur_0.1_15x15.png" alt="0.1 kernel
        width, 15x15 subsamples" title="0.1 kernel width, 15x15
        subsamples" height="511" width="511"> </div>
    <div class="post" id="20200225"> <a href="#20200225" class="date">February









































































































































































































































































































































































































































































































        25, 2020</a>
      <h1>Experiments with <em>Blur</em></h1>
      <p>I would like to include a <em>Blur</em> operator in the <strong>TexSyn</strong>
        library to provide low pass filtering. (This would also provide
        a path to high pass filtering—for edge detection and edge
        enhancement—via <a
          href="https://en.wikipedia.org/wiki/Unsharp_masking">unsharp
          masking</a>.) But there are some problems to solve. One is
        that these are fundamentally kernel-based convolution operators,
        and so require significantly more computation than other
        point-based operators. This gets worse as the size of the
        convolution kernel increases. (Under evolutionary selection, how
        should we limit that size in a principled way? If evolution
        determines that a bigger and bigger kernel improves fitness, who
        are we to argue?) Beyond that, while modern computers have GPUs
        which can significantly accelerate traditional image processing
        operations, these do not directly apply to the procedural
        textures used in <strong>TexSyn</strong>. Textures here are
        resolution-agnostic, and are not stored as 2d arrays of color
        values.</p>
      <p>One possible approach to this stochastic sampling. An early
        example of this in graphics was the “distributed sampling” of
        rays in <a href="https://renderman.pixar.com/">RenderMan</a>.
        So rather than trying to define a rectangular array of “pixels”
        to feed to a traditional image processing discrete convolution
        operation, we can randomly sample points inside the kernel, look
        those up in an input texture, and compute a weighted sum
        according to the convolution kernel. This trades off computation
        for noise. Here is a pair of sharp gratings composited with <em>SoftMatte</em>:</p>
      <pre>spot = Spot(Vec2(), 0.6, white, 0.7, black)
grating1 = Grating(Vec2(), white, Vec2(0.2, 0), black, 0.01)
grating2 = Grating(Vec2(), white, Vec2(0, 0.2), black, 0.01)
no_blur = SoftMatte(spot, grating1, grating2)</pre>
      <img src="images/20200225_no_blur.png" alt="no_blur"
        title="no_blur" height="511" width="511">
      <p>Now the experimental stochastic <em>Blur</em> is applied to
        the inner horizontal grating. Here the width of the LPF kernel
        is 0.2 (note: each pair of black and white stripes has a width
        of 0.2) and 50 subsamples of the input texture are used for each
        output sample, producing this very noisy blurred texture:</p>
      <pre>vs_blur = SoftMatte(spot, grating1, Blur(0.2, grating2))</pre>
      <img src="images/20200225_vs_blur_50_ss.png" alt="vs_blur 50
        subsamples" title="vs_blur 50 subsamples" height="511"
        width="511">
      <p>Here 1000 subsamples are used per output sample. Even at this
        high sampling rate, the blurred image has noticeable noise:</p>
      <img src="images/20200225_vs_blur_1000_ss.png" alt="vs_blur 1000
        subsamples" title="vs_blur 1000 subsamples" height="511"
        width="511">
      <p>Note that in addition to the other issues discussed above, this
        prototype stochastic filter is not repeatable/deterministic. The
        noise pattern could be different on a subsequent run. In the
        future, if this stochastic approach is used, the pseudo-random
        generator should be “re-seeded” for each output sample, perhaps
        by hashing the <em>Vec2</em> texture coordinates of the sample.
        Also the number of subsamples used should probably depend on the
        area of the circular kernel.</p>
    </div>
    <div class="post" id="20200223"> <a href="#20200223" class="date">February









































































































































































































































































































































































































































































































        23, 2020</a>
      <h1>Rigid geometric transforms: <em>Scale</em>, <em>Rotate</em>,
        and <em>Translate</em></h1>
      <p>Generally <strong>TexSyn</strong> includes rigid
        transformations in the specification of its generators and
        operators to help automatic program generation by GP (see <a
          href="#20191219">December 19, 2019</a>). Primarily for
        hand-written code I wanted to include operators for simple rigid
        transformation. Each takes an input texture and either a scale
        factor, rotation angle or translation <em>Vec2</em>. See some
        examples, including simple order-dependent composition below. If
        these were to be made available to GP, <em>Scale</em> and <em>Rotate</em>
        probably should include <code>center</code> parameters.</p>
      <pre>two_spots = Add(Spot(Vec2(+0.2, 0), 0.38, Color(0.7, 0, 0), 0.4, Color()),
                Spot(Vec2(-0.2, 0), 0.38, Color(0, 0, 0.7), 0.4, Color()))</pre>
      <img src="images/20200223_two_spots.png" alt="two_spots"
        title="two_spots" height="511" width="511">
      <pre>scaled_spots = Scale(1.5, two_spots)</pre>
      <img src="images/20200223_scaled_spots.png" alt="scaled_spots"
        title="scaled_spots" height="511" width="511">
      <pre>scale_then_rotate = Rotate(pi / 4, scaled_spots)</pre>
      <img src="images/20200223_scale_then_rotate.png"
        alt="scale_then_rotate" title="scale_then_rotate" height="511"
        width="511">
      <pre>scale_rotate_translate = Translate(Vec2(0, 0.3), scale_then_rotate)</pre>
      <img src="images/20200223_scale_rotate_translate.png"
        alt="scale_rotate_translate" title="scale_rotate_translate"
        height="511" width="511"> </div>
    <div class="post" id="20200128"> <a href="#20200128" class="date">January









































































































































































































































































































































































































































































































        28, 2020</a>
      <h1><em>MobiusTransform</em></h1>
      <p>I was recently reminded of this very nice “explorable
        explanation” of the <a
          href="http://timhutton.github.io/mobius-transforms/">Möbius
          transformation</a> of the complex number plane, by Tim Hutton
        in 2016. Substituting the texture plane for the complex plane,
        and with some math-hand-holding by Robert Bridson to invert the
        transformation (thanks!), I prototyped this <em>MobiusTransform</em>
        texture operator. Its parameters are four points on the plane
        and an input texture. In these examples the input is the <code>plaid</code>
        texture as defined in the entry for <a href="#20200115">January
          15, 2020</a>. Using Hutton's interactive tool I defined the
        four points for the first example, which is just “off” an
        identity transform. That is, the perpendicular diagonal stripes
        of <code>plaid</code> have been rotated and slightly curved by
        the Möbius transformation. The next two (whose “control points”
        were randomly generated <em>Vec2</em> values within 4 units of
        the origin) show increasing amounts of warp. The fourth example
        (also randomly generated) shows an area of significant
        contraction of the input texture. The current texture operators
        use point sampling so areas of strong contraction inevitably
        produce aliasing “confetti” due to undersampling (violating <a
          href="https://en.wikipedia.org/wiki/Nyquist_frequency">Nyquist's









































































































































































































































































































































































































































































































          criteria</a>). Eventually <strong>TexSyn</strong> may be
        extended to detect these contractions and supersample them. Or
        perhaps it will just depend upon genetic programming to “vote
        down” textures with these artifacts.</p>
      <pre>// first example:
MobiusTransform(Vec2(1,2), Vec2(0,.1), Vec2(.1,0), Vec2(1,-2), plaid)

// third example:
MobiusTransform(Vec2(-0.958788, 1.64993), Vec2(-1.54534, -0.593485), Vec2(1.29155, -0.931471), Vec2(0.768266, 0.24665), plaid)</pre>
      <img src="images/20200128_MobiusTransform_0.png"
        alt="MobiusTransform 1" title="MobiusTransform 1" height="511"
        width="511"> <img src="images/20200128_MobiusTransform_5.png"
        alt="MobiusTransform 2" title="MobiusTransform 2" height="511"
        width="511"> <img src="images/20200128_MobiusTransform_1.png"
        alt="MobiusTransform 3" title="MobiusTransform 3" height="511"
        width="511"> <img src="images/20200128_MobiusTransform_2.png"
        alt="MobiusTransform 4" title="MobiusTransform 4" height="511"
        width="511"> </div>
    <div class="post" id="20200124"> <a href="#20200124" class="date">January









































































































































































































































































































































































































































































































        24, 2020</a>
      <h1><em>SliceToRadial</em></h1>
      <p><em>SliceToRadial</em> maps a “slice” of its input
        texture—specified by a <code>tangent</code> vector and <code>center</code>
        point—to rays emanating from the <code>center</code> point. The
        three examples below use the same color noise texture defined in
        the <a href="#20200123">January 23</a> entry. This operator
        introduces a discontinuity along the <code>-tangent</code>
        direction. In the first two examples that can be seen diagonally
        from the center to the lower left, and in the third example from
        the center to the left.</p>
      <pre>SliceToRadial(Vec2(1, 1), Vec2(0, 0), cn);
SliceToRadial(Vec2(1, 1), Vec2(0.5, 0.5), cn);
SliceToRadial(Vec2(1, 0), Vec2(0.5, 0.5), cn);
</pre>
      <img src="images/20200124_SliceToRadial1.png" alt="SliceToRadial
        1" title="SliceToRadial 1" height="511" width="511"> <img
        src="images/20200124_SliceToRadial2.png" alt="SliceToRadial 2"
        title="SliceToRadial 2" height="511" width="511"> <img
        src="images/20200124_SliceToRadial3.png" alt="SliceToRadial 3"
        title="SliceToRadial 3" height="511" width="511"> </div>
    <div class="post" id="20200123"> <a href="#20200123" class="date">January









































































































































































































































































































































































































































































































        23, 2020</a>
      <h1>Texture “slices” and <em>SliceGrating</em></h1>
      <p>As in an earlier version of this library, the term “slice” of a
        texture refers to the pattern of colors along a line in texture
        space. This could also be called a “1d texture” or a “transit.”
        Several operators in this library take a texture as an input,
        then ignore all but one slice of it. The slice is normally
        specified by a tangent vector and a point. The magnitude of the
        tangent serves as a parameter of the operator.</p>
      <p>So for example, <em>SliceGrating</em> takes a slice and
        “sweeps” it perpendicular to the tangent. The slice is specified
        by two <em>Vec2</em> parameters: <code>slice_tangent</code>
        and <code>center</code>. The length of <code>slice_tangent</code>
        becomes a scale factor along the slice, relative to the <code>center</code>.
        Here we see the input texture, then three <em>SliceGrating</em>s
        made from it, each with a different scale. In all three
        examples, the <code>center</code> of the transform is in the
        upper right at (0.5, 0.5).</p>
      <pre>cn = ColorNoise(0.6, Vec2(5, -2), 0.6);
SliceGrating(Vec2(1, 2) * 2.0, Vec2(0.5, 0.5), cn);
SliceGrating(Vec2(1, 2) * 1.0, Vec2(0.5, 0.5), cn);
SliceGrating(Vec2(1, 2) * 0.5, Vec2(0.5, 0.5), cn);
</pre>
      <img src="images/20200123_color_noise.png" alt="color_noise"
        title="color_noise" height="511" width="511"> <img
        src="images/20200123_SliceGrating1.png" alt="sg1" title="sg1"
        height="511" width="511"> <img
        src="images/20200123_SliceGrating2.png" alt="sg2" title="sg2"
        height="511" width="511"> <img
        src="images/20200123_SliceGrating3.png" alt="sg3" title="sg3"
        height="511" width="511"> </div>
    <div class="post" id="20200121"> <a href="#20200121" class="date">January









































































































































































































































































































































































































































































































        21, 2020</a>
      <h1><em>Stretch</em></h1>
      <p>This <em>Stretch</em> operator scales its input texture along
        a given direction by a given factor (“anisotropic scaling”). Two
        before-and-after examples are shown below. </p>
      <p>First, a texture called <code>color_noise</code>, and the
        result of stretching it by a factor of 0.2 at a 45° angle, with
        the transformation centered at the origin. If you trace along
        the diameter from lower left to upper right you might be able to
        see the same color pattern.</p>
      <pre>Stretch(Vec2(0.2, 0).rotate(pi / 4), Vec2(0, 0), color_noise)</pre>
      <img src="images/20200121_ColorNoise.png" alt="ColorNoise"
        title="ColorNoise" height="511" width="511"> <img
        src="images/20200121_Stretch_1.png" alt="Stretch 1"
        title="Stretch 1" height="511" width="511"> <br>
      <br>
      <p>Second, a texture called <code>three_spots</code>, and the
        result of stretching it by a factor of 2, with the
        transformation centered at the cyan spot's center (called <code>p2</code>)
        along the direction from there to the origin. As a result, the
        original cyan spot and the stretched cyan ellipse share the same
        center. The center of the yellow and magenta spots have been
        displaced by the stretch.</p>
      <pre>Stretch((-p2).normalize() * 2, p2, three_spots)</pre>
      <img src="images/20200121_three_spots.png" alt="three_spots"
        title="three_spots" height="511" width="511"> <img
        src="images/20200121_Stretch_2.png" alt="Stretch 2"
        title="Stretch 2" height="511" width="511"> </div>
    <div class="post" id="20200120"> <a href="#20200120" class="date">January









































































































































































































































































































































































































































































































        20, 2020</a>
      <h1><em>ColorNoise</em></h1>
      <p>Like <em>MultiNoise</em>, the <em>ColorNoise</em> generator
        takes parameters <code>float scale</code>, <code>Vec2 center</code>,
        and a float “<code>which</code>” to select among the types of
        noise generators. It creates three “analogous” but uncorrelated
        noise textures which are used as the red, green, and blue color
        components. The three <em>ColorNoise</em> examples below are a
        low frequency basic Perlin <em>Noise</em>, a <em>Wrapulence</em>
        at twice that frequency, and a higher frequency (12.5⨉) <em>Furbulence</em>.</p>
      <pre>ColorNoise(1, Vec2(-7, 4), 0);
ColorNoise(0.5, Vec2(-18, -20), 0.8);
ColorNoise(0.08, Vec2(15, -12), 0.6);</pre>
      <img src="images/20200120_ColorNoise_1.png" alt="ColorNoise 1"
        title="ColorNoise 1" height="511" width="511"> <br>
      <img src="images/20200120_ColorNoise_2.png" alt="ColorNoise 2"
        title="ColorNoise 2" height="511" width="511"> <br>
      <img src="images/20200120_ColorNoise_3.png" alt="ColorNoise 3"
        title="ColorNoise 3" height="511" width="511"> </div>
    <div class="post" id="20200119"> <a href="#20200119" class="date">January









































































































































































































































































































































































































































































































        19, 2020</a>
      <h1>Texture diff tool</h1>
      <p>I made a debugging tool—<code>Texture::diff()</code>—that does
        a “diff” of two textures, prints some numerical metrics, then
        displays the two inputs and diff textures for visual comparison.
        The tool uses a new texture operator called <em>AbsDiff</em>
        that simply takes the absolute value (norm) of the difference
        between corresponding points on the two input textures. This
        “abs of diff” is applied to the three RGB components
        independently. This is a test of the diff utility on two
        completely different textures:</p>
      <pre>n = Noise(0.2, Vec2(), Color(1, 0, 0), Color(1, 1, 0));
g = Grating(Vec2(), Color(0, 1, 1), Vec2(0.1, 0.1), Color(0, 0, 1), 0.5);
Texture::diff(n, g);</pre>
      <img src="images/20200119_Texture_diff.png" alt="Texture::diff()"
        title="Texture::diff()" height="350" width="1000"> <br>
      <br>
      <p>I used this diff tool to test a change to <code>Texture::rasterizeDisk()</code>
        and while trying to come up with a closed form inverse mapping
        for contraction in <em>StretchSpot</em>. A black third panel
        would indicate the new code was equivalent to the old code.
        These colored fringes indicate a mismatch:</p>
      <img src="images/20200119_StretchSpot_diff.png"
        alt="Texture::diff()" title="Texture::diff()" height="349"
        width="1000"> </div>
    <div class="post" id="20200116"> <a href="#20200116" class="date">January









































































































































































































































































































































































































































































































        16, 2020</a>
      <h1><em>StretchSpot</em></h1>
      <p><em>StretchSpot</em> makes a “fish eye” bulge of enlargement
        within a given circular area of an input texture. Or, if the
        scale factor is less than 1, makes a zone of contraction within
        the circular area. The code below corresponds to the first
        image. The second is the same except for the signs on the <code>dist</code>
        values specifying the centers of stretch. Each is a series of
        four applications of <em>StretchSpot</em> applied sequentially,
        two enlarging and two contracting. The input texture is the same
        <code>plaid</code> example used below in the <a
          href="#20200115">January 15</a> entry.</p>
      <pre>radius = 0.8
dist = 0.65
StretchSpot(4.0,
            radius,
            Vec2(+dist, -dist),
            StretchSpot(0.2,
                        radius,
                        Vec2(-dist, -dist),
                        StretchSpot(0.2,
                                    radius,
                                    Vec2(-dist, +dist),
                                    StretchSpot(4.0,
                                                radius,
                                                Vec2(+dist, +dist),
                                                plaid))))</pre>
      <img src="images/20200116_StretchSpot_1.png" alt="StretchSpot 1"
        title="StretchSpot 1" height="511" width="511"> <img
        src="images/20200116_StretchSpot_2.png" alt="StretchSpot 2"
        title="StretchSpot 2" height="511" width="511"> </div>
    <div class="post" id="20200115"> <a href="#20200115" class="date">January









































































































































































































































































































































































































































































































        15, 2020</a>
      <h1><em>Wrap</em></h1>
      <p>Texture operator <em>Wrap</em> takes a half-plane of its input
        texture and wraps it radially around a given point. The wrapping
        is defined by three parameters: a float <code>width</code> that
        determines how much of the half plane is used in the wrap, the <em>Vec2d</em>
        <code>center</code> point of the wrap, and a <em>Vec2d</em> <code>fixed_ray</code>
        from the center that will remain unchanged by the wrap. A
        “strip” of the half plane (<code>width/2</code> on both sides of
        <code>fixed_ray</code>) is transformed radially around <code>center</code>.
        (That is, a series of rays parallel to <code>fixed_ray</code>,
        and displaced perpendicular to it, become radial rays emanating
        from the <code>center</code> point. This is related to a
        rectangular-(Cartesian)-to-polar transform.) <em>Wrap</em>
        leads to a discontinuity in the direction of <code>-fixed_ray</code>
        where the two edges of the “strip” become adjacent in the
        resulting wrapped texture.</p>
      <p>In the example below we see a test pattern called “plaid” and
        its image under two applications of the <em>Wrap</em> operator.
        In <code>wrap1</code>, the center is at the origin and <code>fixed_ray</code>
        points straight up. In <code>wrap2</code>, the center is at
        (0.2, 0.2) and <code>fixed_ray</code> points along the main
        diagonal (1, 1). There is a discontinuity in <code>wrap2</code>
        along (-1, -1) while <code>wrap1</code> just happens to match
        up and appear continuous. In both cases aliasing from point
        sampling is apparent near the center of wrap.</p>
      <pre>plaid = Add(Grating(Vec2(0, 0), Color(1, 0, 0),
                    Vec2(0.1, 0.1), Color(0.3, 0, 0), 0.3),
            Grating(Vec2(0, 0), Color(0, 1, 0),
                    Vec2(-0.1, 0.1), Color(0, 0.3, 0), 0.3))
                    
wrap1 = Wrap(5, Vec2(0, 0), Vec2(0, 1), plaid)
wrap2 = Wrap(5, Vec2(0.2, 0.2), Vec2(1, 1), plaid)</pre>
      <img src="images/20200115_plaid.png" alt="“plaid” texture"
        title="“plaid” texture" height="511" width="511"> <br>
      <img src="images/20200115_Wrap_1.png" alt="Wrap 1" title="Wrap 1"
        height="511" width="511"> <img src="images/20200115_Wrap_2.png"
        alt="Wrap 2" title="Wrap 2" height="511" width="511"> </div>
    <div class="post" id="20200113"> <a href="#20200113" class="date">January









































































































































































































































































































































































































































































































        13, 2020</a>
      <h1><em>BrightnessToHue</em></h1>
      <p>The <em>BrightnessToHue</em> operator takes a texture and a
        hue_phase. It maps luminance values on [0, 1] to hue. Luminance
        values 0 and 1 both map to hue_phase and pass through all other
        hues in between. Here we define a gray scale pattern called
        “gray_gratings” and colorize it with <em>BrightnessToHue</em>.
        We see two version, with hue_phase of 0.0 and 0.5, which are
        180° out of phase. So for example, those 8 spots horizontally
        across the middle are red in one and cyan (“anti-red”) in the
        other.</p>
      <pre>gray_gratings = Add(Grating(Vec2(), black, Vec2(0, 2), gray50, 1),
                    Add(Grating(-basis1, black, basis1, gray25, 1),
                        Grating(-basis2, black, basis2, gray25, 1)))
                        
BrightnessToHue(0.0, gray_gratings)
BrightnessToHue(0.5, gray_gratings)</pre>
      <img src="images/20200113_gray_gratings.png" alt="gray_gratings"
        title="gray_gratings" height="511" width="511"> <br>
      <img src="images/20200113_BrightnessToHue.png"
        alt="BrightnessToHue, hue_phase=0" title="BrightnessToHue,
        hue_phase=0" height="511" width="511"> <img
        src="images/20200113_BrightnessToHue_2.png"
        alt="BrightnessToHue, hue_phase=0.5" title="BrightnessToHue,
        hue_phase=0.5" height="511" width="511"> </div>
    <div class="post" id="20200112"> <a href="#20200112" class="date">January









































































































































































































































































































































































































































































































        12, 2020</a>
      <h1>GP considerations and <em>MultiNoise</em></h1>
      <p>Thinking ahead to use with genetic programming, I added an
        alternate version of the noise textures. <em>MultiNoise</em>
        has the same parameters as the other noise texture generators,
        plus one additional number between 0 and 1 which selects between
        the five noise generators. This serves two purposes. (I think,
        although it remains to be seen.) First, this selection parameter
        is subject to “jiggle” mutation, allowing the type of noise
        (e.g. <em>Turbulence</em> versus <em>Furbulence</em>) to vary
        under the control of evolutionary selection pressure. In
        addition, I was concerned about letting noise textures
        “dominate” the function set by having two many variations. This
        effects the choices made during GP's initial construction of
        random programs, which in turn influences the rest of the run.
        On the other hand, it may make more sense to explicitly control
        this by giving each GP function a “likelihood of being chosen
        for random program construction” parameter. If so, that value
        could be set lower for the five varieties of noise generators.
        The program for this demo texture is perhaps a little too
        fiddly, but roughly: </p>
      <pre>noise = MultiNoise(scale, center, black, magenta, 0.0);
brownian = MultiNoise(scale, center, black, red, 0.2);
turbulence = MultiNoise(scale, center, black, yellow, 0.4);
furbulence = MultiNoise(scale, center, black, green, 0.6);
wrapulence = MultiNoise(scale, center, black, cyan, 0.8);
auto spot = [&amp;](float r){return Spot(center, r, black, r+0.05, white);};
SoftMatte(spot(0.2),
          wrapulence,
          SoftMatte(spot(0.4),
                    furbulence,
                    SoftMatte(spot(0.6),
                              turbulence,
                              SoftMatte(spot(0.8),
                                        brownian,
                                        noise))))</pre>
      <img src="images/20200112_MultiNoise.png" alt="MultiNoise texture
        generator" title="MultiNoise texture generator" height="511"
        width="511"> </div>
    <div class="post" id="20200111"> <a href="#20200111" class="date">January









































































































































































































































































































































































































































































































        11, 2020</a>
      <h1>“<em>Wrapulence</em>”</h1>
      <p><em>Wrapulence</em> is my name for yet another variation on <em>Turbulence</em>.
        Here, each octave of the basic noise signal is scaled up in
        brightness then “wrapped” down into the range [0, 1] using
        something like an <code>floor()</code> or <code>fmod()</code>.
        The result is that the sharp stepwise changes in intensity,
        wrapping from bright to dark, happen “at all scales.” (Or at
        least at several scales. Recall that these multi-octave, 1/f
        fractal noise generators use 10 levels of recursion.)</p>
      <pre>Wrapulence(0.9, Vec2(-2, -9), Color(0, 0, 0), Color(1, 0.6, 0))</pre>
      <img src="images/20200111_Wrapulence.png" alt="Wrapulence noise
        texture generator" title="Wrapulence noise texture generator"
        height="511" width="511"> </div>
    <div class="post" id="20200110"> <a href="#20200110" class="date">January









































































































































































































































































































































































































































































































        10, 2020</a>
      <h1>“<em>Furbulence</em>”</h1>
      <p><em>Furbulence</em> is my name for a variation on <em>Turbulence</em>.
        Like <em>Brownian</em>, each are 1/f fractal noise. Perlin's <em>Turbulence</em>
        introduces sharp features at the bottom (dark) end of the noise
        signal, using an absolute value to fold the negative parts of
        the signal up into the positive range. Similarly, <em>Furbulence</em>
        uses <u>two</u> absolute values, one to fold the dark end up,
        and one to fold the bright end down (with the scaling and
        shifting needed to make that work). The result is that there are
        sharp discontinuities at the dark <u>and</u> bright ends of the
        <em>Furbulence</em> signal. In this example there are sharp
        hair-like features in the bright and dark regions of the
        texture, here colored reddish blue and bluish red.</p>
      <pre>Furbulence(0.25, Vec2(-1, 2), Color(1, .1, .3), Color(.3, .1, 1))</pre>
      <img src="images/20200110_Furbulence.png" alt="Furbulence noise
        texture generator" title="Furbulence noise texture generator"
        height="511" width="511">
      <p>Just a comparison of <em>Furbulence</em> and <em>Turbulence</em>.
        The inner <em>Furbulence</em> can be seen to have sharp
        features in both white and yellow. The outer <em>Turbulence</em>
        has soft cloud-like patches of blue broken by sharp black
        cracks.</p>
      <pre>SoftMatte(Spot(Vec2(0, 0),
               0.1, Color(0, 0, 0),
               0.9, Color(1, 1, 1)),
          Furbulence(0.1, Vec2(1, 2),
                     Color(1, 1, 1), Color(.7, .7, 0)),
          Turbulence(0.2, Vec2(-5, 7),
                     Color(0, 0, 0), Color(.3, .3, 1)))</pre>
      <img src="images/20200110_comparison.png"
        alt="Furbulence/Turbulence comparison"
        title="Furbulence/Turbulence comparison" height="511"
        width="511"> </div>
    <div class="post" id="20200106"> <a href="#20200106" class="date">January









































































































































































































































































































































































































































































































        6, 2020</a>
      <h1>Turbulence</h1>
      <p><em>Turbulence</em> is a variation on noise presented in Ken
        Perlin's groundbreaking original SIGGRAPH 1985 paper: <a
href="http://www.heathershrewsbury.com/dreu2010/wp-content/uploads/2010/07/AnImageSynthesizer.pdf">An









































































































































































































































































































































































































































































































          Image Synthesizer</a>. Like <em>Brownian</em> noise, <em>Turbulence</em>
        is composed of multiple octaves. At each level, the
        negative-going part of the basic noise signal is “folded up”
        with an absolute value operator. This produces soft/rounded
        features in the bright part of the texture and
        sharp/discontinuous features in the dark parts of the texture.
        In the example below, the sharp “valleys” are colored in dark
        magenta, while the soft cloud-like highlights are colored
        orange.</p>
      <pre>Turbulence(0.3, Vec2(2, -5), Color(0.3, 0, 0.3), Color(1, 0.6, 0))</pre>
      <img src="images/20200106_Turbulence.png" alt="Turbulence noise
        texture generator" title="Turbulence noise texture generator"
        height="511" width="511"> </div>
    <div class="post" id="20200104"><a href="#20200104" class="date">January








































































































































































































































































































































































































































































































        4, 2020</a>
      <h1><em>Brownian </em>— fractal 1/f Perlin noise</h1>
      <p>This is 10 layers (octaves) of Perlin noise. The base layer is
        as shown on <a href="#20200103">January 3</a>. Each subsequent
        octave is scaled down by half in amplitude and size (doubling
        its frequency). Subsequent octaves are also rotated by 2.0
        radians to dis-align it with the other layers.</p>
      <pre>Brownian(0.20, Vec2(3, 5), Color(0, 1, 0), Color(0.3, 0.3, 0.3))</pre>
      <img src="images/20200104_Brownian.png" alt="Brownian noise
        texture generator (green clouds on gray)" title="Brownian noise
        texture generator (green clouds on gray)" height="511"
        width="511"> </div>
    <div class="post" id="20200103"><a href="#20200103" class="date">January








































































































































































































































































































































































































































































































        3, 2020</a>
      <h1>Perlin <em>Noise</em></h1>
      <p>This is a <strong>TexSyn</strong> generator wrapped around the
        basic Perlin <code>noise()</code> function as described in his
        <a href="https://mrl.nyu.edu/%7Eperlin/noise/">SIGGRAPH 2002
          paper <em>Improving Noise</em></a>. The parameters are a
        scale factor, a center/translation, and two colors.</p>
      <p><strong>Update</strong>: this “two point” approach was
        implemented on <a href="#20200531">May 31</a>: <span
          class="designnote">(Design note: I don't like that the noise
          pattern is always “axis aligned” in this formulation. I could
          add a rotation angle. But I am leaning toward changing to a
          specification with two points (<em>Vec2</em>s)—like used in <em>Gradation</em>—or








































































































































































































































































































































































































































































































          a point and a basis vector to specify a whole transform:
          translation, rotation, and scale.)</span></p>
      <pre>Noise(0.1, Vec2(3, 5), Color(0, 0, 1), Color(1, 1, 0))</pre>
      <img src="images/20200103_Noise.png" alt="Noise texture generator
        (blue and yellow blobs)" title="Noise texture generator (blue
        and yellow blobs)" height="511" width="511"> </div>
    <div class="post" id="20200101d"> <a href="#20200101d" class="date">January









































































































































































































































































































































































































































































































        1, 2020</a>
      <h1>Catching up: <em>Max</em> and <em>Min</em></h1>
      <p>The <em>Max</em> and <em>Min</em> operators compose two
        textures together. Each point's color comes from whichever input
        texture has the <em>Max</em> (or <em>Min</em>)
        luminance/brightness at the corresponding point. These examples
        show the result with the same two <em>Grating</em>s used in the
        <em>SoftMatte</em> example. In the first image, the white parts
        of the vertical Grating push to the front and the black parts
        push to the back. They are covered by the horizontal Grating.
        Note that magenta is in front of blue. The second image is the
        same code with <em>Min</em> instead.</p>
      <pre>Max(Grating(Vec2(-0.2, 0), Color(0, 0, 0),
            Vec2( 0.2, 0), Color(1, 1, 1), 1),
    Grating(Vec2(0, -0.1), Color(1, 0, 1),
            Vec2(0,  0.1), Color(0, 0, 1), 0.2))</pre>
      <img src="images/20200101_Max.png" alt="Max texture operator"
        title="Max texture operator" height="511" width="511"> <img
        src="images/20200101_Min.png" alt="Min texture operator"
        title="Min texture operator" height="511" width="511"> </div>
    <div class="post" id="20200101c"> <a href="#20200101c" class="date">January









































































































































































































































































































































































































































































































        1, 2020</a>
      <h1>Catching up: <em>SoftMatte</em></h1>
      <p>The <em>SoftMatte</em> Operator take three Textures as
        parameters. The first is interpreted as the “matte.” When it's
        luminance value is zero, the result is taken from the second
        Texture. When the matte's luminance value is one, the result is
        taken from the third Texture. Matte values in between result in
        a linear interpolation between the other Textures. In this
        example the matte Texture is a <em>Spot</em>. The outer dark
        part of the <em>Spot</em> takes the second texture, a sinusoid
        <em>Gradient</em> of vertical black and white bands. The inner
        bright part of the spot takes the third texture, a “slightly
        soft square wave” <em>Gradient</em> of horizontal magenta and
        blue stripes.</p>
      <pre>SoftMatte(Spot(Vec2(0, 0),
               0.2, Color(1, 1, 1),
               0.8, Color(0, 0, 0)),
          Grating(Vec2(-0.2, 0), Color(0, 0, 0),
                  Vec2( 0.2, 0), Color(1, 1, 1), 1),
          Grating(Vec2(0, -0.1), Color(1, 0, 1),
                  Vec2(0,  0.1), Color(0, 0, 1), 0.2))</pre>
      <img src="images/20200101_SoftMatte.png" alt="SoftMatte texture
        operator" title="SoftMatte texture operator" height="511"
        width="511"> </div>
    <div class="post" id="20200101b"> <a href="#20200101b" class="date">January









































































































































































































































































































































































































































































































        1, 2020</a>
      <h1>Catching up: <em>Grating</em></h1>
      <p> The <em>Grating</em> generator creates a swept stripe pattern
        whose cross-sectional “slice” is a periodic waveform. The
        parameters to <em>Grating</em> include: </p>
      <ul>
        <li>two <em>Vec2</em> positions, the endpoints of a line
          segment, which define the orientation and spacing (wavelength)
          of the stripe pattern</li>
        <li>two <em>Colors</em> for the striped pattern</li>
        <li>a “softness” parameter were 0 means square wave and 1 means
          sine wave (so in this example, 30% of the way from square to
          sinusoid).<br>
        </li>
      </ul>
      <pre>Grating(Vec2(0.1, 0.1), Color(0, 0.8, 0),
        Vec2(0.5, 0.3), Color(0.85, 0.85, 0),
        0.3)</pre>
      <img src="images/20200101_Grating.png" alt="Grating texture
        generator (green and yellow stripes)" title="Grating texture
        generator (green and yellow stripes)" height="511" width="511">
    </div>
    <div class="post" id="20200101"> <a href="#20200101" class="date">January









































































































































































































































































































































































































































































































        1, 2020</a>
      <h1>Catching up: <em>Gradation</em></h1>
      <p>Having installed a modern version of <strong>OpenCV</strong>
        and integrated it into <strong>TexSyn</strong>, I can now see
        the textures that before I could only verify with unit tests.</p>
      <p>The <em>Gradation</em> generator defines two colored “half
        planes” with a smooth (sinusoidal) transition between them. The
        parameters to <em>Gradation</em> are two “colored positions.”
        The line segment between the two points defines the orientation
        and width of the transition region. Outside the transition
        region the texture is uniformly the nearest of the two colors.</p>
      <pre>Gradation(Vec2(0.4, -0.2), Color(0.9, 0.0, 0.0),
          Vec2(-0.4, 0.2), Color(0.9, 0.9, 0.9));</pre>
      <img src="images/20200101_Gradation.png" alt="Gradation texture
        generator (red to white)" title="Gradation texture generator
        (red to white)" height="511" width="511"> </div>
    <div class="post" id="20191230"> <a href="#20191230" class="date">December









































































































































































































































































































































































































































































































        30, 2019</a>
      <h1>“First light” and the <em>Spot</em> operator</h1>
      <p>These are the first textures displayed by <strong>TexSyn</strong>
        captured as screen shots. They show the result of the <em>Spot</em>
        texture generator. The first one was tiny and was missing its
        center constant-color region. The <strong>OpenCV</strong>
        utility <code>cv::imshow()</code> is used to rasterize and
        display the procedural texture. The first image used a 24 bit
        (three 8 bit unsigned integers) RGB representation, called <code>CV_8UC3</code>
        in <strong>OpenCV</strong>. A bug caused “full brightness”
        colors to wrap around to zero, causing the black center. I fixed
        the bug and switched to an image format with three 32 bit
        floating point number per stored pixel (<code>CV_32FC3</code>).
        Later I may switch to four channel images to accommodate alpha
        matting. </p>
      <p><strong>TexSyn</strong>'s <code>Texture</code> class supports
        arbitrary resolution—so there is no static storage of pixels—all
        color values are computed procedurally “on the fly” represented
        as three 32-bit floating point values. The <strong>OpenCV</strong>
        images (<code>cv::mat</code> class) are used only at the output
        end, before displaying a texture on the screen, or writing it to
        an image file.</p>
      <p>This <em>Spot</em> Texture has been specified to be centered
        at the origin, with an inner radius of 0.2 and an inner color of
        white. Its outer radius is 0.6 and outer color is black. Between
        0.2 and 0.6 there is a soft sinusoidal transition between the
        inner and outer colors: </p>
      <pre>    Spot(Vec2(0, 0),
         0.2, Color(1, 1, 1),
         0.6, Color(0, 0, 0));</pre>
      <br>
      <img src="images/20191229_first_light_crop.png" alt="“first light”
        image" title="“first light” image" height="560" width="514"> <br>
      <img src="images/20191229_Spot_crop.png" alt="texture generator
        Spot" title="texture generator Spot" height="536" width="515"> </div>
    <div class="post" id="20191229"><a href="#20191229" class="date">December









































































































































































































































































































































































































































































































        29, 2019</a>
      <h1>Infrastructure</h1>
      <p>From December 15 through December 28 the basic infrastructure
        was constructed. This included:</p>
      <ul>
        <li>Basic <code>c++</code> classes for <strong>Texsyn</strong>:</li>
        <ul>
          <li><code>Texture</code> (base class for all types of
            textures)</li>
          <ul>
            <li><code>Generator</code> (creates a texture from primitive
              values)</li>
            <li><code>Operator</code> (combines one or more input
              textures (and primitive values) to produce a new texture)</li>
          </ul>
          <li>Primitive values to parameter textures:</li>
          <ul>
            <li><code>Vec2</code> (a position on the infinite 2D texture
              plane)</li>
            <li><code>Color</code> (very generalized representation of a
              point in color space, defined as RGB values over the
              entire floating point range. During texture composition,
              these values range over [-∞, +∞] but are clipped to [0, 1]
              for display. Includes conversion to hue/saturation/value
              color space and luminance.)</li>
          </ul>
        </ul>
        <li>A <strong>Utilities</strong> package to support
          interpolation, clipping, remapping, randomization, and noise.</li>
        <li>A suite of <strong>unit tests</strong> to verify the
          correct operation of primitives, texture generators, and
          operators.</li>
        <li>An interface to the <strong>OpenCV</strong> library to
          provide:</li>
        <ul>
          <li>Basic utilities such as displaying rasterized textures in
            windows on the screen, and writing them to standard image
            file formats.</li>
          <li>Eventually help with implementing some operators, and
            particularly to provide access to acceleration on GPU or
            other hardware.</li>
        </ul>
        <li><br>
        </li>
      </ul>
      <p>For more details, see the <a
          href="https://github.com/cwreynolds/TexSyn/">code</a> and the
        <a href="https://github.com/cwreynolds/TexSyn/commits/master"><code>git</code>
          commit history</a>.</p>
    </div>
    <div class="post" id="20191219"> <a href="#20191219" class="date">December









































































































































































































































































































































































































































































































        19, 2019</a>
      <h1>Designing for genetic programming</h1>
      <p>Most software libraries are, of course, intended for use by
        human programmers.There is a design aesthetic (“design
        pattern”?) that leans toward minimal functionality (where a
        function should “do just one thing”) and conversely duplication
        should be avoided. More complicated functionality arise from
        composing the minimal units of a library.</p>
      <p>In contrast, <strong>TexSyn</strong> is intended primarily for
        use by a genetic programming system. GP is a type of automatic
        programming technique, driven by evolutionary optimization. My
        experience has been that this suggests different “best
        practices” for library design. This will be revisited in future
        posts, but here is an example to give a sense of this issue.</p>
      <p>Imagine a texture generator called <em>Spot</em>, a disk of
        one color on a field of another color. A minimalist design might
        define a spot with a default diameter (say 1) at a default
        location (say the origin), and default colors (a white spot on a
        black background). As such, this minimalist <em>Spot</em> could
        have zero parameters. That was the initial approach taken in
        2008 for the previous version of this library. Using standard
        techniques of composition of software operators, a programmer
        might use a <em>Scale</em> operator to adjust the spot's
        diameter, a <em>Translation</em> operator to change its
        position, and perhaps a <em>Soft</em><em>Matte</em> operation
        to use the basic black-and-white pattern to modulate other
        colors or textures. </p>
      <p>This suggests a requirement for context in the composition of
        functions when writing programs with this library. If we are
        going to call <em>Spot</em>, we will, in general, need to
        surround it with calls to <em>Scale</em>, <em>Translate</em>,
        etc. A human programmer would understand how to handle this. An
        automated programming system would not, or at least would need
        to be augmented to supply the required context. In random
        programs constructed by GP, without that context, we would
        expect to see a bias toward <em>Spot</em> often exhibiting its
        default radius and position, because it did not happen to be
        modified by <em>Scale</em>, <em>Translate</em>, etc. An
        alternative to declaring and maintaining this context, is to
        make these transformations part of the basic <em>Spot</em>
        definition. So for example <em>Spot</em> could have several
        parameters, such as a center position, a radius, and the two
        colors to use.</p>
      <p><strong>TexSyn</strong> will use this this approach, often
        giving texture generators and operators additional parameters to
        establish context. This makes it explicit to the genetic
        programming system that, for example, a <em>Spot</em> always
        needs a position to be specified because it is a required
        parameter to <em>Spot</em>. This removes the need to add extra
        complexity related to required context. </p>
    </div>
    <div class="post" id="20191215"> <a href="#20191215" class="date">December









































































































































































































































































































































































































































































































        15, 2019</a>
      <h1>A new library</h1>
      <p>Today I created <strong><a
            href="https://github.com/cwreynolds/TexSyn">Texsyn</a></strong>,
        a new repository on <strong>GitHub</strong>, part of a project
        about adversarial evolutionary texture synthesis.</p>
      <p><strong>TexSyn</strong> is a library for procedural texture
        synthesis. It is intended for use by a <em>genetic programming</em>
        (“GP”) system, a type of <em>genetic algorithm</em>. The GP
        system performs <em>simulated evolution</em> on a <em>population</em>
        of individual programs, according to a <em>fitness function</em>.
        (Also known as a <em>fitness metric</em>, <em>utility function</em>,
        or a <em>loss function</em> in machine learning.) In this
        application to texture synthesis, the programs are compositions
        of functions from the <strong>TexSyn</strong> library. When
        executed they describe a <em>color texture</em>, an <em>image</em>.</p>
      <p>This is a re-implementation and update to the <strong>TextureSynthesisTest</strong>
        library as described in <a
          href="https://www.red3d.com/cwr/texsyn/diary.html">Texture
          Synthesis Diary</a> and used as the basis of the 2011 paper <a
          href="https://www.red3d.com/cwr/iec/">Interactive Evolution of
          Camouflage</a>.</p>
      <p>(<span style="background-color: red;">Note:</span> as of <a
          href="#20200506">May 6, 2020</a> an incompatible change was
        made to improve gamma handling. For entries dated before then,
        the given TexSyn code, if re-rendered anew, will produce a
        texture that looks different from the one shown in the doc. It
        will generally be brighter and less saturated.)</p>
    </div>
    <div class="post" id="0">
      <p>This page, and the software it describes, by <a
          href="https://www.red3d.com/cwr">Craig Reynolds</a></p>
    </div>
  </body>
</html>
