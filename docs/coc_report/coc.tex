% \documentclass[sigconf,anonymous,review,timestamp]{acmart}
% \documentclass[sigconf]{acmart}
\documentclass[acmtog]{acmart}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% prototyping CNN diagram
% \documentclass[sigconf, tikz]{acmart}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Conference title}{January 01--02,
  1900}{Someplace, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers of the event,
%% and this ID should be used as the parameter to this command.
\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
\citestyle{acmauthoryear}

%% To fix a margin violation bug, apparently due to poor hyphenation.
\hyphenation{Go-men-so-ro}
%% For introducing terms which have a special meaning in this work.
\newcommand{\jargon}[1]{\textit{#1}}


\usepackage{changepage}% http://ctan.org/pkg/changepage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% prototyping CNN diagram


% \usepackage{import}
% \subimport{./layers/}{init}
% \usetikzlibrary{positioning}

% \def\ConvColor{rgb:yellow,5;red,2.5;white,5}
% \def\ConvReluColor{rgb:yellow,5;red,5;white,5}
% \def\PoolColor{rgb:red,1;black,0.3}
% \def\DcnvColor{rgb:blue,5;green,2.5;white,5}
% \def\SoftmaxColor{rgb:magenta,5;black,7}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{document}

\title{Coevolution of Camouflage}

%% Author
\author{Craig Reynolds}
\email{cwr@red3d.com}
\orcid{0000-0001-8203-712X}
\affiliation{%
  \institution{unaffiliated researcher}
  \country{USA}
}

\renewcommand{\shortauthors}{Craig Reynolds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
  Natural camouflage seems to arise from competition between predators who must find prey to survive, and prey who must avoid detection to survive. This work simulates a simplified, abstract model of this adversarial relationship. It looks at \textit{crypsis} through evolving prey camouflage patterns (as color textures) in competition with evolving predator vision which learns to locate camouflaged prey. The environment for this 2D simulation is provided by a set of photographs, typically of natural scenes. This model is based on an evolving population of predators and another of prey. The mutual conflict between these populations tends to produce both effective camouflage and skilled predators. This simulation provides a method for creating camouflage patterns for arbitrary backgrounds, and more significantly an experimental \textit{artificial life} model for investigating aspects of camouflage evolution in nature. All code and research notes are available on GitHub.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Generate your CCSCML using http://dl.acm.org/ccs.cfm.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010341.10010349.10011810</concept_id>
       <concept_desc>Computing methodologies~Artificial life</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371.10010382.10010384</concept_id>
       <concept_desc>Computing methodologies~Texturing</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224</concept_id>
       <concept_desc>Computing methodologies~Computer vision</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
    <concept>
       <concept_id>10010147.10010178.10010224.10010245.10010246</concept_id>
       <concept_desc>Computing methodologies~Interest point and salient region detections</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
    <concept>
        <concept_id>10010147.10010257.10010293.10011809.10011813</concept_id>
        <concept_desc>Computing methodologies~Genetic programming</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial life}
\ccsdesc[300]{Computing methodologies~Texturing}
\ccsdesc[300]{Computing methodologies~Computer vision}
\ccsdesc[300]{Computing methodologies~Interest point and salient region detections} 
\ccsdesc[300]{Computing methodologies~Genetic programming}

%% Keywords
\keywords{camouflage, coevolution, nature, biology, predator, prey, vision, learning, texture synthesis, simulation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Teaser figure that appears on the top of the article.
\begin{teaserfigure}
    %% TODO note: use images without the “predator prediction crosshairs”.
    \includegraphics[scale=0.24]{images/20220918_step_7372.png}
    \hfill
    \includegraphics[scale=0.24]{images/20220926_step_6143.png}
    \hfill
    \includegraphics[scale=0.24]{images/20221003_step_3667.png}
    \hfill
    \includegraphics[scale=0.24]{images/20220930_step_6093.png}
    \caption{Photographs of natural textures, each overlaid with three camouflaged \textit{prey}. The prey are randomly placed 2D disks, each with its own evolved camouflage texture. (Note: for best results zoom into the digital version. [QQQ replace stand-in images]}
    \Description{Examples of camouflage textures produced by the simulation.}
    \label{fig:teaser}
    % \vspace{5mm} %5mm vertical space
    \vspace{3mm} % 3mm vertical space
\end{teaserfigure}

%% Lay out the single column “top matter” defined above.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[scale=0.16]{images/20221030_1220_step_19.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_1045.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_2014.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_3059.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_6650.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_7467.png}
    \caption{Prey camouflage evolving over simulation time to become more effective in a given environment.}
    \Description{Sequence of images showing evolution of prey camouflage over simulated time.}
    \label{fig:time_sequence}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
This work aims to create a simple, abstract 2d simulation model of camouflage evolution in nature. These camouflage patterns emerge from the interaction — the coevolution — of a population of simulated \jargon{prey} each with a candidate texture and a population of simulated \jargon{predators} each with a learned visual detector. The input to the simulation is a set of photos of a background environment. The prey evolve to be \jargon{cryptic} (hard to see) against the background images. The predators evolve and learn to hunt the prey by locating their position within that 2D environment.
\par
Computational models of complex biological systems have several benefits. Constructing them, getting them to work as seen in nature, helps crystallize our thinking about natural phenomenon. Computational models also allow experimentation \textit{in silico} to help characterize complex natural systems.
\par
Following the approach of \citet{Reynolds2011} a population of prey, each with a synthetic camouflage texture, are evolved in response to negative selection by a predator seeking the prey which are most visually conspicuous against a given background. In that game-like interactive simulation, the predator was a human “player.” The simulation would display a photographic background image overlaid with camouflaged prey. The human predator would select, with a mouse click, the most conspicuous five out of ten displayed prey. Those prey — “eaten” by the predator — were removed from the population.  They were replaced by offspring created with genetic \jargon{crossover} between surviving prey (as \jargon{parents}) followed by \jargon{mutation}. This simulation step was repeated about 2000 times.
\par
In the simulation described here, evolution of prey camouflage closely follows that earlier work. But now, the human-in-the-loop is replaced with a second population, of predators, each based on a deep neural network. These CNN networks take an image as input and produce a \jargon{prediction}, an estimate, of where in the image the most conspicuous prey is located. The input to these neural nets is a a small RGB image (a 128x128x3 tensor: 49,152 floating point numbers) and the output is two floats, interpreted as an \textit{xy} position relative to the image. 
\par
(Note that the images in this paper are rendered at 512² pixels and the prey disks have a diameter of 100 pixels. These images are downsampled to 128² for use by predator's vision CNNs.)
\par
A bit of explanation for the term “abstract model.” In artificial life models it is common to focus in detail on one aspect of a natural system. In the current model, that aspect is the coevolutionary dynamics between prey camouflage and predator vision. To make this feasible, other levels of organization, are ignored, or assumed, or represented by a simple computation stand-in. So for example, the entire living organism that embodies these predators and prey, are simply assumed to exist, behaving as animals do, and are otherwise ignored. This model has a simple abstract representation of biological morphogenesis as programs (nested expressions) in TexSyn's language for procedural texture synthesis. \textit{Genetic Programming} provides a simple model of evolution which acts on this “genetic” representation, creating new “offspring” textures through crossover and mutation. On the predator side, we ignore all of the animal's existence except for the key aspect of hunting behavior: looking at a scene and forming an opinion about where in the scene a prey is likely located. These predators can adapt to the appearance of an environment and the prey found there, by learning from its experience. Predators compete with each other on the basis of their ability to hunt prey and so eat. These details of simulated evolution, morphogenesis, vision, and genetic representation are all completely unlike the natural world. The assumption is that they are sufficiently similar in effect to allow a plausible simulation of the natural system, producing analogous results, and so may provide insights about the natural system.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[width=\textwidth]{images/coc_overview.pdf}
    \caption{Overview of one step of the coevolutionary simulation of camouflage. Three prey are selected at random from their population of 200. Similarly for three predators from their population of 20. A random background image is selected from the given set, and a random crop of 512×512 pixels. The three prey are rendered to random non-overlapping locations. This composite image is given to each predator which estimates a position (circled crosshairs in tournament image, see also Figure \ref{fig:predator_responses}) predicting the center point of the most conspicuous prey. The predators are scored by “aim error” — the distance from their estimate to the ground truth center of the nearest prey. If the best predator's estimate is \textit{inside} a prey's disk, that prey is eaten and replaced by a new offspring of the other two prey. If the worst scoring predator finds no prey, it may die of starvation, to be replaced by a new offspring.}
    \Description{Overview diagram of the coevolutionary simulation of camouflage evolution.}
    \label{fig:simulation_overview}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
This work builds on \citet{Reynolds2011} by replacing the human predator there with an evolving population of procedural predators. Those predators \jargon{hunt} using a learning vision model. An earlier attempt at this approach, using coevolution and procedural predators, was described by \citet{harrington_coevolution_2014}.
\par
Closely related work on learning surface textures to camouflage 3D objects within real 3D scenes is described for cubes using one technique in \citet{owens_camouflaging_2014} and for arbitrarily shaped 3D objects using an improved technique in \citet{guo_ganmouflage_2022}. In both cases, the 3D scene is described by a set of photos of it from various viewpoints. The textures mapped onto the object to be camouflaged must trade off being inconspicuous from all viewpoints in the scene.
\par
Other computer graphics work related to camouflage include detailed reproduction of coloration patterns on real animals \cite{de_gomensoro_malheiros_leopard_2020} and generation of visual puzzles incorporating camouflaged images \cite{chu_camo_image_2010} \cite{Zhang_Yin_Nie_Zheng_2020}. CamoEvo \cite{hancock_camoevo_2022} is a toolbox for authoring online camouflage evolution games to understand evolution in real biological species. Being web-based, such simulations can be powered by very large numbers of human volunteers — citizen scientists — an instance of \jargon{human-based computation}.
\par
This adversarial coevolutionary simulation has clear similarities to \jargon{generative adversarial networks} (GANs) originally described in \citet{goodfellow_gan_2014}. Observer-driven optimization of camouflage patterns is a natural application of GANs, as in CamoGAN \cite{talas_camogan_2020}. However the goal of the current work (as suggested in the Future Work section of \citet{Reynolds2011}) is to produce a simulation of a natural system, suitable for “what if” experiments which cannot be performed in the natural world. [... QQQ see several related thoughts in Notes app ...]
\par
The procedural texture synthesis used here to generate camouflage patterns has a long history. This work is perhaps most directly inspired by \citet{perlin_image_1985} where images are rendered from purely procedural representation of 3D textures. A recent example of this approach is found in \citet{Guerrero_MatFormer_2022}. The combination of texture synthesis under control of a genetic algorithm goes back to \citet{sims_artificial_1991}, which in turn was inspired by the interactive biomorph evolution demo in \citet{dawkins_blind_1986}. [... Latham and Todd ...]
\par
Evolution is represented in this model using \jargon{genetic programming} (GP), a type of population-based evolutionary optimization algorithm, first described by \citet{cramer_representation_1985} and popularized by \citet{koza_genetic_1992}. GP is a variation of \jargon{genetic algorithms} (GA). GA traditionally uses a fixed length bit string as its genetic representation, while GP uses an arbitrarily-sized tree-shaped representation. GP trees conveniently map onto nested expressions in a domain specific language. Texture synthesis in this work is based on nested expressions of texture operators from the TexSyn library, see Figure \ref{fig:TexSyn_overview}. A prey population of these textures is optimized for camouflage effectiveness by GP using the selection pressure of a population of predators which serve to determine fitness. TexSyn is used with the \jargon{strongly typed} variant of Genetic Programming known as STGP \cite{montana_strongly_1995}, one of several grammar-based GP variants \cite{Mckay_2010}.
\par
The biological literature on camouflage and related topics is vast. A few starting points include: [... historical survey of campoouflage in nature ... \citet{thayer_concealing-coloration_1909}, early work on mathematical models of biological patterns \citet{turing_chemical_1952}, revisiting Turing's work with modern computation ... \citet{murray_how_1988},  ...how life evolves and learns... \citet{valiant_probably_2013}, QQQ find other basic bio references from my 2010 paper? ...]
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Camouflaged object detection}
In the last several years, there has been burst of related research on \jargon{camouflaged object detection} (COD) in computer vision. See for example this well-curated list of COD publications by \citet{visionxiang_cod}. COD is part of predator behavior: detecting the presence of a camouflaged object. But these COD systems focus on \jargon{segmenting} the camouflaged object: finding a per-pixel mask of its projection in an image. A recent example, which surveys this topic and presents its own strong solution, is \citet{Zhang2022}. Other research on COD include some based on boundaries \cite{chen_boundary-guided_2022} \cite{sun_boundary-guided_2022}, an attempt to rank camouflaged objects by “conspicuousness” \cite{lv_cod_2022}, and a mixed-scale approach \cite{pang_zoom_2022}.
\par
COD attempts \textit{a priori} camouflage “breaking” — detecting the presence of well camouflaged objects — without learning either the background or the typical appearance of prey camouflage found in a given environment. That is, they seek to be a \jargon{generalist} predator, effectively using a strong form of \jargon{salience}. As summarized in \citet{Zhang2022}, COD is based on several labeled datasets (CHAMELEON, CAMO, and COD10K) carefully annotated by hand at the pixel level. Incremental progress in this area is measured by the per-pixel accuracy of predicted shapes of camouflaged objects.
\par
In contrast, the goal of the simulation reported in this paper is to pit camouflage evolution against vision-based hunting. In that context, determining the exact (pixel level) shape of the prey is largely irrelevant. (Especially small differences, as between a mask that is for example, 93\% correct versus 96\% correct.) The simulation described here ignores segmentation, abstracting prey as a disk of constant size, so it is sufficiently characterized by its center position. A real world predator can aim its attack at a prey's center without an exact segmentation. This work simulates predators learning to find prey despite evolving camouflage patterns. Thus this model \jargon{adapts} to dynamic camouflage rather than approach COD as a static task of generalist detection. Significantly, this work requires no hand-labeled datasets at all, using a form of \jargon{self-supervision}.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[width=\textwidth]{images/texsyn_overview.pdf}
    \caption{TexSyn expression trees and crossover between them, using a simplified version of TexSyn with three texture operators (\texttt{spots}, \texttt{stripes}, and \texttt{warp}) plus four named solid color textures. Minimal operator trees are shown in (a) and (b). \textit{Crossover} between (a) and (b) is shown in (c) and (d). (c) is \texttt{spots} where \texttt{blue} is replaced with \texttt{stripes}. (d) is \texttt{stripes} where \texttt{gray} is replaced with \texttt{spots}. (e) and (f) show (c) and (d) under a \texttt{warp} operator. (See Supplemental Material for actual TexSyn c++ code used for these examples.)}
    \Description{Overview of TexSyn representation of texture and the idea of \textit{crossover} between expression trees.}
    \label{fig:TexSyn_overview}
\end{figure*}

%% maybe more detail about GP and TexSyn in appendix? I think that does not add to page count

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Components of the simulation}
\subsection{Coevolution, populations, and fitness}
This camouflage simulation is based on two adversarial \jargon{populations}: one of \jargon{predators} and one of \jargon{prey}. Individual prey compete for survival within their own population, similarly for predators. Predators must \jargon{hunt} successfully to “eat” and so survive. Prey survive if inconspicuous (“cryptic”) enough to avoid being found and eaten. A prey hunted and eaten, or a predator perished from hunger, is removed from its population. It is replaced by an \jargon{offspring} of parents from the surviving population.
\par
Predators define the fitness of prey: being easy to spot is bad, blending in is good. Similarly prey define the fitness of predators: being fooled by camouflage is bad, spotting cryptic prey is good. From this adversarial interaction, the two populations \jargon{coevolve}. If one side has some sort of flaw, the other side has a motivation to exploit it. As a result both sides tend to improve over simulation time.
\par
Initial random prey have coloration likely to contrast with the background. Initial predators have a “pre-trained” ability to find conspicuous objects which may allow them to hunt these initial un-camouflaged prey. As coevolution proceeds, prey become better camouflaged against the given background images. In parallel, predators become more attuned to hunting these prey on the given backgrounds.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tournaments, competition, relative fitness}
\label{subsec:tournaments}
It is common in \jargon{evolutionary computation} to define \jargon{fitness} as a function that maps an \jargon{individual} (of the evolving population) into a number. That is, the function somehow evaluates the individual and assigns it a score. Typically this fitness function is \jargon{idempotent}: its value depends only on properties of the given individual. This can be seen as \jargon{absolute fitness}.
\par
In contrast, this simulation uses \jargon{relative fitness}, determined by \jargon{competition} between individuals, in the context of \jargon{tournaments}. A tournament is a game — a contest — between multiple individuals. 
\par
A simple example of relative fitness is a foot race. The winner of the race is the first to cross the finish line. The order of finishing \jargon{sorts} the racers by speed. Races have been run this way since ancient times. Today it is simple to measure each runner's speed but it is not required to determine who won the race. Now consider two people playing chess. We do not know how to measure or predicts a player's skill. But by pitting them against each other, having them play a game (or a series of games) the results provide a useful measurement of relative fitness. 
\par
Throughout this model, \textbf{tournaments involve three individuals}. (This contrasts with the work in \citet{Reynolds2011} which used tournaments of 10.) So in overview, one simulation step consists of drawing three prey individuals out of the population to compete in a tournament. Like the foot race, or chess game, the tournament serves to sort the individual according to relative fitness, without requiring assigning absolute fitness. That relative fitness is provided by the action of adversarial predators. During the same simulation step, three predators are drawn from their population. The predators look at the same input: an image with three camouflaged prey overlaid on a background image. The predators compete with each other for accurate hunting of the prey. The prey compete with each other by hiding (avoiding being targeted) by the predators.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Negative selection, drift, and mixability}

A modern synthesis \cite{livnat_sex_2016} of evolution theory, game theory, and machine learning (“no-regret learning” — the multiplicative weights update algorithm) suggests evolution may act to optimize \jargon{mixability} — a type of modularity in the genetic representation of phenotypes. As described in \citet{chastain_multiplicative_2013} this perspective focuses on the “...special case of weak selection in which all fitness values are assumed to be close to one another. ...hypothesizing that evolution proceeds for the most part not by substantial increases in fitness but by essentially random drift...” This concept goes back to the “Neutral Theory” in \citet{kimura_evolutionary_1968}. Another helpful perspective on the interaction of evolution and learning can be found in \citet{valiant_probably_2013}. [... out of place here? QQQ]
\par
The evolutionary model used in the current work, implemented in software called \jargon{LazyPredator,} attempts to operate in this “neutral” mode. Genetic algorithms often seek to promote the population's highest fitness individuals. They allow these high fitness individuals to survive longer and reproduce more often. This skews the evolutionary search: too much \jargon{exploiting} without enough \jargon{exploring.}  In contrast, LazyPredator uses \textit{negative selection} to encourage \jargon{drift}. Higher fitness individuals experience almost no difference in  relative fitness. It is the lowest fitness individuals that get culled by predation. In each tournament (see section \ref{subsec:tournaments}) the goal is to sort the three individuals by relative fitness. In fact, the only requirement is to identify the \textbf{least fit} of the three individuals.
\par 
For example, a predator looks at a scene then predicts the location of the most conspicuous of the three prey. If this location is within the disk-shaped “body” of a prey, it is the \textbf{least} fit of the tournament and gets “eaten.” The relative fitness ordering of the other two prey is not significant (it is arbitrary). If the predator \jargon{fails} and predicts a position outside all three prey, then the entire simulation step is abandoned, no prey is eaten, and the prey population is unchanged.
\par
In the predator population, a tournament is ranked by the \textbf{distance from a predator's prediction to the center of the nearest prey} — essentially the predator's “aiming error.” The worst predator may then die from \jargon{starvation} based on its recent history of hunting success: has it eaten enough to survive? (Currently this is 35\% hunting success over its last 20 attempts.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Offspring, crossover, and mutation}
In the prey population, a tournament is used to find the lowest relative fitness. This corresponds to the worst camouflage, the \jargon{most conspicuous} of three prey in a tournament. If a predator successfully locates a prey, we assume it is captured and eaten. The object representing that departed prey is removed from its population and replaced with a new individual. This is the \jargon{population update} stage of a steady-state genetic programming system.
\par
The new prey, which replaces the one killed by predator, is called the \jargon{offspring} of two other prey. This is the motivation for using tournaments of size 3: the least fit prey dies, and is replaced by the offspring of the other two prey. We don't know which of them has higher relative fitness, but either way they become the \jargon{parents} given as input to the \jargon{crossover} operation.
\par
In GP, individuals are represented as tree structures. (Which in this work are interpreted as TexSyn programs as described in the next subsection.) The crossover operation is defined on two abstract trees. The two parents are first copied to preserve them. One copy is chosen as recipient and one as donor. In each a “random subtree” is selected. The recipient's random subtree is replaced by the donor's random subtree, by splicing the pointers between tree nodes. This is shown in Figure \ref{fig:TexSyn_overview}.
\par
After crossover, a mutation operator further modifies the offspring tree. It basically traverses the tree, finding all the leaf nodes, which here correspond to numerical constants in the texture programs. Because LazyPredator uses \jargon{strongly typed genetic programming} \cite{montana_strongly_1995} each leaf belongs to a specific user defined type (for example a floating point number between 0 and 1). A method of the type's class can mutate (“jiggle”) the constant value of a leaf node (for example, add a small random offset, then clip into range). After crossover and mutation the new offspring is inserted into the population, replacing the dead prey.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Texture synthesis}
Textures are represented in this simulation as trees of procedural texture operators. These correspond directly to nested expressions in a typical programming language. \jargon{TexSyn} is a simple domain specific language for describing textures.
\par
The details of TexSyn are not central to understanding this camouflage model. A quick overview is given here. TexSyn is library, an API, with many \jargon{operators} each of which return a \jargon{Texture}. Almost all of them also take Textures as input parameters, along with simple values like colors, 2d vectors, and floating point numbers. Writing nested functional expressions of operators corresponds to trees of operator instances.
\par
These TexSyn style textures are represented as operators, trees, and parameters. They do not store pixel data, providing instead a function to sample the texture's color at an arbitrary floating point \textit{xy} location. This is similar to GPU \jargon{fragment shaders} and the Pixel Stream Editor functions in \cite{perlin_image_1985}.
\par
[... see Figure \ref{fig:TexSyn_overview} ...]
\par
 [... moved to here from Related Work: ... These textures are object-oriented so specific types are implemented as (c++) classes, with specific parameters stored in instances. The base class provides interfaces for operations such as returning a color for a given position on the (infinite) texture plane. ...]
 \par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% prototyping CNN diagram

\begin{figure}
    \includegraphics[width=\columnwidth]{images/predator_cnn.pdf}
    \caption{Architecture of a predator's deep neural net which maps a 128² RGB image into an \textit{xy} location within the image where it predicts the most conspicuous prey is centered. All convolution layers but the first use strides of (2,2) while increasing the number of learned filters. Then fully connected layers reduce the flattened features, by factors of 4, down to an output layer of 2 values. (Additional details in Supplemental Materials.)}
    \Description{Diagram of Predator's convolutional neural net.}
    \label{fig:predator_cnn}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Full step/tournament image with predator responses

\begin{figure}
    \includegraphics[width=\columnwidth]{images/20221007_0806_step_7030.png}
    \caption{Tournament image after simulation step. Starts with crop from given background set. Composited over that are three prey with evolved camouflage patterns. Over that are drawn three \jargon{crosshair} marks showing the responses of three predators. These are ranked by minimum distance to a prey center. Best is in black and white, second best is green and black, third is red and black. Here, only the top ranked predator chose a position inside a prey's disk, the other two failed. Choice of prey, predators, background image, crop, and prey placement — are all uniform random selection.}
    \Description{Full tournament image showing background, three prey, and three predator responses.}
    \label{fig:predator_responses}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predator vision}
[... some of this is covered in the Introduction, should any of that be moved here, or is the duplication OK? ...]
\par
It is the predator's job in this simulation to look at an image and “hunt” for prey. Specifically an image is built from a portion of a background photo and overlaid with three camouflaged prey. The camouflage texture for each disk shaped prey is rendered on the TexSyn side. Because all images in this simulation are synthetic, they are labeled with \jargon{ground truth} position data for each prey. This allows predators to learn in a \jargon{self-supervised} manner.
\par
The basis for a predator's visual system is \textbf{a \jargon{pre-trained} deep neural net model for a “find conspicuous disk” task}, see Figure \ref{fig:predator_cnn}. The goal of this task is to look at an arbitrary image and locate the center point of the most conspicuous (salient) region, assumed to be a prey-sized disk. This pre-trained model is created as a preparatory step then reused for each subsequent camouflage evolution run. TexSyn was used to generate a dataset of 20,000 labeled training examples called \jargon{FCD5}. Each example was an RGB image, a 128×128×3 tensor, with an associated label: an \textit{xy} coordinate pair indicating a location in the image.
\par
Each image has a random crop of an image from a background set. Over it was rendered one or three prey disks with \jargon{random texture}. While a random texture is a slippery concept, the meaning here is the sort of prey texture used to initialize the prey population before an evolution run. The LazyPredator genetic programming engine has facilities for creating random trees of a given size from a user-defined \jargon{function set} such as one corresponding to the TexSyn library. So the random tree corresponds to a random nested expression of TexSyn operators with randomly chosen leaf constants. As such these prey textures are quite varied, but have a fair amount of structure, they are not uncorrelated noise.
\par
Each image in the “find conspicuous disk” training dataset is generated in one of three styles chosen with equal probability. Style 1 has a single prey disk, and the label is its center point. Style 2 has three different prey. Style 3 has three copies of a single prey disk. The latter two cases reduce the visibility of two of the three disks, by blending or dithering the pixels of the prey disk into the background. The label corresponds to the disk whose visibility is not reduced. See Figure \ref{fig:fcd5_samples}.
\par
During simulations of camouflage evolution, each predator is initialized as a copy of the pre-trained “find conspicuous disk” model to which noise is added. (Zero mean noise of less than 0.003 is added to each parameter of the deep neural net.) In each simulation step, the three predators chosen to participate in the tournament (see Figure \ref{fig:simulation_overview}) predict a prey position. Then \textbf{each predator is \jargon{fine-tuned} based on a dataset of labeled images collected during this simulation run}. This dataset is a collection of images from earlier simulation steps in this run. It is essentially the memory of recent activity in this environment. The fine-tuning dataset starts empty, then collects each tournament image until the dataset holds 500 images. Thereafter, each new tournament image replaces one of the 500 chosen at random. Labels for this dataset correspond to predictions made by the “best” predator (least aim error) in each step (see Figure \ref{fig:predator_responses}).
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Runs — temp section — QQQ}
[... this recap and discussion of runs feels less like Discussion and more part of Methods — which I don't have but maybe should be the final subsection in \textbf{Components of the Simulation} — for now I will break this off into a new temp section ...]
\par
To run this coevolutionary simulation model, two processes are launched. One is the evolutionary texture synthesis system that models a population of prey with camouflage [... c++ / TexSyn / LazyPredator ...]. The other is a “predator server” that manages a population of visual hunters and fine-tunes their visual perception [... Python / PredatorEye ...]. The prey side produces a labeled tournament image. It is inspected by the predator side, which sends back a target location within the image, indicating its estimate of where the most conspicuous prey is. The tournament image is used to fine-tune the predators. The target location drives the fitness function for prey evolution.
\par
The parameters for each simulation run are primarily a choice of background set [... see section QQQ ...], a scale factor for the background images, and a seed for random number generation. There are a few other parameters to the command [... which are listed in the Supplemental Materials ...]. Other key parameters are summarized in Supplemental Materials ZZZ
\par
A typical run consists of 6000 steps with a prey population of 200 and predator population of 20. A preliminary version, reimplementing the interactive approach of \citet{Reynolds2011}, used a prey population of 120 and typical runs of 2000 to 3000 steps. This was also typical for an intermediate version using a single pre-trained predator. The introduction of a population of predators reduced the rate of fine-tuning of predator's neural net models, leading to the longer 6000 step simulations.
\par
During a simulation run various data is collected in log files. The most important output is a “visual log” — periodically saving tournament images (a crop of the given backgrounds overlaid with camouflaged prey, as in Figure \ref{fig:teaser}) along with the predator response data. To “compress” this data, these images are saved “occasionally.” Originally it was every 20 steps, but changed to every 19 steps to be relatively prime to cycling through the 6 or 10 subpopulations (demes/islands) of the prey population.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion QQQ}
This simulation model was applied to a variety of environments — background sets — to produce prey camouflage patterns suited to those environments. See Figure [... QQQ ...] which shows several examples from each of several runs. (Many additional examples, and other details of the development of this model, can be found in the project diary, whose URL is hidden for purposes of double-blind review. [... can I say this? reconsider after response from papersadmin@siggraph.org ...])
\par
[... QQQ ...]
\par
[... QQQ ...]
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limitations QQQ}
[... no objective measure of camouflage effectiveness ...]
[... among other things, makes it hard to evaluate adjustments to the model: is the new parameter value better? can't measure with a numeric value ...]
[... suggest  \cite{lv_cod_2022} as a way to rank conspicuousness ...]
\par
[... See blog entry: \href{https://cwreynolds.github.io/TexSyn/#20220930}{Reliability: what \textit{is} the likelihood of camouflage?} see also comment “Perhaps the common issue is large areas of nearly uniform [but contrasting] colors” in post \href{https://cwreynolds.github.io/TexSyn/#20221119}{Trouble with orange pyracantha}. I next did the elm(?) leaves and as expected that run went fine. Maybe the way to say it is that “easy” backgrounds are those where a correctly chosen uniform color can do a reasonable job of matching the backgrounds. \textbf{Maybe this topic belongs in Discussion?} ...]
\par
[... all results are hand selected, “cherry picked” ...] 
\par
[... in email to Ken I wrote: \textit{The aspect of my project I'm unsure how to approach is lack of rigor. My evaluations are all subjective. It comes down to “we can see that the effectiveness of the camouflage clearly increases during the simulation.”}]
\par
[... inherently 2d ...]
\par
[... texture synthesis lacks genetic or biological plausibility ...]
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Future Work QQQ}
[... see note about future work at post “\href{https://cwreynolds.github.io/TexSyn/#20221106}{Rerun kitchen granite}” ...]
\par
[... propose a crowd sourced user study of camouflage quality ... could be based on time to find ... like the interactive web games of \href{https://www.visual-ecology.com/2020/10/06/martin-stevens/}{Martin Stevens} nuthatch egg? ...] 
\par
[... currently a “mixed paradigm” model using both evolution and learning ... (maybe note these typically co-occur in nature  \cite{valiant_probably_2013}) ... propose an all-evolution model with evolved detectors for predators [... perhaps like the work of \href{https://people.wgtn.ac.nz/Mengjie.Zhang}{Mengjie Zhang} or his students like \href{https://yingbi92.github.io/homepage/}{Ying Bi}? find a representative paper to cite, perhaps coauthored by them ...] or an all learning model, something like CamoGAN \cite{talas_camogan_2020} ...]
\par
An obvious next step is to apply this model to 3d environments. Perhaps along the lines described in \citet{miller_color_2022}. One key simplifying assumption of the current purely 2d model is that plausible, naturally complex, environments are provided by simple photographs taken with the phone in our pocket. Providing plausibly complex 3d environments will be much harder. Perhaps vision techniques like NeRF (e.g. \cite{gao_nerf_2022}) will meet that need.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Acknowledgements
\begin{acks}
I want to express my deep appreciation to all who helped me with this work, starting with my supportive family. Also to: Ken Perlin for — well, lots, but especially — \citet{perlin_image_1985}, Andrew Glassner for teaching me everything I know about deep learning \cite{glassner_deep_2021}, and Pat Hanrahan for some key career advice (“just do the research”).
\par
I've been working on this project on and off since 2007, based on inspirations by papers in the early 1990s (\citet{witkin_reaction_1991}, \citet{turk_generating_1991}, \citet{angeline_competitive_1993}, \citet{sims_artificial_1991}, and \citet{sims_evolving_1994}) and one published a year before I was born: \citet{turing_chemical_1952}.
Also thanks for various help from:
Richard Dawkins,
Steve DiPaola,
Aaron Hertzmann,
John Koza,
Dominic Mallinson,
Nick Porcino,
Michael Wahrman, 
and
QQQ.

% Ken Perlin,
% Aaron Hertzmann,
% Andrew Glassner,
% Pat Hanrahan,
% Karl Sims,
% John Koza,
% Richard Dawkins,
% Witkin/Kass/Turk,
% Peter Angeline,
% Jordan Pollack.
% and of course, Alan Turing. [too weird/maudlin? QQQ]

\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Bibliography.
\bibliographystyle{ACM-Reference-Format}
\bibliography{coc.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Appendix
\newpage
\onecolumn
\appendix
\section{Supplemental Materials}

%% maybe this section should switch to single column layout?
%% maybe this should be in a separate .tex / .pdf file?

%% QQQ ZZZ
\subsection{Key Simulation Parameters}
The fully detailed description of this simulation is the source code [... which is open source on the web at this URL ...]. This table summariezes some of the key parameters which are held constant for all results shown in this paper. [... maybe move this into the main paper if it fits ...]
\par

\begin{adjustwidth}{1cm}{0cm}
    \begin{tabular}{ |l|r| } 
        \hline
        \textbf{Parameter} & \textbf{value} \\ 
        \hline
        predator population & 20 \\ 
        prey population & 200 \\ 
        prey subpopulations (demes) & 10 \\
        prey max init tree size & 100 \\
        prey min tree size after crossover & 50 \\
        prey max tree size after crossover & 150 \\
        \hline
        prey render diameter (pixels) & 100 \\ 
        % tournament output image size & 512² \\ 
        % predator input image size & 128² \\ 
        tournament output image size & 512×512 \\ 
        predator input image size & 128×128 \\ 
        \hline
        simulation steps per run (typical) & 6000 \\
        tournament image save frequency & 19 \\
        \hline
    \end{tabular}
\end{adjustwidth}


\subsection{TexSyn \texttt{c++} code for Figure \ref{fig:TexSyn_overview}}
% Better to use listings package?  https://ctan.org/pkg/listings
\begin{small}
\begin{verbatim}
Uniform white(1);
Uniform gray(0.1);
Uniform blue(0, 0, 1);
Uniform green(0, 1, 0);
LotsOfSpots spots(0.9, 0.05, 0.3, 0.02, 0.02, blue, white);
Grating stripes(Vec2(), green, Vec2(0.1, 0.2), gray, 0.3, 0.5);
NoiseWarp warp_stripes(1, 0.1, 0.7, stripes);
LotsOfSpots spots2(0.9, 0.05, 0.3, 0.02, 0.02, stripes, white);
Grating stripes2(Vec2(), green, Vec2(0.1, 0.2), spots, 0.3, 0.5);
NoiseWarp warp_all(1, 0.1, 0.7, stripes2);
\end{verbatim}
\end{small}

% Note (from https://s2022.siggraph.org/technical-papers-submissions-faq/): 
% The Conference Paper track encourages submissions for high-quality, 
% ground-breaking research that fits within a strict 7-page limit (plus 
% additional pages for references), and may be more appropriate for
% research that is less-polished but still potentially-impactful. Journal 
% papers do not have a page limit, and may include more thorough experiments
% and derivations within the main paper. 

\subsection{Background image sets}
[... list names of background sets used in each figure ... say anything about availability? ...]

\subsection{Additional examples}
[... include some results of other runs, perhaps similar to the format of my blog posts,  ...]

\subsection{Details on pre-trained predator model}
The pre-trained predator model, shown in \ref{fig:predator_cnn}, is a Keras TensorFlow model. Here is its \texttt{model.summary()}:
\begin{adjustwidth}{1cm}{0cm}
\begin{small}
\begin{verbatim}

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 128, 128, 16)      1216
 dropout (Dropout)           (None, 128, 128, 16)      0
 conv2d_1 (Conv2D)           (None, 64, 64, 32)        12832
 dropout_1 (Dropout)         (None, 64, 64, 32)        0
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        51264
 dropout_2 (Dropout)         (None, 32, 32, 64)        0
 conv2d_3 (Conv2D)           (None, 16, 16, 128)       204928
 dropout_3 (Dropout)         (None, 16, 16, 128)       0
 conv2d_4 (Conv2D)           (None, 8, 8, 256)         819456
 dropout_4 (Dropout)         (None, 8, 8, 256)         0
 flatten (Flatten)           (None, 16384)             0
 dense (Dense)               (None, 128)               2097280
 dropout_5 (Dropout)         (None, 128)               0
 dense_1 (Dense)             (None, 32)                4128
 dense_2 (Dense)             (None, 8)                 264
 dense_3 (Dense)             (None, 2)                 18
=================================================================
Total params: 3,191,386
Trainable params: 3,191,386
Non-trainable params: 0
\end{verbatim}
\end{small}
\end{adjustwidth}

\begin{figure*}[b]
    \includegraphics[width=\linewidth]{images/fcd5_samples.png}
    \caption{examples of the three types of labeled training example in the FCD5 training dataset for “find conspicuous disk” pre-training.  [QQQ Temporary figure.]}
    \Description{Full tournament image showing background, three prey, and three predator responses.}
    \label{fig:fcd5_samples}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
