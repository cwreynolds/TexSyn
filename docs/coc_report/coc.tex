% \documentclass[sigconf,anonymous,review,timestamp]{acmart}
\documentclass[sigconf]{acmart}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% prototyping CNN diagram
% \documentclass[sigconf, tikz]{acmart}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Conference title}{January 01--02,
  1900}{Someplace, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers of the event,
%% and this ID should be used as the parameter to this command.
\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
\citestyle{acmauthoryear}

%% To fix a margin violation bug, apparently due to poor hyphenation.
\hyphenation{Go-men-so-ro}
%% For introducing terms which have a special meaning in this work.
\newcommand{\jargon}[1]{\textit{#1}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% prototyping CNN diagram


% \usepackage{import}
% \subimport{./layers/}{init}
% \usetikzlibrary{positioning}

% \def\ConvColor{rgb:yellow,5;red,2.5;white,5}
% \def\ConvReluColor{rgb:yellow,5;red,5;white,5}
% \def\PoolColor{rgb:red,1;black,0.3}
% \def\DcnvColor{rgb:blue,5;green,2.5;white,5}
% \def\SoftmaxColor{rgb:magenta,5;black,7}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{document}

\title{Coevolution of Camouflage}

%% Author
\author{Craig Reynolds}
\email{cwr@red3d.com}
\orcid{0000-0001-8203-712X}
\affiliation{%
  \institution{unaffiliated researcher}
  \country{USA}
}

\renewcommand{\shortauthors}{Craig Reynolds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
  Natural camouflage seems to arise from competition between predators who must find prey to survive, and prey who must avoid detection to survive. This work simulates a simplified, abstract model of this adversarial relationship. It looks at \textit{crypsis} through evolving prey camouflage patterns (as color textures) in competition with evolving predator vision which learns to locate camouflaged prey. The environment for this 2D simulation is provided by a set of photographs, typically of natural scenes. This model is based on an evolving population of predators and another of prey. The mutual conflict between these populations tends to produce both effective camouflage and skilled predators. This simulation provides a method for creating camouflage patterns for arbitrary backgrounds, and more significantly an experimental \textit{artificial life} model for investigating aspects of camouflage evolution in nature. All code and research notes are available on GitHub.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Generate your CCSCML using http://dl.acm.org/ccs.cfm.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010341.10010349.10011810</concept_id>
       <concept_desc>Computing methodologies~Artificial life</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371.10010382.10010384</concept_id>
       <concept_desc>Computing methodologies~Texturing</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224</concept_id>
       <concept_desc>Computing methodologies~Computer vision</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
    <concept>
       <concept_id>10010147.10010178.10010224.10010245.10010246</concept_id>
       <concept_desc>Computing methodologies~Interest point and salient region detections</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
    <concept>
        <concept_id>10010147.10010257.10010293.10011809.10011813</concept_id>
        <concept_desc>Computing methodologies~Genetic programming</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial life}
\ccsdesc[300]{Computing methodologies~Texturing}
\ccsdesc[300]{Computing methodologies~Computer vision}
\ccsdesc[300]{Computing methodologies~Interest point and salient region detections} 
\ccsdesc[300]{Computing methodologies~Genetic programming}

%% Keywords
\keywords{camouflage, coevolution, nature, biology, predator, prey, vision, texture synthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Teaser figure that appears on the top of the article.
\begin{teaserfigure}
    %% TODO note: use images without the “predator prediction crosshairs”.
    \includegraphics[scale=0.24]{images/20220918_step_7372.png}
    \hfill
    \includegraphics[scale=0.24]{images/20220926_step_6143.png}
    \hfill
    \includegraphics[scale=0.24]{images/20221003_step_3667.png}
    \hfill
    \includegraphics[scale=0.24]{images/20220930_step_6093.png}
    \caption{Photographs of natural textures, each overlaid with three camouflaged \textit{prey}. The prey are randomly placed 2D disks, each with its own evolved camouflage texture. (Note: for best results zoom into the digital version. [QQQ replace stand-in images]}
    \Description{Examples of camouflage textures produced by the simulation.}
    \label{fig:teaser}
    % \vspace{5mm} %5mm vertical space
    \vspace{3mm} % 3mm vertical space
\end{teaserfigure}

%% Lay out the single column “top matter” defined above.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[scale=0.16]{images/20221030_1220_step_19.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_1045.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_2014.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_3059.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_6650.png}
    \hfill
    \includegraphics[scale=0.16]{images/20221030_1220_step_7467.png}
    \caption{Evolution of prey camouflage over simulated time}
    \Description{Sequence of images showing evolution of prey camouflage over simulated time.}
    \label{fig:time_sequence}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
This work aims to create a simple, abstract 2d simulation model of camouflage evolution in nature. These camouflage patterns emerge from the interaction — the coevolution — of a population of simulated \textit{prey} each with a candidate texture and a population of simulated \textit{predators} each with a learned visual detector. The input to the simulation is a set of photos of a background environment. The prey evolve to be \textit{cryptic} (hard to see) against the background images. The predators evolve and learn to hunt the prey by locating their position within that 2D environment.
\par
Computational models of complex biological systems have several benefits. Constructing them, getting them to work as seen in nature, helps crystallize our thinking about natural phenomenon. Computational models also allow experimentation \textit{in silico} to help characterize complex natural systems.
\par
Following the approach of \citet{Reynolds2011} a population of prey, each with a synthetic camouflage texture, are evolved in response to negative selection by a predator seeking the prey which are most visually conspicuous against a given background. In that game-like interactive simulation, the predator was a human “player.” The simulation would display a photographic background image overlaid with camouflaged prey. The human predator would select, with a mouse click, the most conspicuous five out of ten displayed prey. These “eaten” prey were removed from the population, and replaced by offspring created by genetic \textit{crossover} between surviving prey followed by \textit{mutation}. This simulation step was repeated around 2000 times.
\par
In the simulation described here, evolution of prey camouflage closely follows that earlier work. But now, the human-in-the-loop is replaced with a second population, of predators, each based on a deep neural network. These CNN networks take an image as input and produce a \jargon{prediction}, an estimate, of where in the image the most conspicuous prey is located. The input to these neural nets is a a small RGB image (a 128x128x3 tensor: 49,152 floating point numbers) and the output is two floats, interpreted as an \textit{xy} position relative to the image. 
\par
(Note that the images in this paper are rendered at 512² pixels and the prey disks have a diameter of 100 pixels. These images are downsampled to 128² for use by predator's vision CNNs.)
\par
A bit of explanation for the term “abstract model.” In artificial life models it is common to focus in detail on one aspect of a natural system. In the current model, that aspect is the coevolutionary dynamics between prey camouflage and predator vision. To make this feasible, other levels of organization, are ignored, or assumed, or represented by a simple computation stand-in. So for example, the entire living organism that embodies these predators and prey, are simply assumed to exist, behaving as animals do, and are otherwise ignored. This model has a simple abstract representation of biological morphogenesis as programs (nested expressions) in TexSyn's language for procedural texture synthesis. \textit{Genetic Programming} provides a simple model of evolution which acts on this “genetic” representation, creating new “offspring” textures through crossover and mutation. On the predator side, we ignore all of the animal's existence except for the key aspect of hunting behavior: looking at a scene and forming an opinion about where in the scene a prey is likely located. These predators can adapt to the appearance of an environment and the prey found there, by learning from its experience. Predators compete with each other on the basis of their ability to hunt prey and so eat. These details of simulated evolution, morphogenesis, vision, and genetic representation are all completely unlike the natural world. The assumption is that they are sufficiently similar in effect to allow a plausible simulation of the natural system, producing analogous results, and so may provide insights about the natural system.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[width=\textwidth]{images/coc_overview.pdf}
    \caption{One step of the coevolutionary simulation of camouflage evolution. Three prey are selected at random from their population of 200. Similarly for three predators from their population of 20. A random background image is selected from the given set, and a random crop of 512×512 pixels. The three prey are rendered to random non-overlapping locations. This composite image is given to each predator which estimates a position in the image, predicting the center point of the most conspicuous prey. The predators are scored by “aim error” — the distance from their estimate to the ground truth center of the nearest prey. If the best predator's estimate is \textit{inside} a prey's disk, that prey is eaten and replaced by a new offspring of the other two prey. If the worst scoring predator finds no prey, it may die of starvation, to be replaced by a new offspring.}
    \Description{Overview diagram of the coevolutionary simulation of camouflage evolution.}
    \label{fig:simulation_overview}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related work}
This work builds on \citet{Reynolds2011} by replacing the human predator there with an evolving population of procedural predators. Those predators \jargon{hunt} using a learning vision model. An earlier attempt at this approach, using coevolution and procedural predators, was described by \citet{harrington_coevolution_2014}.
\par
Closely related work on learning surface textures to camouflage 3D objects within real 3D scenes is described for cubes using one technique in \citet{owens_camouflaging_2014} and for arbitrarily shaped 3D objects using an improved technique in \citet{guo_ganmouflage_2022}. In both cases, the 3D scene is described by a set of photos of it from various viewpoints. The textures mapped onto the object to be camouflaged must trade off being inconspicuous from all viewpoints in the scene.
\par
Other computer graphics work related to camouflage include detailed reproduction of coloration patterns on real animals \cite{de_gomensoro_malheiros_leopard_2020} and generation of visual puzzles incorporating camouflaged images \cite{chu_camo_image_2010} \cite{Zhang_Yin_Nie_Zheng_2020}. CamoEvo \cite{hancock_camoevo_2022} is a toolbox for authoring online camouflage evolution games to understand evolution in real biological species. Being web-based, such simulations can be powered by very large numbers of human volunteers — citizen scientists — an instance of \jargon{human-based computation}.
\par
This adversarial coevolutionary simulation has clear similarities to \jargon{generative adversarial networks} (GANs) originally described in \citet{goodfellow_gan_2014}. Observer-driven optimization of camouflage patterns is a natural application of GANs, as in CamoGAN \cite{talas_camogan_2020}. However the goal of the current work (as suggested in the Future Work section of \citet{Reynolds2011}) is to produce a simulation of a natural system, suitable for “what if” experiments which cannot be performed in the natural world. [... QQQ see several related thoughts in Notes app ...]
\par
The procedural texture synthesis used here to generate camouflage patterns has a long history. This work is perhaps most directly inspired by \citet{perlin_image_1985} where images are rendered from purely procedural representation. [... maybe this should be in the section on texture synthesis? ... These textures are object-oriented so specific types are implemented as (c++) classes, with specific parameters stored in instances. The base class provides interfaces for operations such as returning a color for a given position on the (infinite) texture plane. ...] The combination of texture synthesis under control of a genetic algorithm goes back to \citet{sims_artificial_1991}, which in turn was inspired by the interactive biomorph evolution demo in \citet{dawkins_blind_1986}. [... Latham and Todd ...]
\par
Evolution is represented in this model using \jargon{genetic programming} (GP), a type of population-based evolutionary optimization algorithm, first described by \citet{cramer_representation_1985} and popularized by \citet{koza_genetic_1992}. GP is a variation of \jargon{genetic algorithms} (GA). GA traditionally uses a fixed length bit string as its genetic representation, while GP uses an arbitrarily-sized tree-shaped representation. GP trees conveniently map onto nested expressions in a domain specific language. Texture synthesis in this work is based on nested expressions of texture operators from the TexSyn library, see Figure \ref{fig:TexSyn_overview}. A prey population of these textures is optimized for camouflage effectiveness by GP using the selection pressure of a population of predators which serve to determine fitness. TexSyn is used with the \jargon{strongly typed} variant of Genetic Programming known as STGP \cite{montana_strongly_1995}, one of several grammar-based GP variants \cite{Mckay_2010}.
\par
The biological literature on camouflage and related topics is vast. A few starting points include: [... historical survey of campoouflage in nature ... \citet{thayer_concealing-coloration_1909}, early work on mathematical models of biological patterns \citet{turing_chemical_1952}, revisiting Turing's work with modern computation ... \citet{murray_how_1988},  ...how life evolves and learns... \citet{valiant_probably_2013}, QQQ find other basic bio references from my 2010 paper? ...]
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Camouflaged object detection}
In the last several years, there has been burst of related research on \jargon{camouflaged object detection} (COD) in computer vision. See for example this well-curated list of COD publications by \citet{visionxiang_cod}. COD is part of predator behavior: detecting the presence of a camouflaged object. But these COD systems focus on \jargon{segmenting} the camouflaged object: finding a per-pixel mask of its projection in an image. A recent example, which surveys this topic and presents its own strong solution, is \citet{Zhang2022}. Others works on COD include some based on boundaries \cite{chen_boundary-guided_2022} \cite{sun_boundary-guided_2022}, an attempt to rank camouflaged objects by “conspicuousness” \cite{lv_cod_2022}, and a mixed-scale approach \cite{pang_zoom_2022}.
\par
COD attempts \textit{a priori} camouflage “breaking” — detecting the presence of well camouflaged objects — without learning either the background or the typical appearance of prey camouflage found in a given environment. That is, they seek to be a \jargon{generalist} predator, effectively using a strong form of \jargon{salience}. As summarized in \citet{Zhang2022}, COD is based on several labeled datasets (CHAMELEON, CAMO, and COD10K) carefully annotated by hand at the pixel level. Incremental progress in this area is measured by the per-pixel accuracy of predicted shapes of camouflaged objects.
\par
In contrast, the goal of the simulation reported in this paper is to pit camouflage evolution against vision-based hunting. In that context, determining the exact (pixel level) shape of the prey is largely irrelevant, especially small differences, as between a mask that is for example, 93\% correct versus 96\% correct. The simulation described here ignores segmentation, abstracting prey as a disk of constant size, so it is sufficiently characterized by its center position. A real world predator can aim its attack at a prey's center without an exact segmentation. This work simulates predators learning to find prey despite evolving camouflage patterns. Thus this model \jargon{adapts} to dynamic camouflage rather than approach COD as a static task of generalist detection. Significantly, this work requires no hand-labeled datasets at all, using a form of \jargon{self-supervision}.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[width=\textwidth]{images/texsyn_overview.pdf}
    \caption{TexSyn expression trees and crossover between them, using a simplified version of TexSyn with three texture operators (\texttt{spots}, \texttt{stripes}, and \texttt{warp}) plus four solid color textures. Minimal operator trees are shown in (a) and (b). \textit{Crossover} between (a) and (b) is shown in (c) and (d). (c) is \texttt{spots} where \texttt{blue} is replaced with \texttt{stripes}. (d) is \texttt{stripes} where \texttt{gray} is replaced with \texttt{spots}. (e) and (f) show (c) and (d) under a \texttt{warp} operator. (See Supplemental Material for actual TexSyn c++ code used for these.)}
    \Description{Overview of TexSyn representation of texture and the idea of \textit{crossover} between expression trees.}
    \label{fig:TexSyn_overview}
\end{figure*}

%% maybe more detail about GP and TexSyn in appendix? I think that does not add to page count

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Components of the simulation}
\subsection{Coevolution, populations, and fitness}
This camouflage simulation is based on two adversarial \jargon{populations}: one of \jargon{predators} and one of \jargon{prey}. Individual prey compete for survival within their own population, similarly for predators. Predators must \jargon{hunt} successfully to “eat” and so survive. Prey survive if inconspicuous (“cryptic”) enough to avoid being found and eaten. A prey hunted and eaten, or a predator perished from hunger, is removed from its population. It is replaced by an \jargon{offspring} of parents from the surviving population.
\par
Predators define the fitness of prey: being easy to spot is bad, blending in is good. Similarly prey define the fitness of predators: being fooled by camouflage is bad, spotting cryptic prey is good. From this adversarial interaction, the two populations \jargon{coevolve}. If one side has some sort of flaw, the other side has a motivation to exploit it. As a result both sides tend to improve over simulation time.
\par
Initial random prey have coloration likely to contrast with the background. Initial predators have a “pre-trained” ability to find conspicuous objects which may allow them to hunt these initial un-camouflaged prey. As coevolution proceeds, prey become better camouflaged against the given background images. In parallel, predators become more attuned to hunting these prey on the given backgrounds.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tournaments, competition, relative fitness}
\label{subsec:tournaments}
It is common in \jargon{evolutionary computation} to define \jargon{fitness} as a function that maps an \jargon{individual} (of the evolving population) into a number. That is, the function somehow evaluates the individual and assigns it a score. Typically this fitness function is \jargon{idempotent}: its value depends only on properties of the given individual. This can be seen as \jargon{absolute fitness}.
\par
In contrast, this simulation uses \jargon{relative fitness}, determined by \jargon{competition} between individuals, in the context of \jargon{tournaments}. A tournament is a game — a contest — between multiple individuals. 
\par
A simple example of relative fitness is a foot race. The winner of the race is the first to cross the finish line. The order of finishing \jargon{sorts} the racers by speed. Races have been run this way since ancient times. Today it is simple to measure each runner's speed but it is not required to determine who won the race. Now consider two people playing chess. We do not know how to measure or predicts a player's skill. But by pitting them against each other, having them play a game (or a series of games) the results provide a useful measurement of relative fitness. 
\par
Throughout this model, \textbf{tournaments involve three individuals}. (This contrasts with the work in \citet{Reynolds2011} which used tournaments of 10.) So in overview, one simulation step consists of drawing three prey individuals out of the population to compete in a tournament. Like the foot race, or chess game, the tournament serves to sort the individual according to relative fitness, without requiring assigning absolute fitness. That relative fitness is provided by the action of adversarial predators. During the same simulation step, three predators are drawn from their population. The predators look at the same input: an image with three camouflaged prey overlaid on a background image. The predators compete with each other for accurate hunting of the prey. The prey compete with each other by hiding (avoiding being targeted) by the predators.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Negative selection, drift, and mixability}

A modern synthesis \cite{livnat_sex_2016} of evolution theory, game theory, and machine learning (“no-regret learning” — the multiplicative weights update algorithm) suggests evolution may act to optimize \jargon{mixability} — a type of modularity in the genetic representation of phenotypes. As described in \citet{chastain_multiplicative_2013} this perspective focuses on the “...special case of weak selection in which all fitness values are assumed to be close to one another. ...hypothesizing that evolution proceeds for the most part not by substantial increases in fitness but by essentially random drift...” This concept goes back to the “Neutral Theory” in \citet{kimura_evolutionary_1968}. Another helpful perspective on the interaction of evolution and learning can be found in \citet{valiant_probably_2013}. [... out of place here? QQQ]
\par
The evolutionary model used in the current work, implemented in software called \jargon{LazyPredator,} attempts to operate in this “neutral” mode. Genetic algorithms often seek to promote the population's highest fitness individuals. They allow these high fitness individuals to survive longer and reproduce more often. This skews the evolutionary search: too much \jargon{exploiting} without enough \jargon{exploring.}  In contrast, LazyPredator uses \textit{negative selection} to encourage \jargon{drift}. Higher fitness individuals experience almost no difference in  relative fitness. It is the lowest fitness individuals that get culled by predation. In each tournament (see section \ref{subsec:tournaments}) the goal is to sort the three individuals by relative fitness. In fact, the only requirement is to identify the \textbf{least fit} of the three individuals.
\par 
For example, a predator looks at a scene then predicts the location of the most conspicuous of the three prey. If this location is within the disk-shaped “body” of a prey, it is the \textbf{least} fit of the tournament and gets “eaten.” The relative fitness ordering of the other two prey is not significant (it is arbitrary). If the predator \jargon{fails} and predicts a position outside all three prey, then the entire simulation step is abandoned, no prey is eaten, and the prey population is unchanged.
\par
In the predator population, a tournament is ranked by the \textbf{distance from a predator's prediction to the center of the nearest prey} — essentially the predator's “aiming error.” The worst predator may then die from \jargon{starvation} based on its recent history of hunting success: has it eaten enough to survive? (Currently this is 35\% hunting success over its last 20 attempts.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Offspring, crossover, and mutation}
In the prey population, a tournament is used to find the lowest relative fitness. This corresponds to the worst camouflage, the \jargon{most conspicuous} of three prey in a tournament. If a predator successfully locates a prey, we assume it is captured and eaten. The object representing that departed prey is removed from its population and replaced with a new individual. This is the \jargon{population update} stage of a steady-state genetic programming system.
\par
The new prey, which replaces the one killed by predator, is called the \jargon{offspring} of two other prey. This is the motivation for using tournaments of size 3: the least fit prey dies, and is replaced by the offspring of the other two prey. We don't know which of them has higher relative fitness, but either way they become the \jargon{parents} given as input to the \jargon{crossover} operation.
\par
In GP, individuals are represented as tree structures. (Which in this work are interpreted as TexSyn programs as described in the next subsection.) The crossover operation is defined on two abstract trees. The two parents are first copied to preserve them. One copy is chosen as recipient and one as donor. In each a “random subtree” is selected. The recipient's random subtree is replaced by the donor's random subtree, by splicing the pointers between tree nodes. This is shown in Figure \ref{fig:TexSyn_overview}.
\par
After crossover, a mutation operator further modifies the offspring tree. It basically traverses the tree, finding all the leaf nodes, which here correspond to numerical constants in the texture programs. Because LazyPredator uses \jargon{strongly typed genetic programming} \cite{montana_strongly_1995} each leaf belongs to a specific user defined type (for example a floating point number between 0 and 1). A method of the type's class can mutate (“jiggle”) the constant value of a leaf node (for example, add a small random offset, then clip into range). After crossover and mutation the new offspring is inserted into the population, replacing the dead prey.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Texture synthesis}
Textures are represented in this simulation as trees of procedural texture operators. These correspond directly to nested expressions in a typical programming language. \jargon{TexSyn} is a simple domain specific language for describing textures.
\par
The details of TexSyn are not central to understanding this camouflage model. A quick overview is given here. TexSyn is library, an API, with many \jargon{operators} each of which return a \jargon{Texture}. Almost all of them also take Textures as input parameters, along with simple values like colors, 2d vectors, and floating point numbers. Writing nested functional expressions of operators corresponds to trees of operator instances.
\par
These TexSyn style textures are represented as operators, trees, and parameters. They do not store pixel data, providing instead a function to sample the texture's color at an arbitrary floating point \textit{xy} location. This is similar to GPU \jargon{fragment shaders} and the Pixel Stream Editor functions in \cite{perlin_image_1985}.
\par
[... see Figure \ref{fig:TexSyn_overview} ...]
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% prototyping CNN diagram

\begin{figure}
    \includegraphics[width=\columnwidth]{images/predator_cnn.pdf}
    \caption{Architecture of a predator's deep neural net which maps a 128² RGB image into an \textit{xy} location within the image where it predicts the most conspicuous prey is centered.}
    \Description{Diagram of Predator's convolutional neural net.}
    \label{fig:predator_cnn}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Full step/tournament image with predator responses

\begin{figure}
    \includegraphics[width=\columnwidth]{images/20221007_0806_step_7030.png}
    \caption{Tournament image after simulation step. Starts with crop from given background set. Composited over that are three prey with evolved camouflage patterns. Over that are drawn three \jargon{crosshair} marks showing the responses of three predators. These are ranked by minimum distance to a prey center. Best is in black and white, second best is green and black, third is red and black. Here, only the top ranked predator chose a position inside a prey's disk, the other two failed. Choice of prey, predators, background image, crop, and prey placement — are all uniform random selection.}
    \Description{Full tournament image showing background, three prey, and three predator responses.}
    \label{fig:predator_responses}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predator vision}
[... some of this is covered in the Introduction, should any of that be moved here, or is the duplication OK? ...]
\par
In this simulation it is a predator's job to look at an image and “hunt” for prey. Specifically an image is built from a portion of a background photo and overlaid with three camouflaged prey. The camouflage texture for each disk shaped prey is rendered on the TexSyn side. Because all images in this simulation are synthetic, they are inherently labeled with \jargon{ground truth} position data for each prey. This allows the predators to learn in a \jargon{self-supervised} manner.
\par

\subsubsection{Pre-train to Find Conspicuous Disk}
The basis for a predator's visual system is a \jargon{pre-trained} neural net model. This deep neural net model was created before camouflage evolution was attempted. TexSyn was used to generate a large dataset of [... QQQ lookup size ...] labeled training examples. Each example was an RGB image, a 128×128×3 tensor, with an associated label in the form of an \textit{xy} pair of indicating a location in the image in normalized coordinates. [... QQQ oh, wait was this in pixel coordinates? ...]
\par
Each image was a random portion cropped out of one of the background images, chosen at random. Over it was rendered one to three prey disks with \jargon{random texture}. While a random texture is a slippery concept, the meaning here is the sort of prey texture used to initialize the population before an evolution run. The LazyPredator genetic programming engine has facilities for creating random trees of a given size from a user-defined \jargon{function set} such as the TexSyn library. So the random tree corresponds to a random nested expression of TexSyn operators with randomly chosen leaf constants. As such these prey textures are quite varied, but have a fair amount of structure, they are not just independently chosen random pixel values.

\subsubsection{Fine tune during evolution run}
[... see Figure \ref{fig:predator_cnn} ...]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
[... ...]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limitations}
[... no objective measure of camouflage effectiveness ...]
[... among other things, makes it hard to evaluate adjustments to the model: is the new parameter value better? can't measure with a numeric value ...]
[... suggest  \cite{lv_cod_2022} as a way to rank conspicuousness ...]
\par
[... all results are hand selected, “cherry picked” ...] 
\par
[... in email to Ken I wrote: \textit{The aspect of my project I'm unsure how to approach is lack of rigor. My evaluations are all subjective. It comes down to “we can see that the effectiveness of the camouflage clearly increases during the simulation.”}]
\par
[... inherently 2d ...]
\par
[... texture synthesis lacks genetic or biological plausibility ...]
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Future Work}
[... see note about future work at post “\href{https://cwreynolds.github.io/TexSyn/#20221106}{Rerun kitchen granite}” ...]
\par
[... propose a crowd sourced user study of camouflage quality ... could be based on time to find ... like the interactive web games of \href{https://www.visual-ecology.com/2020/10/06/martin-stevens/}{Martin Stevens} nuthatch egg? ...] 
\par
[... currently a “mixed paradigm” model using both evolution and learning ... (maybe note these typically co-occur in nature  \cite{valiant_probably_2013}) ... propose an all-evolution model with evolved detectors for predators [... perhaps like the work of \href{https://people.wgtn.ac.nz/Mengjie.Zhang}{Mengjie Zhang} or his students like \href{https://yingbi92.github.io/homepage/}{Ying Bi}? find a representative paper to cite, perhaps coauthored by them ...] or an all learning model, something like CamoGAN \cite{talas_camogan_2020} ...]
\par


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Acknowledgements
\begin{acks}
[... Many thanks to all who helped me with this work: my supportive family, Andrew Glassner for teaching me everything I know about deep learning \cite{glassner_deep_2021}, Ken Perlin for, lots, but especially \cite{perlin_image_1985}. Pat Hanrahan for helpful career advice (“just do the research”). ...]
\par
[... I've been working on this on and off since 2007, based on inspirations by papers in the early 1990s (Witkin/Kass, Turk, Angeline/Pollack, Sims) ...]
\par
[maybe just names (roughly backward in time): 
Ken Perlin,
Aaron Hertzmann,
Andrew Glassner,
Pat Hanrahan,
Karl Sims,
John Koza,
Richard Dawkins,
Witkin/Kass/Turk,
Peter Angeline,
Jordan Pollack.
and of course, Alan Turing. [too weird/maudlin? QQQ]
]
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Bibliography.
\bibliographystyle{ACM-Reference-Format}
\bibliography{coc.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Appendix
\newpage
\appendix
\section{Supplemental Materials}

%% maybe this section should switch to single column layout?
%% maybe this should be in a separate .tex / .pdf file?

\subsection{TexSyn \texttt{c++} code for Figure \ref{fig:TexSyn_overview}}
% Better to use listings package?  https://ctan.org/pkg/listings
\begin{small}
\begin{verbatim}
Uniform white(1);
Uniform gray(0.1);
Uniform blue(0, 0, 1);
Uniform green(0, 1, 0);
LotsOfSpots spots(0.9, 0.05, 0.3, 0.02, 0.02, blue, white);
Grating stripes(Vec2(), green, Vec2(0.1, 0.2), gray, 0.3, 0.5);
NoiseWarp warp_stripes(1, 0.1, 0.7, stripes);
LotsOfSpots spots2(0.9, 0.05, 0.3, 0.02, 0.02, stripes, white);
Grating stripes2(Vec2(), green, Vec2(.1, .2), spots, 0.3, 0.5);
NoiseWarp warp_all(1, 0.1, 0.7, stripes2);
\end{verbatim}
\end{small}

% Note (from https://s2022.siggraph.org/technical-papers-submissions-faq/): 
% The Conference Paper track encourages submissions for high-quality, 
% ground-breaking research that fits within a strict 7-page limit (plus 
% additional pages for references), and may be more appropriate for
% research that is less-polished but still potentially-impactful. Journal 
% papers do not have a page limit, and may include more thorough experiments
% and derivations within the main paper. 

\subsection{Background image sets}
[... list names of background sets used in each figure ... say anything about availability? ...]

\subsection{Additional examples}
[... include some results of other runs, perhaps similar to the format of my blog posts,  ...]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
