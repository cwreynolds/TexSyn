%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Coevolution of Camouflage
% Submission for ALIFE 2023
%
% March 12, 2023 submit draft for review.
% April 21, 2023 convert to non-anonymous draft to post on arXiv.
%                So edit author info, uncomment Acknowledgments.
%                Convert pkgT, pkgE, pkgV to real names, add citations.
% April 22, 2023 That worked in Overleaf, but caused problems when I tried
%                to upload to arXiv. Now changed (the obviously wrong) 
%                \bibliographystyle{ACM-Reference-Format} to 
%                \bibliographystyle{plainnat}
% May 17, 2023   Began editing for final ALIFE version. Then realized that
%                my citations in the body of the paper are wrong: most
%                obviously they use brackets [] instead of parentheses.
%                Found that problem was using "plainnat" instead of
%                "apalike" in \bibliographystyle

\documentclass[letterpaper]{article}
\usepackage{natbib,alifeconf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% To fix a margin violation bug, apparently due to poor hyphenation.
\hyphenation{Go-men-so-ro}

% Allows expressions of measurements. (Included in The ACM Publishing System (TAPS) List of Accepted LaTeX Packages)
\usepackage{calc}

% Adding for ALife, maybe previously loaded by ACM style?
\usepackage[hyphens]{xurl}
\usepackage{hyperref}
\usepackage{tabularx}

% added 20230421 to allow SIGGRAPH-style “teaser figure” under title.
\usepackage{authblk}
\usepackage{titlepic}
\usepackage{caption}
\usepackage{float}
\usepackage[T1]{fontenc} % ??? QQQ -- "<"

%% For introducing terms which have a special meaning in this work.
\newcommand{\jargon}[1]{\textit{#1}}

%% Stand-ins for software package names, anonymized for review.
% \newcommand{\texsyn}[0]{pkgT}
% \newcommand{\lazypredator}[0]{pkgE}
% \newcommand{\predatoreye}[0]{pkgV}
\newcommand{\texsyn}[0]{TexSyn}
\newcommand{\lazypredator}[0]{LazyPredator}
\newcommand{\predatoreye}[0]{PredatorEye}

%% Use like: {\runID backyard\_oak\_20230113\_2254}
\newcommand{\runID}{\footnotesize}

%% for laying out a row of 4, 6, or 9 images
\newcommand{\igfour}[1]{\includegraphics[width=0.24\linewidth]{#1}}
\newcommand{\igsix}[1]{\includegraphics[width=0.16\linewidth]{#1}}
\newcommand{\ignine}[1]{\includegraphics[width=0.105\linewidth]{#1}}

% small fixed-width font
\newcommand{\stt}[1]{{\small \texttt{#1}}}

\graphicspath{ {images/} {images/fcd5/} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Coevolution of Camouflage}
\author{Craig Reynolds\authorcr
    unaffiliated researcher\authorcr 
    cwr@red3d.com}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\captionsetup{hypcap=false}

\titlepic{\igfour{20221121_1819_step_6464.png} \hfill \igfour{20221108_2018_step_6562.png} \hfill\igfour{20221215_step_7182.png}\hfill\igfour{20221216_step_5997.png} \captionof{figure}{Photographs of natural textures, each overlaid with three camouflaged \textit{prey}. The prey are randomly placed 2D disks, each with its own evolved camouflage texture. (Background photos of: plum leaf litter, tree and sky, gravel, oxalis sprouts. Zoom in for detail. Disk diameter is 20\% of image width. See \href{https://arxiv.org/abs/2304.11793}{preprint} with full resolution images and Supplemental Materials.} \label{fig:teaser}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Remove today's date being inserted after the title/author information.
\date{}

%% Lay out the single column “top matter” defined above.
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
    Camouflage in nature seems to arise from competition between predator and prey. To survive, predators must find prey, and prey must avoid being found. This work simulates an abstract model of that adversarial relationship. It looks at \textit{crypsis} through evolving prey camouflage patterns (as color textures) in competition with evolving predator vision. During their “lifetime” predators learn to better locate camouflaged prey. The environment for this 2D simulation is provided by a set of photographs, typically of natural scenes. This model is based on two evolving populations, one of prey and another of predators. Mutual conflict between these populations can produce both effective prey camouflage and predators skilled at “breaking” camouflage. The result is an open source \textit{artificial life} model to help study camouflage in nature, and the perceptual phenomenon of camouflage more generally.
\end{abstract}

% \keywords{camouflage, coevolution, nature, biology, predator, prey, vision, learning, texture synthesis, simulation, competition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \igsix{20221030_1220_step_19.png}
    \hfill
    \igsix{20221030_1220_step_1045.png}
    \hfill
    \igsix{20221030_1220_step_2014.png}
    \hfill
    \igsix{20221030_1220_step_3059.png}
    \hfill
    \igsix{20221030_1220_step_6650.png}
    \hfill
    \igsix{20221030_1220_step_7467.png}
    \caption{Prey camouflage evolving over simulation time to become more effective in a given environment.}
    % \Description{Sequence of images showing evolution of prey camouflage over simulated time.}
    \label{fig:time_sequence}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
This work aims to create a simple abstract 2d simulation model of camouflage evolution in nature. These simulated camouflage patterns emerge from the interaction, the coevolution, of a population of simulated \jargon{prey} each with a candidate texture and a population of simulated \jargon{predators} each with a learning visual detector. The simulation's main input is a set of photos of a background environment. Prey evolve to be \jargon{cryptic} (hard to find) against the background. Evolving predators learn to hunt the prey by locating their position within that 2D environment.
\par
Computational models of complex biological systems have several benefits. Constructing them, making them work as observed in nature, helps crystallize our thinking about natural phenomena. Computational models also allow experimentation \textit{in silico} to help understand complex natural systems.
\par
This work follows the approach of \citet{reynolds_iec_2011} where a population of camouflaged prey evolve in response to negative selection from a predator seeking conspicuous prey. In that earlier interactive game-like simulation, the predator was a human “player.” That simulation displayed a photographic background image overlaid with camouflaged prey. With mouse clicks, the human predator selects the most conspicuous five out of ten displayed prey. Those prey, “eaten” by the predator, were removed from the population.  They were replaced by \jargon{offspring} created with genetic \jargon{crossover} between surviving prey (as \jargon{parents}) followed by \jargon{mutation}. That simulation step was repeated about 2000 times.
\par
Here, evolution of prey camouflage closely follows that earlier work. However the human-in-the-loop is replaced with a second population of predators, each based on a deep neural network. These CNN networks take an image as input and produce a \jargon{prediction}, an estimate, of where in the image the most conspicuous prey is located.
\par
In abstract artificial life models, it is common to focus in detail on one aspect of a natural system. In the current model, that aspect is the coevolutionary dynamics between prey camouflage and predator vision. To make this feasible, other levels of organization, are ignored, or assumed, or represented by a simple computational stand-in. So for example, the entire living organism that embodies these predators and prey, are simply assumed to exist, behaving as animals do, and are otherwise ignored. This model has a simple abstract representation of biological morphogenesis as programs (nested expressions) in \texsyn{}'s language for procedural texture synthesis \citep{reynolds_texsyn_2019}. \jargon{Genetic Programming} provides a simple model of evolution which acts on this “genetic” representation, creating new “offspring” textures through crossover and mutation. On the predator side, all of the animal's existence is ignored except for the key aspect of hunting behavior: looking at a scene and forming an opinion about where in the scene a prey is likely located. These predators can adapt to the appearance of an environment and the prey found there, by learning from its experience. Predators compete with each other on the basis of their ability to hunt prey and so eat. These details of simulated evolution, morphogenesis, vision, and genetic representation are all quite unlike the natural world. But they appear to be sufficiently similar in their effect to allow a plausible simulation of the natural system, producing analogous results, and so may provide insights about the natural system.
\par
To help ground this abstract simulation, consider a bird (predator), hunting for tasty but camouflaged beetles (prey), seen against the bark of a tree trunk (background image).
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[width=\textwidth]{coc_overview.pdf}
    \caption{Overview of one step of the coevolutionary simulation of camouflage. Three prey are selected at random from their population of 400. Similarly for three predators from their population of 40. A random background image is selected from the given set, and a random crop of 512² pixels is made. The three prey are rendered to random non-overlapping locations. This composite image is given to each predator which estimates a position (circled crosshairs in tournament image, see also Figure \ref{fig:predator_responses}) predicting the center point of the most conspicuous prey. The predators are scored by “aim error” — the distance from their estimate to the \jargon{ground truth} center of the nearest prey. If the best predator's estimate is \textit{inside} a prey's disk, that prey is eaten and replaced by a new offspring of the other two prey. If all predators fail, all prey survive. If the worst scoring predator's estimate is \textit{outside} all prey, it may die of starvation, to be replaced by a new offspring.}
    % \Description{Overview diagram of the coevolutionary simulation of camouflage evolution.}
    \label{fig:simulation_overview}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
This work builds on \citet{reynolds_iec_2011} by replacing the human predator there with an evolving population of procedural predators, which \jargon{hunt} using a learning vision model. \citet{harrington_coevolution_2014} also used coevolution between prey and predators to create camouflage. Their predators detected prey with a multiscale convolution filter whose weights were evolved with a genetic algorithm.
\par
Closely related work on learning surface textures to camouflage 3D objects within real 3D scenes is described in \citet{owens_camouflaging_2014} for cubes, using one technique, and in \citet{guo_ganmouflage_2022}, for arbitrarily shaped 3D objects, using an improved technique. In both cases, the 3D scene is described by a set of photos from various viewpoints. The camouflage textures mapped onto these objects must trade off being inconspicuous from all viewpoints in the scene.
\par
Other computer graphics work related to camouflage include meticulously detailed reproduction of coloration patterns on real animals \citep{de_gomensoro_malheiros_leopard_2020} and generation of visual puzzles incorporating camouflaged images \citep{chu_camo_image_2010}, \citep{Zhang_Yin_Nie_Zheng_2020}. CamoEvo \citep{hancock_camoevo_2022} is a toolbox for authoring online camouflage evolution games to understand evolution in real biological species. Such web-based simulations can be powered by very large numbers of human volunteers (“citizen scientists”) engaged in \jargon{human-based computation}.
\par
This adversarial coevolutionary simulation has clear similarities to \jargon{generative adversarial networks} (GANs) originally described in \citet{goodfellow_gan_2014}. Observer-driven optimization of camouflage patterns is an ideal application of GANs, as in CamoGAN \citep{talas_camogan_2020}. However the goal of the current work (as suggested in the Future Work section of \citet{reynolds_iec_2011}) is not \textbf{learning} camouflage, but to produce a simulation of a biological evolutionary system, suitable for “what if” experiments which cannot be performed in the natural world, e.g.: how does evolved camouflage change as the ratio of predators to prey changes?
\par
The procedural texture synthesis used here to generate camouflage patterns has a long history. This work is perhaps most directly inspired by \citet{perlin_image_1985} where images are rendered from purely procedural representation of 3D textures. A recent example of this approach is found in \citet{Guerrero_MatFormer_2022}. Using texture synthesis under the control of a genetic algorithm goes back to \citet{sims_artificial_1991}, which in turn was inspired by the interactive biomorph evolution demo in \citet{dawkins_blind_1986}. FormSynth \citep{latham_form_1989} inspired Mutator \citep{todd_evolutionary_1994} and other tools.
\par
Evolution is represented in this model using \jargon{genetic programming} (GP), a population-based evolutionary optimization algorithm. It was first described by \citet{cramer_representation_1985} and popularized by \citet{koza_genetic_1992}. GP is a variation of \jargon{genetic algorithms} (GA). GA traditionally use a fixed length bit string as its genetic representation, while GP uses an arbitrarily-sized tree-shaped representation. GP trees conveniently map onto nested expressions in a domain specific language. Texture synthesis in this work is based on nested expressions of texture operators from the \texsyn{} library, see Figure \ref{fig:TexSyn_overview}. A prey population of these textures is optimized for camouflage effectiveness by GP using the selection pressure from a population of predators which determine fitness. \texsyn{} is used with the \jargon{strongly typed} variant of Genetic Programming known as STGP \citep{montana_strongly_1995}, one of several grammar-based GP variants \citep{Mckay_2010}. The GP implementation used here is called \lazypredator{} \citep{reynolds_lazypredator_2020}.
\par
The biological literature on camouflage and related topics is vast. A few starting points include: an early survey of camouflage in nature \citep{thayer_concealing-coloration_1909}, pioneering work on mathematical models of biological patterns \citep{turing_chemical_1952}, revisiting Turing's work with improved computation \citep{murray_how_1988}, and a modern perspective on how life evolves and learns \citep{valiant_probably_2013}. A recent survey of how camouflage increases survival \citep{de_alcantara_viana_predator_2022} reiterates how it need not be perfect. A small change, increasing a predator's search time, or reducing its attack rate, is enough to drive camouflage evolution.
\par


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Camouflaged Object Detection (COD)}
The last several years has seen a surge of computer vision research on \jargon{camouflaged object detection}, a part of predator behavior. (See this well-curated publication list: \citet{visionxiang_cod}.) COD systems seek to \jargon{segment} camouflaged objects in images: identifying the pixels they cover. A recent example surveys this topic and presents a strong solution: \citet{Zhang2022}. Other research on COD include some based on boundaries \citep{chen_boundary-guided_2022}, \citep{sun_boundary-guided_2022}, a mixed-scale approach \citep{pang_zoom_2022}, one using transformer architecture \citep{yin_camoformer_2022}, and an attempt to rank camouflaged objects by “conspicuousness” \citep{lv_cod_2022}.
\par
COD attempts \textit{a priori} camouflage “breaking” — detecting the presence of well camouflaged objects — without learning either the background or typical appearance of prey camouflage found in a given environment. That is, COD is a \jargon{generalist} predator, effectively using a form of \jargon{salience}. As summarized in \citet{Zhang2022}, COD is based on several labeled datasets (CHAMELEON, CAMO, and COD10K) carefully annotated by hand at the pixel level.
\par
In contrast, the goal of the simulation reported in this paper is to pit camouflage evolution against vision-based hunting. So determining the exact (pixel level) shape of the prey is largely irrelevant. This simulation ignores segmentation, abstracting prey as a disk of constant size, so sufficiently characterized by its center position. A real world predator can aim its attack at a prey's center without an exact segmentation. This work needs to find the \textbf{most} conspicuous prey, not all prey. This work simulates predators learning to find prey despite evolving camouflage patterns. Thus this model \jargon{adapts} to dynamic camouflage rather than approach COD as a static task of generalist detection. Significantly, this work requires no hand-labeled training datasets at all, since it uses a form of \jargon{self-supervision}.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \includegraphics[width=\textwidth]{texsyn_overview.pdf}
    \caption{\texsyn{} expression trees and crossover between them, illustrated here with a simplified version of \texsyn{} with just three texture operators (\stt{spots}, \stt{stripes}, and \stt{warp}) plus four named solid color textures. Minimal operator trees are shown in (a) and (b). \textit{Crossover} between (a) and (b) is shown in (c) and (d). (c) is \stt{spots} where \stt{blue} is replaced with \stt{stripes}. (d) is \stt{stripes} where \stt{gray} is replaced with \stt{spots}. (e) and (f) show (c) and (d) under a \texttt{warp} operator. (See Supplemental Material for actual \texsyn{} c++ code used for these examples.)}
    % \Description{Overview of \texsyn{} representation of texture and the idea of \textit{crossover} between expression trees.}
    \label{fig:TexSyn_overview}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Components of the Simulation}
\subsection{Coevolution, Populations, and Fitness}
This camouflage simulation is based on two adversarial \jargon{populations}: one of \jargon{predators} and one of \jargon{prey}. Individual prey compete for survival within their own population and similarly for predators. Predators must \jargon{hunt} successfully to “eat” and so survive. Prey survive if they are inconspicuous (cryptic) enough to avoid being found and eaten. A prey eaten by a predator, or a predator perished from hunger, is removed from its population. It is replaced by an \jargon{offspring} of parents from the surviving population.
\par
Predators define the fitness of prey: being easy to spot is bad, blending in is good. Similarly prey define the fitness of predators: being fooled by camouflage is bad, spotting cryptic prey is good. From this adversarial interaction, the two populations \jargon{coevolve}. If one side has some sort of flaw, the other side is motivated to exploit it. As a result both sides tend to improve over simulated time.
\par
Initial random prey have coloration likely to contrast with the background. Initial predators have a \jargon{pre-trained} ability to find conspicuous (salient) objects which often allow them to hunt these initially un-camouflaged prey. As coevolution proceeds, prey become better camouflaged against the given background images. In response, predators learn to better hunt these prey on those backgrounds.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tournaments, Competition, Relative Fitness}
\label{subsec:tournaments}
It is common in \jargon{evolutionary computation} to define \jargon{fitness} as a function that maps an \jargon{individual} (member of the evolving population) into a number. That is, the function somehow evaluates the individual and assigns it a score. Typically this fitness function is \jargon{idempotent}: its value depends only on static properties of the given individual. This can be seen as \jargon{absolute fitness}.
\par
In contrast, this simulation uses \jargon{relative fitness}, determined by \jargon{competition} between individuals, in \jargon{tournaments}. A tournament is a contest between multiple individuals. 
\par
A simple example of relative fitness is a foot race. The winner of the race is the first to cross the finish line. The order of finishing \jargon{sorts} the racers by speed. Races have been run this way since ancient times. Today it is simple to precisely measure each runner's absolute speed but that is not required to determine who won the race. Now consider two people playing chess. We do not know how to measure or predict a player's skill. But by pitting them against each other, having them play a game (or a series of games) the results provide a useful measurement of relative fitness. 
\par
Throughout this model, \textbf{tournaments involve three individuals}. (\citet{reynolds_iec_2011} used tournaments of 10.) One simulation step (see Figure \ref{fig:simulation_overview}) consists of drawing three prey individuals out of their population to compete in a tournament. Like a foot race, or chess game, a tournament serves to sort the individuals according to relative fitness. That relative fitness results from the behavior of adversarial predators. During the same simulation step, three predators are drawn from their population. Each predator looks at the same input: an image with three camouflaged prey overlaid on a background image. Predators compete with each other for accurately targeting prey. Prey compete with each other by hiding from (not being found by) the predators.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Negative Selection, Drift, and Mixability}

A modern synthesis \citep{livnat_sex_2016} of evolution theory, game theory, and machine learning (the multiplicative weights update algorithm: “no-regret learning”) suggests evolution may act to optimize \jargon{mixability}, a type of modularity in the genetic representation of phenotypes. \citet{chastain_multiplicative_2013} shows it focuses on the “...special case of weak selection in which all fitness values are assumed to be close to one another...hypothesizing that evolution proceeds for the most part not by substantial increases in fitness but by essentially random drift...” This concept goes back to the “Neutral Theory” of \citet{kimura_evolutionary_1968}.
\par
\jargon{\lazypredator{},} the evolutionary model used here, operates in this “neutral” mode (as did \citet{reynolds_iec_2011} and \citet{harrington_coevolution_2014}). Genetic algorithms are often designed to promote the population's highest fitness individuals. They allow these high fitness individuals to survive longer and reproduce more often. This \jargon{elitism} can skew the evolutionary search: too much \jargon{exploiting} without enough \jargon{exploring.}  In contrast, \lazypredator{} uses \textit{negative selection} to encourage \jargon{drift}. All high performing individuals have similar fitness. It is the lowest fitness individuals that get culled by predation. In each tournament (see \nameref{subsec:tournaments}) the goal is to sort the three individuals by relative fitness. In fact, the only requirement is to identify the \textbf{least fit} of the three individuals.
\par 
For example, a predator looks at a scene then predicts the location of the most conspicuous of three prey. If this location is within the disk-shaped “body” of a prey, it is the \textbf{least} fit of the prey tournament and gets “eaten.” The relative fitness ordering of the other two prey is not significant and is ignored. If all predators \jargon{fail} and predict positions outside all three prey, then the entire simulation step is abandoned, no prey is eaten, and the prey population is unchanged.
\par
In the predator population, a tournament is ranked by the \textbf{distance from a predator's prediction to the center of the nearest prey} — essentially the predator's “aiming error.” The worst predator might then die from \jargon{starvation} based on its recent history of hunting success: has it eaten enough to survive? Currently this threshold is 40\% hunting success over its last 20 attempts. (More parameters in Table \ref{table:key_simulation_parameters}.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Offspring, Crossover, and Mutation}
In the prey population, a tournament is used to find the lowest relative fitness (\nameref{subsec:tournaments}). This corresponds to the worst camouflage, the \jargon{most conspicuous} of three prey in a tournament. If a predator successfully locates a prey, it is captured and “eaten.” The object representing that prey is removed from its population and replaced with a new individual. This is the \jargon{population update} stage of a steady-state genetic programming system \citep{syswerda_study_1991}.
\par
The new prey, replacing the one eaten by the predator, is the \jargon{offspring} of two other prey. This is the motivation for using tournaments of size 3: the least fit prey dies, and is replaced by the offspring of the tournament's two surviving prey. This is why their relative fitness is irrelevant, they both become the \jargon{parents} given as input to the \jargon{crossover} operation.
\par
In GP (and \lazypredator{}) individuals are represented as tree structures. In this simulation, those trees are interpreted as \texsyn{} programs as described in \nameref{subsec:texture_synthesis}. The GP crossover operation is defined on two abstract parent trees. First they are copied to preserve the originals. One copy is chosen as recipient and one as donor. In each a “random subtree” is selected. The recipient's random subtree is replaced by the donor's random subtree, by splicing the pointers between tree nodes, see Figure \ref{fig:TexSyn_overview}.
\par
After crossover, a mutation operator further modifies the offspring. It traverses the tree, finding all the leaf nodes, which here correspond to numerical constants in the texture programs. Because \lazypredator{} uses \jargon{strongly typed genetic programming} \citep{montana_strongly_1995} each leaf belongs to a specific application-defined type (for example: a floating point number between 0 and 1). A method of the type's class can mutate (“jiggle”) the constant value of a leaf node (for example, add a small signed random offset, then clip back into the type's range). After crossover and mutation the new offspring is inserted into the population, replacing the dead prey. For predator offspring, a random parent is copied, then mutated by adding signed noise to each model parameter.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% prototyping CNN diagram

\begin{figure}
    \includegraphics[width=\columnwidth]{predator_cnn.pdf}
    \caption{Architecture of a predator's neural net which maps a 128² RGB image into an \textit{xy} location where it estimates the most conspicuous prey is centered. All convolution layers but the first use strides of (2,2) while doubling the number of learned filters. Then fully connected layers reduce the flattened features, by factors of 4, down to an output layer of 2 values. This model contains 3.2 million parameters (More details in Supplemental Materials.)}
    % \Description{Diagram of Predator's convolutional neural net.}
    \label{fig:predator_cnn}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Texture Synthesis}
\label{subsec:texture_synthesis}
Camouflage textures are represented in this simulation as trees of procedural texture operators. These correspond directly to nested expressions in a typical programming language. \jargon{\texsyn{}} is a simple domain specific language for describing textures \citep{reynolds_texsyn_2019}.
\par
The details of \texsyn{} are not central to understanding this camouflage model. A quick overview is given here. \texsyn{} is library, an API, with many \jargon{operators} each of which return a \jargon{Texture}. Most of them also take Textures as input parameters, along with simple values like colors, 2d vectors, and scalars. Nested expressions of \texsyn{} operators (“source code”) are compiled into trees of operator instances.
\par
These \texsyn{} textures are represented by operators, trees, and parameters — but not pixel data. Instead the Texture class has a function to sample its color at any floating point \textit{xy} location. That color is computed on the fly, similar to GPU \jargon{fragment shaders} or the Pixel Stream Editor functions in \citet{perlin_image_1985}. Figure \ref{fig:TexSyn_overview} shows some simple examples and how these textures can be recombined with tree crossover.
\par
Note that the images in this paper are rendered at 512² pixels and the prey disks have a diameter of 100 pixels. These are downsampled to 128² for use by predator's vision CNNs.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Full step/tournament image with predator responses
\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{20221007_0806_step_7030.png}
    % \caption{Tournament image after simulation step: three camouflaged prey on a random background crop. Over that are drawn three \jargon{crosshair} marks showing the responses of three predators. These are ranked by minimum distance to a prey center. Best is in black/white, second is green/black, third is red/black. Here, only the top ranked predator chose a position inside a prey's disk, the other two failed. Choice of prey, predators, background image, crop, and prey placement — are all uniform random selection.}
    \caption{Tournament image after simulation step: three camouflaged prey on a random background crop. Three \jargon{crosshair} marks show the responses of three predators, ranked by minimum distance to a prey center. Details in Supplemental Materials.}
    % \Description{Full tournament image showing background, three prey, and three predator responses.}
    \label{fig:predator_responses}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predator Vision}
It is the predator's job to look at a tournament image and “hunt” for prey. These images are built from a portion of a background photo and overlaid with three randomly placed camouflaged prey. The camouflage texture for each disk shaped prey is rendered on the \texsyn{} side. Because all images in this simulation are synthetic, they are labeled with the random \jargon{ground truth} position data for each prey. This allows predators to learn in a \jargon{self-supervised} manner.
\par
\subsubsection{Pre-Training Predator's Vision}
\label{sec:pre_train_predator}
The basis for a predator's visual system is \textbf{a \jargon{pre-trained} deep neural net model for a “find conspicuous disk” task}, see Figure \ref{fig:predator_cnn}. The goal of this task is to look at an arbitrary image and locate the centerpoint of the most conspicuous (salient) region, assumed to be a prey-sized disk. This pre-training is done once then reused for each subsequent camouflage evolution run. \texsyn{} was used to generate a dataset of 20,000 labeled training examples called \jargon{FCD5}. (Effective training set size was 500,000 via augmentation by random variation.) Each example was an RGB image, a 128×128×3 tensor, with an associated label: an \textit{xy} coordinate pair indicating a location in the image.
\par
Each training image starts with a \jargon{random texture} or a random crop of a photo (from a library of background images, see \nameref{subsec:background_sets}) over which are one or three prey disks with random texture. While “random texture” is a slippery concept, the meaning here is the sort of prey texture used to initialize the prey population before an evolution run. The \lazypredator{} genetic programming engine has facilities for creating random trees of a given size from a user-defined \jargon{function set} such as one for \texsyn{}. A random tree is interpreted as a random nested expression of \texsyn{} operators with randomly chosen leaf constants. As such these prey textures are quite varied, but have a fair amount of structure, they are not uncorrelated noise. See leftmost image in Figure \ref{fig:time_sequence}.
\par
Each image in the “find conspicuous disk” training dataset is generated in one of three styles chosen with equal probability. Style 1 has a single prey disk, the label is its center point. Style 2 has three different prey, style 3 has three copies of one prey disk. The latter two cases reduce visibility of two disks, by blending or dithering pixels of the prey disk into the background. The label corresponds to the unchanged disk, presumed to be more conspicuous.
\par
\subsubsection{Fine-Tuning Predator's Vision}
During simulations of camouflage evolution, each predator is initialized as a copy of the pre-trained “find conspicuous disk” model to which noise is added. (Zero mean noise of less than ±0.003 is added to each parameter of the deep neural net.) In each simulation step (see Figure \ref{fig:simulation_overview}) the three predators chosen to participate in the tournament predict a prey position. Then \textbf{each predator is \jargon{fine-tuned} based on a dataset of labeled images collected during this simulation run}. 
% This dataset is a collection of images from earlier simulation steps in this run. It is essentially the memory of prey coloration recently found in this environment. The fine-tuning dataset starts empty, then collects each tournament image until the dataset holds 500 images. Thereafter, each new tournament image replaces one of the 500 chosen at random. Labels for this dataset correspond to predictions made by the “best” predator (least aim error) in each step, see Figure \ref{fig:predator_responses}.
Each predator collects its own dataset consisting of tournament images in which it participated, essentially its memory of prey coloration in this environment. The fine-tuning dataset starts empty, then collects each tournament image until the dataset holds 500 images, then replaces one of the 500 chosen at random. Labels for a predator's dataset are prediction it made in each tournament. See Figure \ref{fig:predator_responses}.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Background Sets}
\label{subsec:background_sets}
Each simulation run is based on a \jargon{background set} of images, usually photographs of natural scenes. These images provide the background of tournament images, over which camouflaged prey are drawn. These background images play the role of an \jargon{environment} in which prey must hide to avoid being found and eaten by predators. Because the model is purely 2d, photographs offer an easy way to provide varied environments, of plausible natural complexity, in which to test camouflage evolution.
\par
Background sets used in this work consist of from 1 to 14 photographs, 4 or 5 being typical. Almost all are casual snapshots taken with a mobile phone. The images within a given background set are all similar. For example a set called \stt{oak\_leaf\_litter} has six images, all of slightly different portions of leaves fallen in gutter. Each image in this set is taken with the camera pointing straight down, all from about the same height. As a result the images have features (e.g. leaves) of about the same size. This “similarity” helps the camouflage evolution process by providing many unique yet analogous backgrounds in which to try hiding.
\par
As used here, “similarity” is meant to suggest image features have a \jargon{stationary distribution}, that different patches (say of a size comparable to prey) have the same statistical distribution of color and spatial frequency. When background sets are “less stationary” (e.g. large areas of uniform color) it becomes \jargon{harder} for evolution to find good camouflage. See faster progress for easy background in Figure \ref{fig:sqm_plot}.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Simulation Runs}
\label{subsec:simulation_runs}
To run this coevolutionary simulation model, two processes are launched. One is the evolutionary texture synthesis system that models a population of camouflaged prey. It is c++ code based on \texsyn{} and \lazypredator{}. The other is a “predator server” that manages a population of visual hunters and fine-tunes their visual perception. This \jargon{\predatoreye{}} \citep{reynolds_predatoreye_2021} is Python code using Keras \citep{chollet_keras_2015} and TensorFlow \citep{tensorflow_whitepaper_2015}. The prey side produces a labeled tournament image. That is inspected by the predator side, which sends back a target location within the image, indicating its estimate of where the most conspicuous prey is. The labeled tournament image is used to fine-tune the predators. The target location drives the fitness function for prey evolution.
\par
Main parameters for a simulation run are a choice of \nameref{subsec:background_sets}, a scale factor for the background images, and a random number seed, see Table \ref{table:key_simulation_parameters}.
\par
A typical run consists of 12,000 steps with a prey population of 400 and predator population of 40. A reimplementation of the interactive approach of \citet{reynolds_iec_2011} used a prey population of 120 and typical runs of 2000 to 3000 steps, as did the first version with a neural net predator. Introducing a population of predators reduced the rate of fine-tuning of predator's neural net models, leading to simulations of 6000, then 12,000 steps.
\par
During a simulation run various data is collected in log files. The most important output is a “visual log” — periodically saving tournament images (a crop of the given backgrounds overlaid with camouflaged prey, as in Figure \ref{fig:teaser}) along with the predator response data. These images are saved “occasionally.” Originally it was every 20 steps, but was changed to every 19 steps to be relatively prime and so cycle through the 6, 10, or 20 \jargon{subpopulations} (\jargon{demes} or \jargon{islands}) of the prey population.
\par
To track the objective progress of camouflage coevolution, the pre-trained FCD neural net model can serve as a \jargon{standard predator} to provide an unchanging \jargon{static quality metric}, see Figure \ref{fig:sqm_plot}. Each prey is scored by the fraction of ten trials this standard predator is “fooled” — fails to find the prey on a random background.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \includegraphics[width=\columnwidth]{SQM_plot_easy_vs_hard.pdf}
    \caption{Static quality metric versus simulation time. Metric is based on failure of pre-trained FCD model to find prey. Compares “easy” background (oak\_leaf\_litter) versus “hard” (yellow\_flower\_on\_green).}
    % \Description{...}
    \label{fig:sqm_plot}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Discussion}
\label{sec:discussion}
This simulation model was applied to a variety of background sets to produce prey camouflage patterns suited to those environments. See examples in Figures \ref{fig:teaser}, \ref{fig:time_sequence}, \ref{fig:rock_wall_4x}, Supplemental Materials, and TexSyn blog \citep{reynolds_texsyn_blog_2023}.
\par
These experiments were run on a 2021 Apple MacBook Pro with an M1 Max chip. Typical simulation runs show the predator vision/learn process taking about “600\%” of the processing power (that is, about 6 of 10 cores) and the prey texture render/evolve process taking about 10\% of a core (many steps reuse cached prey renderings). The time to complete a single simulation step is about 1 second. Typical simulation runs are 12,000 steps so take about 3.5 hours. When using the static quality metric to evaluate progress (Figure \ref{fig:sqm_plot}) total simulation run time is about 10 hours.
\par
A key observation from these experiments is that background sets present varying levels of difficulty. For some background sets, a simulated evolution run easily produces effective camouflage. Occasionally it is good enough to momentarily fool a human observer: “Wait, where is the third prey?!” Sometimes runs fail to produce effective results. Re-running the simulation with a new random seed will often find a solution. This suggests a “hard” background set may have a likelihood of success during a run of say 1/2 or 1/3 while “easy” background have likelihood close to 1.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}
    \igfour{20221112_1555_step_6495.png}
    \hfill
    \igfour{20221112_1555_step_5510.png}
    \hfill
    \igfour{20221112_1555_step_5681.png}
    \hfill
    \igfour{20221112_1555_step_6370.png}
    \caption{Four tournament images from run {\runID rock\_wall\_20221112\_1555}.}
    % \Description{QQQ.}
    \label{fig:rock_wall_4x}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limitations}
\label{subsec:limitations}
This work has the nature of an “existence proof.” The model of camouflage coevolution presented here is a starting point. Its architecture and parameters (Table \ref{table:key_simulation_parameters}) are unlikely to be the best. It was tuned well enough to allow camouflage evolution, but surely can be improved.
\par
Another limitation is that all results are hand selected: “cherry picked” (see Figures \ref{fig:teaser}, \ref{fig:time_sequence}, \ref{fig:rock_wall_4x}, Supplemental Materials). After a simulation run, the resulting images are examined by a human who selects some as representative of the results, and makes a mental judgement of their effectiveness.
\par
Other limitations include the inherently 2D nature of the simulation, that simulated time is discrete, and that the model of texture synthesis lacks genetic or biological plausibility. These are among the simplifying abstractions intended to make this initial model of camouflage coevolution tractable. Still it may be fair to complain that as a result, the model is \textit{too} abstract to provide much biological insight.
\par

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Future Work}
The goal of this work has been to build a computational model of the coevolution of camouflage from the interaction of predator and prey. It demonstrates that camouflage can in fact arise from such a system. It also provides a simple process to generate camouflage for a given environment from photos. The most important contribution of the work has been to create an open source model allowing future experiments, to help study camouflage in nature, and the perceptual phenomenon of camouflage more generally.
\par
A \jargon{static quality metric} for camouflage (discussed in \nameref{subsec:simulation_runs} and Figure \ref{fig:sqm_plot}) is based on using the pre-trained predator model (see \nameref{sec:pre_train_predator}) as a standard. This is not ideal. It seems likely this metric may saturate (reach the top of its range, failing to find a prey in all 10 trials) while coevolution continues to improve camouflage quality. Other approaches should be investigated. For example work \citep{lv_cod_2022} suggests potential ways to rank “conspicuousness.” The model presented here would be a useful testbed for evaluating candidate camouflage metrics. It should also be possible to validate these candidate metrics with crowd-sourced rating of camouflage quality, as in SEE Games \citep{stevens_games_2022} and CamoEvo \citep{hancock_camoevo_2022}.
\par
The static quality metric may allow automating the tedious and potentially biased “cherry picking” of high quality examples from a 12,000 step run.
\par
All of the parameters in Table \ref{table:key_simulation_parameters} should be reexamined using a static camouflage metric. Similarly, the neural net architecture in Figure \ref{fig:predator_cnn} is simply the first one that worked. Other designs should be designed and evaluated.
\par
\par
In early versions, simulated predators often incorrectly predicted prey to be at the tournament image's center. Perhaps this \jargon{center preference} is a “lazy” default strategy, since that is the mean over all prey positions. To work around this, \texsyn{}'s random prey placement was constrained to avoid positions within one prey diameter of the image center. This seems like a bug that should be better understood and fixed in a more principled way.
\par
This simulation is currently a mixed paradigm model using both evolution and learning. While these typically co-occur in nature \citep{valiant_probably_2013}, what about an all-evolution model with evolved detectors for predators? Perhaps along the lines of \citet{harrington_coevolution_2014} and \citet{bi_genetic_2022}. Or conversely, an all learning model, something similar to CamoGAN \citep{talas_camogan_2020}.
\par
An obvious next step is to apply this model to 3d environments. Perhaps as described in \citet{miller_color_2022}. One key simplifying assumption of the current purely 2d model is that plausible, naturally complex, environments are provided by simple photographs from the phone in our pocket. Providing plausibly complex 3d environments is much harder. Perhaps neural techniques (like NeRF \citep{gao_nerf_2022} or NIP \citep{sharp_spelunking_2022}) will meet that need.
\par


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Acknowledgements
% \begin{acks}
\section{Acknowledgements}
I deeply appreciate everyone who helped me with this work: my family for loving support, Ken Perlin for (well, lots, but especially) PSE \citep{perlin_image_1985}, Andrew Glassner for teaching me everything I know about deep learning \citep{glassner_deep_2021}, and Pat Hanrahan for some key career advice (“just do the research”).
\par
I've been working on this project on and off since 2007, based on inspirations by papers in the early 1990s: \citet{witkin_reaction_1991}, \citet{turk_generating_1991}, \citet{angeline_competitive_1993}, \citet{sims_artificial_1991}, and \citet{sims_evolving_1994}). Also one paper published a year before I was born: \citet{turing_chemical_1952}.
Thanks for additional help from:
Bilal Abbasi,
Jan Allbeck,
Rebecca Allen,
Richard Dawkins,
Steve DiPaola,
Aaron Hertzmann,
Bjoern Knafla,
John Koza,
Dominic Mallinson,
Nick Porcino,
and Michael Wahrman.
% \par
% Thanks also to my neighbors whose landscaping provided many of the background images used here.
Thanks also to my neighbors whose landscaping provided many of the background images used here, collected on daily walks during COVID-19 lockdown.
\par
% \end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Bibliography.

\bibliographystyle{apalike}
\bibliography{coc.bib}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{tiny}
\begin{footnotesize}
\centering
% \vspace{0.3cm}
\vspace{0.7cm}
\begin{tabular}{ |l|r|r| }
\hline
\textbf{Parameter} & \textbf{value} \\ 
\hline
predator population & 40 \\ 
prey population & 400 \\ 
prey subpopulations (demes) & 20 \\
prey max init tree size & 100 \\
prey min tree size after crossover & 50 \\
prey max tree size after crossover & 150 \\
\hline
prey render diameter (pixels) & 100 \\ 
tournament output image size & 512×512 \\ 
predator input image size & 128×128 \\ 
\hline
tournament image save frequency & 19 \\
simulation steps per run (typical) & 12,000 \\
prey generation equiv (steps/pop) & 30 \\
predator fail rate (typical) & 15\%-30\% \\
predator starvation threshold & \\
\hspace{0.2cm}(success in previous 20 attempts) & < 40\% \\ 
\hline
predator “FCD” pre-training: & \\
\hspace{0.2cm} synthetic dataset size & 20,000 \\
\hspace{0.2cm} effective size with augmentation & 500,000 \\
\hline
max signed “jiggle” noise added to  & \\
\hspace{0.2cm} all params of new predator CNN & ±0.003 \\
\hline
static quality metric: trials per prey & 10\\
\hline
\end{tabular}
\captionof{table}{Key simulation parameters. Details in source code.} 
\label{table:key_simulation_parameters} 
\vspace{0.7cm}
% \end{tiny}
\end{footnotesize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix
\onecolumn
\section{Supplemental Materials}
\setcounter{page}{0}

\subsection{Background Image Sets}
\begin{minipage}{\linewidth}
Names and descriptions for the sets of background images used in this paper, see section \nameref{subsec:background_sets}. Each set is composed of several photographs of a similar natural scene.
\par
\hspace*{1cm}
\begin{minipage}{\linewidth-1.1cm}
\begin{table}[H]
\raggedright 
\begin{tabular}{ |l|l|c|c| }
\hline
\textbf{name} & \textbf{description} & \textbf{photos} & \textbf{figures} \\ 
\hline
backyard\_oak &
    under canopy of California live oak (\textit{Quercus agrifolia}) &
    12 & \ref{fig:backyard_oak_4x} \\
\hline
bean\_soup\_mix &
    mixture of dried beans from grocery store &
    4 & \ref{fig:bean_soup_mix_4x} \\
\hline
jans\_oak\_leaves &
    white(?) oak leaf litter (by Jan Allbeck in Fairfax, Virginia) &
    6 & \ref{fig:jans_oak_leaves_4x} \\
\hline
kitchen\_granite &
    polished granite counter-top in our kitchen &
    6 & \ref{fig:kitchen_granite_4x} \\
\hline
mbta\_flowers &
    flowers (impatiens?) near MBTA Northeastern stop &
    4 & \ref{fig:predator_responses}, \ref{fig:mbta_flowers_4x} \\
\hline
michaels\_gravel &
    gravel bed in neighbor’s front yard &
    4 & \ref{fig:teaser}, \ref{fig:predator_cnn}, \ref{fig:michaels_gravel_4x} \\
\hline
oak\_leaf\_litter &
    fallen oak leaves on edge of road &
    6 & \ref{fig:time_sequence} \\
\hline
oxalis\_sprouts &
    sprouts of Oxalis push through leaf litter after first rain &
    5 & \ref{fig:teaser} \\
\hline
plum\_leaf\_litter &
    fallen leaves from plum and other trees, near sunset &
    5 & \ref{fig:teaser}, \ref{fig:plum_leaf_litter_4x} \\
\hline
redwood\_leaf\_litter &
    dried redwood leaf/needle litter collected in a roadside gutter &
    4 & \ref{fig:redwood_leaf_litter_4x} \\
\hline
rock\_wall &
    “dry stack” retaining wall in a neighbor's front yard &
    14 & \ref{fig:rock_wall_4x} \\
\hline
tiger\_eye\_beans &
    dried heirloom “tiger eye” beans from farmers market  &
    5 & \ref{fig:simulation_overview} \\
\hline
tree\_leaf\_blossom\_sky &
    small trees (branches, leaves, and blossoms) sky background &
    5 & \ref{fig:teaser}, \ref{fig:tree_leaf_blossom_sky_4x} \\
\hline
yellow\_flower\_on\_green &
    “Scot's broom” (or “French broom”?) in neighbor's yard &
    6 & \ref{fig:yellow_flower_4x} \\
\hline
\end{tabular}
\label{table:background_sets}
\end{table}
\end{minipage}
\end{minipage}

\subsection{Details of Pre-Trained Predator Model}
The pre-trained predator visual system, shown in Fig. \ref{fig:predator_cnn}, is a Keras TensorFlow CNN model with about 3.2 million parameters. Its input is a 128×128 pixel RGB image and its output is an \textit{xy} location where it estimates the most conspicuous prey is centered. Here is its \texttt{model.summary()}:
\par
\begin{minipage}{\linewidth-1.1cm}
\hspace*{1cm}
\begin{minipage}{\linewidth-1.1cm}
\begin{small}
\begin{verbatim}

Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 128, 128, 16)      1216
 dropout (Dropout)           (None, 128, 128, 16)      0
 conv2d_1 (Conv2D)           (None, 64, 64, 32)        12832
 dropout_1 (Dropout)         (None, 64, 64, 32)        0
 conv2d_2 (Conv2D)           (None, 32, 32, 64)        51264
 dropout_2 (Dropout)         (None, 32, 32, 64)        0
 conv2d_3 (Conv2D)           (None, 16, 16, 128)       204928
 dropout_3 (Dropout)         (None, 16, 16, 128)       0
 conv2d_4 (Conv2D)           (None, 8, 8, 256)         819456
 dropout_4 (Dropout)         (None, 8, 8, 256)         0
 flatten (Flatten)           (None, 16384)             0
 dense (Dense)               (None, 128)               2097280
 dropout_5 (Dropout)         (None, 128)               0
 dense_1 (Dense)             (None, 32)                4128
 dense_2 (Dense)             (None, 8)                 264
 dense_3 (Dense)             (None, 2)                 18
=================================================================
Total params: 3,191,386
Trainable params: 3,191,386
Non-trainable params: 0
\end{verbatim}
\end{small}
\end{minipage}
\end{minipage}
\par

\subsection{\texsyn{} \texttt{c++} Code for Figure \ref{fig:TexSyn_overview}}
% Is it any better to use LaTeX "listings" package?
%     https://ctan.org/pkg/listings
%     note that package IS approved by SIGGRAPH/ACM TAPS
%     https://authors.acm.org/proceedings/production-information/accepted-latex-packages
\begin{minipage}{\linewidth}
\hspace*{1cm}
\begin{minipage}{\linewidth-1.1cm}
\begin{small}
\begin{verbatim}
Uniform white(1);
Uniform gray(0.1);
Uniform blue(0, 0, 1);
Uniform green(0, 1, 0);
LotsOfSpots spots(0.9, 0.05, 0.3, 0.02, 0.02, blue, white);
Grating stripes(Vec2(), green, Vec2(0.1, 0.2), gray, 0.3, 0.5);
NoiseWarp warp_stripes(1, 0.1, 0.7, stripes);
LotsOfSpots spots2(0.9, 0.05, 0.3, 0.02, 0.02, stripes, white);
Grating stripes2(Vec2(), green, Vec2(0.1, 0.2), spots, 0.3, 0.5);
NoiseWarp warp_all(1, 0.1, 0.7, stripes2);
\end{verbatim}
\end{small}
\end{minipage}
\end{minipage}

\subsection{Additional command line arguments to pkgT for simulation runs}
\begin{minipage}{\linewidth-1.1cm}
\begin{small}
\hspace*{1cm}
\begin{minipage}{\linewidth-1.1cm}
\begin{verbatim}
background image directory (required)
output directory (defaults to .)
background scale (defaults to 0.5)
random seed (else: default seed)
window width (defaults to 1200)
window height (defaults to 800)
individuals (defaults to 120)
subpopulations (defaults to 6)
max init tree size (defaults to 100)
min crossover tree size (default max init tree size  * 0.5)
max crossover tree size (default max init tree size  * 1.5)
\end{verbatim}
\end{minipage}
\end{small}
\end{minipage}


\subsection{Procedurally generated training set for “find conspicious disk”}


\begin{figure}[H]
    \begin{minipage}{\linewidth}
    \ignine{20220303_UVSfqCzewt_38_20.png}
    \hfill
    \ignine{20220303_SBWaLRHOzk_56_33.png}
    \hfill
    \ignine{20220303_bUMqcbutgJ_25_78.png}
    \hfill
    \ignine{20220303_HZzUzWWqcC_54_28.png}
    \hfill
    \ignine{20220303_inuPKUxnHQ_72_71.png}
    \hfill
    \ignine{20220303_RRGCwhmcJc_101_84.png}
    \hfill
    \ignine{20220303_PYinyJAWaj_61_60.png}
    \hfill
    \ignine{20220303_TNXfhQtzYa_92_91.png}
    \hfill
    \ignine{20220303_cDMtFaTYKk_63_54.png}
    \end{minipage}
    \begin{minipage}{\linewidth}
    \vspace{0.1cm}
    \ignine{20220303_wIRPERwSCh_49_63.png}
    \hfill
    \ignine{20220303_edDsCjbHdf_61_92.png}
    \hfill
    \ignine{20220303_fGMFBgMQDX_93_86.png}
    \hfill
    \ignine{20220303_jQREPLQyuL_33_39.png}
    \hfill
    \ignine{20220303_ijBOHTccYX_104_101.png}
    \hfill
    \ignine{20220303_KAoOFAqFyU_80_58.png}
    \hfill
    \ignine{20220303_NExMwxEbzU_85_92.png}
    \hfill
    \ignine{20220303_kpcUyhHXOh_91_98.png}
    \hfill
    \ignine{20220303_oWPwPGkcSb_82_22.png}
    \end{minipage}
    \begin{minipage}{\linewidth}
    \vspace{0.1cm}
    \ignine{20220303_uAEPxMZbeo_83_45.png}
    \hfill
    \ignine{20220303_cADfBauZUV_47_32.png}
    \hfill
    \ignine{20220303_YAMfudJxeH_30_84.png}
    \hfill
    \ignine{20220303_JeyBgDfMcN_40_82.png}
    \hfill
    \ignine{20220303_OaOJaByhbU_90_55.png}
    \hfill
    \ignine{20220303_mhYpDjxaKf_78_57.png}
    \hfill
    \ignine{20220303_ASsEgFUlly_23_60.png}
    \hfill
    \ignine{20220303_nzgItDrYqT_71_99.png}
    \hfill
    \ignine{20220303_QuHYtnPora_72_73.png}
    \end{minipage}
    \caption{Examples of the three types of labeled training example in the FCD5 training dataset for “find conspicuous disk” pre-training. \textbf{Top row, type 1}: single random prey over random background. \textbf{Middle row, type 2}: three different prey. \textbf{Bottom row, type 3}: three copies of one prey. For types 2 and 3, one prey is unaltered, the other two are blended into background, by differing amounts, to make them more muted, less conspicuous. See Section \nameref{sec:pre_train_predator}.}
    % \Description{Full tournament image showing background, three prey, and three predator responses.}
    \label{fig:fcd5_examples}
\end{figure}

\newpage 

\subsection{Additional Samples of Predator Responses}
Similar to Fig. \ref{fig:predator_responses}, these examples from run {\runID tiger\_eye\_beans\_20220903\_1401} show the prediction output of all three predators in each tournament. The crosshairs are ordered by least “aim error” with the best drawn in black and white, second in black and green, and worst in black and red. From left to right: (a) all three predators miss all three prey, (b) best is near center of one prey, second is off center but still inside another prey, and third fails to find any prey, (c) all three succeed, best inside one prey, the other two inside another prey, and (d) all three predators select the same prey which has especially conspicuous coloration.
\par

\begin{figure}[H]
    \igfour{20220904_step_4883.png}
    \hfill
    \igfour{20220904_step_4788.png}
    \hfill
    \igfour{20220904_step_3914.png}
    \hfill
    \igfour{20220904_step_4636.png}
    \caption{Four tournament images, plus predator responses as crosshairs, from run {\runID tiger\_eye\_beans\_20220903\_1401}.}
    % \Description{QQQ.}
    \label{fig:tiger_eye_beans_4x}
\end{figure}


\subsection{Additional Samples of Simulation Runs}

\begin{figure}[H]
    \igfour{20230115_step_6902.png}
    \hfill
    \igfour{20230115_step_7682.png}
    \hfill
    \igfour{20230115_step_7942.png}
    \hfill
    \igfour{20230115_step_12413.png}
    \caption{Four tournament images from run {\runID backyard\_oak\_20230113\_2254}.}
    % \Description{QQQ.}
    \label{fig:backyard_oak_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20230106_step_11019.png}
    \hfill
    \igfour{20230106_step_11204.png}
    \hfill
    \igfour{20230106_step_11689.png}
    \hfill
    \igfour{20230106_step_11995.png}
    \caption{Four tournament images from run {\runID mbta\_flowers\_20230105\_1114}.}
    % \Description{QQQ.}
    \label{fig:mbta_flowers_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20221215_step_5867.png}
    \hfill
    \igfour{20221215_step_5892.png}
    \hfill
    \igfour{20221215_step_6830.png}
    \hfill
    \igfour{20221215_step_6916.png}
    \caption{Four tournament images from run {\runID michaels\_gravel\_20221214\_1837}.}
    % \Description{QQQ.}
    \label{fig:michaels_gravel_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20230111_step_5576.png}
    \hfill
    \igfour{20230111_step_6159.png}
    \hfill
    \igfour{20230111_step_6303.png}
    \hfill
    \igfour{20230111_step_6726.png}
    \caption{Four tournament images from run {\runID kitchen\_granite\_20230110\_1758}.}
    % \Description{QQQ.}
    \label{fig:kitchen_granite_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20221218_step_5396.png}
    \hfill
    \igfour{20221218_step_5641.png}
    \hfill
    \igfour{20221218_step_5947.png}
    \hfill
    \igfour{20221218_step_6753.png}
    \caption{Four tournament images from run {\runID yellow\_flower\_on\_green\_20221217\_1826}. This background set was a notoriously “hard” non-stationary test case.}
    % \Description{QQQ.}
    \label{fig:yellow_flower_4x}
\end{figure}





\begin{figure}[H]
    \igfour{20230116_step_5868.png}
    \hfill
    \igfour{20230116_step_6057.png}
    \hfill
    \igfour{20230116_step_6347.png}
    \hfill
    \igfour{20230116_step_6814.png}
    \caption{Four tournament images from run {\runID redwood\_leaf\_litter\_20230115\_1730}.}
    % \Description{QQQ.}
    \label{fig:redwood_leaf_litter_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20221108_2018_step_4655.png}
    \hfill
    \igfour{20221108_2018_step_5498.png}
    \hfill
    \igfour{20221108_2018_step_5947.png}
    \hfill
    \igfour{20221108_2018_step_6562.png}
    \caption{Four tournament images from run {\runID tree\_leaf\_blossom\_sky\_20221108\_2018}.}
    % \Description{QQQ.}
    \label{fig:tree_leaf_blossom_sky_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20221121_1819_step_6324.png}
    \hfill
    \igfour{20221121_1819_step_6464.png}
    \hfill
    \igfour{20221121_1819_step_6677.png}
    \hfill
    \igfour{20221121_1819_step_6755.png}
    \caption{Four tournament images from run {\runID plum\_leaf\_litter\_20221121\_1819}.}
    % \Description{QQQ.}
    \label{fig:plum_leaf_litter_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20230101_step_10561.png}
    \hfill
    \igfour{20230101_step_10750.png}
    \hfill
    \igfour{20230101_step_10950.png}
    \hfill
    \igfour{20230101_step_11861.png}
    \caption{Four tournament images from run {\runID bean\_soup\_mix\_20221231\_1317}.}
    % \Description{QQQ.}
    \label{fig:bean_soup_mix_4x}
\end{figure}

\begin{figure}[H]
    \igfour{20221228_step_6662.png}
    \hfill
    \igfour{20221228_step_7095.png}
    \hfill
    \igfour{20221228_step_7574.png}
    \hfill
    \igfour{20221228_step_7742.png}
    \caption{Four tournament images from run {\runID jans\_oak\_leaves\_20221227\_1717}.}
    % \Description{QQQ.}
    \label{fig:jans_oak_leaves_4x}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}