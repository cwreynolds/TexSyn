
@misc{akiba_evolutionary_2024,
	title = {Evolutionary {Optimization} of {Model} {Merging} {Recipes}},
	url = {http://arxiv.org/abs/2403.13187},
	abstract = {We present a novel application of evolutionary algorithms to automate the creation of powerful foundation models. While model merging has emerged as a promising approach for LLM development due to its cost-effectiveness, it currently relies on human intuition and domain knowledge, limiting its potential. Here, we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing for optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models like a Japanese LLM with Math reasoning capabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with significantly more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese VLMs. This work not only contributes new state-of-the-art models back to the open-source community, but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.},
	publisher = {arXiv},
	author = {Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13187 [cs]},
	keywords = {deep\_learning, evolution, optimization, ga, hybrid, Computer Science - Neural and Evolutionary Computing, merging, model, sakana},
}

@inproceedings{angeline_competitive_1993,
	address = {Pittsburgh, PA},
	title = {Competitive {Environments} {Evolve} {Better} {Solutions} for {Complex} {Tasks}},
	isbn = {978-0-8058-0426-3},
	shorttitle = {{ICGA} 1},
	abstract = {In the typical genetic algorithm experiment, the fitness function is constructed to be independent of the contents of the population to provide a consistent objective measure. Such objectivity entails significant knowledge about the environment which suggests either the problem has previously been solved or other non-evolutionary techniques may be more efficient. Furthermore, for many complex tasks an independent fitness function is either impractical or impossible to provide. In this paper, we demonstrate that competitive fitness functions, i.e. fitness functions that are dependent on the constituents of the population, can provide a more robust training environment than independent fitness functions. We describe three differing methods for competitive fitness, and discuss their respective advantages.},
	booktitle = {Proceedings {Of} {The} {First} {International} {Conference} {On} {Genetic} {Algorithms} {And} {Their} {Applications}},
	publisher = {Psychology Press},
	author = {Angeline, Peter J. and Pollack, Jordan B.},
	editor = {Grefenstette, John J.},
	month = jun,
	year = {1993},
	keywords = {adversary, coevolution, competition, cooperative, evolution, games, genetic algorithm, Genetic programming},
	pages = {1--7},
	file = {Angeline and Pollack - 1993 - Competitive Environments Evolve Better Solutions f.pdf:/Users/cwr/Zotero/storage/Z8P7CATV/Angeline and Pollack - 1993 - Competitive Environments Evolve Better Solutions f.pdf:application/pdf;Citeseer - Snapshot:/Users/cwr/Zotero/storage/DCGBXU6M/summary.html:text/html},
}

@article{bi_genetic_2022,
    title={Genetic Programming-Based Evolutionary Deep Learning for Data-Efficient Image Classification},
    author={Bi, Ying and Xue, Bing and Zhang, Mengjie},
    journal={IEEE Transactions on Evolutionary Computation},
    year={2022},
    doi={10.1109/TEVC.2022.3214503},
    pages={15},
    volume = {0},
    number = {0},
    publisher={IEEE}
}

@article{bradley_generation_2023,
	title = {The {Generation} of {Visually} {Credible} {Adversarial} {Examples} with {Genetic} {Algorithms}},
	volume = {3},
	issn = {2688-299X},
	url = {https://dl.acm.org/doi/10.1145/3582276},
	doi = {10.1145/3582276},
	abstract = {An adversarial example is an input that a neural network misclassifies although the input differs only slightly from an input that the network classifies correctly. Adversarial examples are used to augment neural network training data, measure the vulnerability of neural networks, and provide intuitive interpretations of neural network output that humans can understand. Although adversarial examples are defined in the literature as similar to authentic input from the perspective of humans, the literature measures similarity with mathematical norms that are not scientifically correlated with human perception. Our main contributions are to construct a genetic algorithm (GA) that generates adversarial examples more similar to authentic input than do existing methods and to demonstrate with a survey that humans perceive those adversarial examples to have greater visual similarity than existing methods. The GA incorporates a neural network, and we test many parameter sets to determine which fitness function, selection operator, mutation operator, and neural network generate adversarial examples most visually similar to authentic input. We establish which mathematical norms are most correlated with human perception, which permits future research to incorporate the human perspective without testing many norms or conducting intensive surveys with human subjects. We also document a tradeoff between speed and quality in adversarial examples generated by GAs and existing methods. Although existing adversarial methods are faster, a GA provides higher-quality adversarial examples in terms of visual similarity and feasibility of adversarial examples. We apply the GA to the Modified National Institute of Standards and Technology (MNIST) and Canadian Institute for Advanced Research (CIFAR-10) datasets.},
	number = {1},
	urldate = {2023-07-04},
	journal = {ACM Transactions on Evolutionary Learning and Optimization},
	author = {Bradley, James R. and Blossom, A. Paul},
	month = mar,
	year = {2023},
	keywords = {camouflage, cocish, evolution, coevolution, neural networks, Genetic algorithms, adversarial examples, CIFAR-10, MNIST},
	pages = {2:1--2:44},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/FWA8B4JG/Bradley and Blossom - 2023 - The Generation of Visually Credible Adversarial Ex.pdf:application/pdf},
}

@misc{brichard_natural_2023,
	title = {From natural to sexual selection: {Revealing} a hidden preference for camouflage patterns},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {From natural to sexual selection},
	url = {https://www.biorxiv.org/content/10.1101/2023.09.27.559753v1},
	doi = {10.1101/2023.09.27.559753},
	abstract = {Natural and sexual selection can be in conflict in driving the evolution of sexual ornamentation. Sexual selection favours detectability to potential mates, whereas natural selection penalises detectability to avoid predators. Focusing on signal efficiency rather than detectability, however, suggests that natural and sexual selection need not be antagonistic. Considerable evidence demonstrates that people prefer images that match the statistics of natural scenes, likely because they are efficiently processed by the brain. This “processing bias” suggests that background-matching camouflage can be favoured by natural and sexual selection. We conducted an online experiment and showed for the first time human preference for camouflaged stimuli. Because the underlying visual mechanisms are shared across vertebrates, our results suggest that camouflage patterns could serve as evolutionary precursors of sexual signals.},
	language = {en},
	urldate = {2023-11-15},
	publisher = {bioRxiv},
	author = {Brichard, Yseult Hejja and Raymond, Michel and Cuthill, Innes C. and Mendelson, Tamra C. and Renoult, Julien P.},
	month = sep,
	year = {2023},
	note = {Pages: 2023.09.27.559753
Section: New Results},
	keywords = {camouflage, cocish, predator, perception, humanfitnessfunction, sexual\_selection, preference, signal\_efficiency},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/LH3HJUFM/Brichard et al. - 2023 - From natural to sexual selection Revealing a hidd.pdf:application/pdf},
}

@misc{casgrain_deep_2022,
	title = {Deep {Q}-{Learning} for {Nash} {Equilibria}: {Nash}-{DQN}},
	shorttitle = {Deep {Q}-{Learning} for {Nash} {Equilibria}},
	url = {http://arxiv.org/abs/1904.10554},
	doi = {10.48550/arXiv.1904.10554},
	abstract = {Model-free learning for multi-agent stochastic games is an active area of research. Existing reinforcement learning algorithms, however, are often restricted to zero-sum games, and are applicable only in small state-action spaces or other simplified settings. Here, we develop a new data efficient Deep-Q-learning methodology for model-free learning of Nash equilibria for general-sum stochastic games. The algorithm uses a local linear-quadratic expansion of the stochastic game, which leads to analytically solvable optimal actions. The expansion is parametrized by deep neural networks to give it sufficient flexibility to learn the environment without the need to experience all state-action pairs. We study symmetry properties of the algorithm stemming from label-invariant stochastic games and as a proof of concept, apply our algorithm to learning optimal trading strategies in competitive electronic markets.},
	urldate = {2024-05-25},
	publisher = {arXiv},
	author = {Casgrain, Philippe and Ning, Brian and Jaimungal, Sebastian},
	month = oct,
	year = {2022},
	note = {arXiv:1904.10554 [cs, q-fin, stat]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Quantitative Finance - Computational Finance, Statistics - Machine Learning},
}

@incollection{chastain_multiplicative_2013,
	address = {New York, NY, USA},
	title = {Multiplicative updates in coordination games and the theory of evolution},
	isbn = {978-1-4503-1859-4},
	url = {https://doi.org/10.1145/2422436.2422444},
	abstract = {In this paper we point out a new and unexpected connection between three fields: Evolution Theory, Game Theory, and Algorithms. In particular, we study the standard equations of population genetics for Evolution, in the presence of recombination (sex), focusing on the important special case of weak selection [1,2] in which all fitness values are assumed to be close to one another. Weak selection is the mathematical regime capturing the widely accepted Neutral Theory proposed by Kimura in the 1970s [3], hypothesizing that evolution proceeds for the most part not by substantial increases in fitness but by essentially random drift. We show that in this regime evolution through natural selection and sex is tantamount to a game played through the multiplicative weight updates game dynamics [4]. The players of the game are the genes (genetic loci), the strategies available to each player are the alleles of the gene, and the probabilities whereby a player plays a strategy is the strategy's frequency in the population. The utility to each player/gene of each strategy profile is the fitness of the corresponding genotype (organism). That is, the game is a coordination game between genes, in which the players' interests are perfectly aligned. Importantly, the utility maximized in this game, as well as the amount by which each allele is boosted, is precisely the allele's mixability, or average fitness, a quantity recently proposed in [5] as a novel concept that is crucial in understanding natural selection under sex, thus providing a rigorous demonstration of that insight. We also establish a result regarding the maintenance of genetic diversity (multiplicity of alleles per gene). We prove that the equilibria in two-person coordination games are likely to have large supports, and thus genetic diversity need not suffer much at equilibrium. Establishing large supports involves answering through a novel technique the following question: what is the probability that for a random square matrix \$A\$ (with entries drawn independently from smooth distributions that are symmetric around zero) both systems Ax=1 and ATy=1 have positive solutions? The proof is through a simple potential function argument. Both the question and the technique may be of broader interest. It has often seemed astonishing --- even to experienced students of Evolution, Darwin included --- that the crude mechanism of natural selection is responsible for producing the dazzling variety of Life around us. The present mathematical connection of Evolution with the multiplicative weight updates algorithm --- a technique that has surprised our field time and again with its fantastic effectiveness and versatility --- may carry some explanatory force in this regard.},
	booktitle = {Proceedings of the 4th conference on {Innovations} in {Theoretical} {Computer} {Science}},
	publisher = {Association for Computing Machinery},
	author = {Chastain, Erick and Livnat, Adi and Papadimitriou, Christos and Vazirani, Umesh},
	month = jan,
	year = {2013},
	keywords = {algorithmic game theory, multiplicative weight updates, theory of evolution},
	pages = {57--58},
	file = {Chastain et al. - 2013 - Multiplicative updates in coordination games and t.pdf:/Users/cwr/Zotero/storage/N882C76B/Chastain et al. - 2013 - Multiplicative updates in coordination games and t.pdf:application/pdf},
}

@article{chen_boundary-guided_2022,
	title = {Boundary-guided network for camouflaged object detection},
	volume = {248},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122004294},
	doi = {https://doi.org/10.1016/j.knosys.2022.108901},
	abstract = {Compared with the traditional object segmentation/detection, camouflaged object detection is much more difficult due to the indefinable boundaries and high intrinsic similarities between the camouflaged regions and the background. Although various algorithms have been proposed to solve the issue, these methods still suffer from coarse boundaries and are not competent to identify the camouflaged objects from the background in complex scenarios. In this paper, we propose a novel boundary-guided network to address this challenging problem in a coarse-to-fine manner. Specifically, we design a locating module to infer the initial location of the camouflaged objects by exploiting local detailed cues and global contextual information. Moreover, a boundary-guided fusion module is proposed to explore the complementary relationship between the camouflaged regions and their boundaries. By leveraging the boundary feature, we can not only generate prediction maps with sharper boundaries but also effectively eliminate background noises. Equipped with the two key modules, our BgNet is capable of segmenting camouflaged regions accurately and quickly. Extensive experimental results on four widely used benchmark datasets demonstrate that the proposed BgNet runs at a real-time speed (36 FPS) on a single NVIDIA Titan XP GPU and outperforms 17 state-of-the-art competing algorithms in terms of six standard evaluation metrics. Source code will be publicly available at https://github.com/clelouch/BgNet upon paper acceptance.},
	journal = {Knowledge-Based Systems},
	author = {Chen, Tianyou and Xiao, Jin and Hu, Xiaoguang and Zhang, Guofeng and Wang, Shaojie},
	year = {2022},
	keywords = {Boundary guidance, Camouflage object detection, Coarse-to-fine refinement, Convolutional neural network},
	pages = {108901},
}

@misc{chen_diffusion_2023,
	title = {Diffusion {Model} for {Camouflaged} {Object} {Detection}},
	url = {http://arxiv.org/abs/2308.00303},
	doi = {10.48550/arXiv.2308.00303},
	abstract = {Camouflaged object detection is a challenging task that aims to identify objects that are highly similar to their background. Due to the powerful noise-to-image denoising capability of denoising diffusion models, in this paper, we propose a diffusion-based framework for camouflaged object detection, termed diffCOD, a new framework that considers the camouflaged object segmentation task as a denoising diffusion process from noisy masks to object masks. Specifically, the object mask diffuses from the ground-truth masks to a random distribution, and the designed model learns to reverse this noising process. To strengthen the denoising learning, the input image prior is encoded and integrated into the denoising diffusion model to guide the diffusion process. Furthermore, we design an injection attention module (IAM) to interact conditional semantic features extracted from the image with the diffusion noise embedding via the cross-attention mechanism to enhance denoising learning. Extensive experiments on four widely used COD benchmark datasets demonstrate that the proposed method achieves favorable performance compared to the existing 11 state-of-the-art methods, especially in the detailed texture segmentation of camouflaged objects. Our code will be made publicly available at: https://github.com/ZNan-Chen/diffCOD.},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Chen, Zhennan and Gao, Rongrong and Xiang, Tian-Zhu and Lin, Fan},
	month = aug,
	year = {2023},
	note = {arXiv:2308.00303 [cs]},
	keywords = {camouflage, vision, segmentation, Computer Science - Computer Vision and Pattern Recognition, diffusion, cod},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/P9MFX46I/Chen et al. - 2023 - Diffusion Model for Camouflaged Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/7HR26BYC/2308.html:text/html},
}

@misc{chen_camodiffusion_2023,
	title = {{CamoDiffusion}: {Camouflaged} {Object} {Detection} via {Conditional} {Diffusion} {Models}},
	shorttitle = {{CamoDiffusion}},
	url = {http://arxiv.org/abs/2305.17932},
	doi = {10.48550/arXiv.2305.17932},
	abstract = {Camouflaged Object Detection (COD) is a challenging task in computer vision due to the high similarity between camouflaged objects and their surroundings. Existing COD methods primarily employ semantic segmentation, which suffers from overconfident incorrect predictions. In this paper, we propose a new paradigm that treats COD as a conditional mask-generation task leveraging diffusion models. Our method, dubbed CamoDiffusion, employs the denoising process of diffusion models to iteratively reduce the noise of the mask. Due to the stochastic sampling process of diffusion, our model is capable of sampling multiple possible predictions from the mask distribution, avoiding the problem of overconfident point estimation. Moreover, we develop specialized learning strategies that include an innovative ensemble approach for generating robust predictions and tailored forward diffusion methods for efficient training, specifically for the COD task. Extensive experiments on three COD datasets attest the superior performance of our model compared to existing state-of-the-art methods, particularly on the most challenging COD10K dataset, where our approach achieves 0.019 in terms of MAE.},
	urldate = {2023-05-30},
	publisher = {arXiv},
	author = {Chen, Zhongxi and Sun, Ke and Lin, Xianming and Ji, Rongrong},
	month = may,
	year = {2023},
	note = {arXiv:2305.17932 [cs]},
	keywords = {deep\_learning, neural\_nets, camouflage, cocish, segmentation, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Object detection, diffusion},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/YD38QYRM/Chen et al. - 2023 - CamoDiffusion Camouflaged Object Detection via Co.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/XUHKSBL5/2305.html:text/html},
}

@misc{chollet_keras_2015,
    title={Keras},
    author={Chollet, Fran\c{c}ois and others},
    year={2015},
    howpublished={\url{https://keras.io}},
}

@article{chu_camo_image_2010,
    author = {Chu, Hung-Kuo and Hsu, Wei-Hsin and Mitra, Niloy J. and Cohen-Or, Daniel and Wong, Tien-Tsin and Lee, Tong-Yee},
    title = {Camouflage Images},
    year = {2010},
    issue_date = {July 2010},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {29},
    number = {4},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/1778765.1778788},
    doi = {10.1145/1778765.1778788},
    abstract = {Camouflage images contain one or more hidden figures that remain imperceptible or unnoticed for a while. In one possible explanation, the ability to delay the perception of the hidden figures is attributed to the theory that human perception works in two main phases: feature search and conjunction search. Effective camouflage images make feature based recognition difficult, and thus force the recognition process to employ conjunction search, which takes considerable effort and time. In this paper, we present a technique for creating camouflage images. To foil the feature search, we remove the original subtle texture details of the hidden figures and replace them by that of the surrounding apparent image. To leave an appropriate degree of clues for the conjunction search, we compute and assign new tones to regions in the embedded figures by performing an optimization between two conflicting terms, which we call immersion and standout, corresponding to hiding and leaving clues, respectively. We show a large number of camouflage images generated by our technique, with or without user guidance. We have tested the quality of the images in an extensive user study, showing a good control of the difficulty levels.},
    journal = {ACM Trans. Graph.},
    month = {7},
    articleno = {51},
    numpages = {8}
}

@article{conni_visual_2021,
	title = {Visual and data stationarity of texture images},
	volume = {30},
	issn = {1017-9909, 1560-229X},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-30/issue-4/043001/Visual-and-data-stationarity-of-texture-images/10.1117/1.JEI.30.4.043001.full},
	doi = {10.1117/1.JEI.30.4.043001},
	abstract = {The stationarity of a texture can be considered a fundamental property of images, although the property of stationarity is difficult to define precisely. We propose a stationarity test based on multiscale, locally stationary, 2D wavelets. Three separate experiments were performed to evaluate the capabilities and the limitations of this test. The experiments comprised a chessboard stationarity analysis, two classification tasks, and a psychophysical experiment. The classification tasks were performed on 110 texture images from a texture database. In one subtask, five texture feature vectors were extracted from each image and the classification accuracy of two classical methods compared, whereas in the second subtask, the classification accuracy of several methods was compared to the descriptors defined for each image within the database. In the psychophysical experiment, the correlation between the classification results and observer judgements of texture similarity were determined. It was found that a combination of wavelet shrinkage and rotation-invariant local binary pattern best predicted the observer response. The results show that the proposed stationarity test is able to provide relevant information for texture analysis.},
	number = {4},
	urldate = {2024-05-22},
	journal = {Journal of Electronic Imaging},
	author = {Conni, Michele and Deborah, Hilda and Nussbaum, Peter and Green, Philip},
	month = jul,
	year = {2021},
	note = {Publisher: SPIE},
	keywords = {image, local\_statistical\_properties, stationarity, texture},
	pages = {043001},
	file = {Full Text:/Users/cwr/Zotero/storage/UV55M9FN/Conni et al. - 2021 - Visual and data stationarity of texture images.pdf:application/pdf},
}

@book{cott_adaptive_1940,
	address = {London},
	title = {Adaptive {Coloration} in {Animals}},
	url = {http://books.google.com/books?id=Mlipx90CC8YC},
	publisher = {Methuen and Co},
	author = {Cott, Hugh B.},
	year = {1940},
	keywords = {camouflage, coc, cocish, history, survey},
}

@inproceedings{cramer_representation_1985,
	address = {Carnegie-Mellon University, Pittsburgh, PA, USA},
	title = {A {Representation} for the {Adaptive} {Generation} of {Simple} {Sequential} {Programs}},
	shorttitle = {{ICGA} 1985},
	abstract = {An adaptive system for generating short sequential computer functions is described. The created functions are written in the simple "number-string" language JB, and in TB, a modified version of JB with a tree-like structure. These languages have the feature that they can be used to represent well-formed, useful computer programs while still being amenable to suitably defined genetic operators. The system is used to produce two-input, single-output multiplication functions that are concise and well-defined. Future work, dealing with extensions to more complicated functions and generalizations of the techniques, is also discussed.},
	booktitle = {Proceedings of an {International} {Conference} {On} {Genetic} {Algorithms} {And} {Their} {Applications}},
	publisher = {Lawrence Erlbaum Associates},
	author = {Cramer, Nichael Lynn},
	month = jul,
	year = {1985},
	keywords = {algorithm, evolution, ga, Genetic programming, gp, history, induction, optimization, programming},
	pages = {183--187},
	file = {Cramer - 1985 - A Representation for the Adaptive Generation of Si.pdf:/Users/cwr/Zotero/storage/DNLKQ4TB/Cramer - 1985 - A Representation for the Adaptive Generation of Si.pdf:application/pdf},
}

@article{cuthill_camouflage_2019,
	title = {Camouflage},
	volume = {308},
	copyright = {© 2019 The Zoological Society of London},
	issn = {1469-7998},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jzo.12682},
	doi = {10.1111/jzo.12682},
	abstract = {Animal camouflage has long been used to illustrate the power of natural selection, and provides an excellent testbed for investigating the trade-offs affecting the adaptive value of colour. However, the contemporary study of camouflage extends beyond evolutionary biology, co-opting knowledge, theory and methods from sensory biology, perceptual and cognitive psychology, computational neuroscience and engineering. This is because camouflage is an adaptation to the perception and cognition of the species (one or more) from which concealment is sought. I review the different ways in which camouflage manipulates and deceives perceptual and cognitive mechanisms, identifying how, and where in the sequence of signal processing, strategies such as transparency, background matching, disruptive coloration, distraction marks, countershading and masquerade have their effects. As such, understanding how camouflage evolves and functions not only requires an understanding of animal sensation and cognition, it sheds light on perception in other species.},
	language = {en},
	number = {2},
	urldate = {2023-09-02},
	journal = {Journal of Zoology},
	author = {Cuthill, I. C.},
	year = {2019},
	keywords = {camouflage, cocish, crypsis, evolution, coevolution, pattern, perception, survey, defensive coloration, animal coloration, visual perception, predator-prey interactions},
	pages = {75--92},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/M53A3G98/Cuthill - 2019 - Camouflage.pdf:application/pdf;Snapshot:/Users/cwr/Zotero/storage/MERD6FGM/jzo.html:text/html},
}

@article{de_alcantara_viana_predator_2022,
	title = {Predator responses to prey camouflage strategies: a meta-analysis},
	volume = {289},
	shorttitle = {Predator responses to prey camouflage strategies},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2022.0980},
	doi = {10.1098/rspb.2022.0980},
	abstract = {Although numerous studies about camouflage have been conducted in the last few decades, there is still a significant gap in our knowledge about the magnitude of protective value of different camouflage strategies in prey detection and survival. Furthermore, the functional significance of several camouflage strategies remains controversial. Here we carried out a comprehensive meta-analysis including comparisons of different camouflage strategies as well as predator and prey types, considering two response variables: mean predator search time (ST) (63 studies) and predator attack rate (AR) of camouflaged prey (28 studies). Overall, camouflage increased the predator ST by 62.56\% and decreased the AR of prey by 27.34\%. Masquerade was the camouflage strategy that most increased predator ST (295.43\%). Background matching and disruptive coloration did not differ from each other. Motion camouflage did not increase ST but decreases AR on prey. We found no evidence that eyespot increases ST and decreases AR by predators. The different types of predators did not differ from each other, but caterpillars were the type of prey that most influenced the magnitude of camouflage's effect. We highlight the potential evolutionary mechanisms that led camouflage to be a highly effective anti-predatory adaptation, as well as potential discrepancies or redundancies among strategies, predator and prey types.},
	number = {1982},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {de Alcantara Viana, João Vitor and Vieira, Camila and Duarte, Rafael Campos and Romero, Gustavo Quevedo},
	month = {9},
	year = {2022},
	note = {Publisher: Royal Society},
	keywords = {behavior, cocish, evolution, predator, prey, detection, survey, predator-prey interactions, disruptive coloration, antipredator behaviour, background-matching, concealment, masquerade},
	pages = {20220980},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/92ACYCMY/de Alcantara Viana et al. - 2022 - Predator responses to prey camouflage strategies .pdf:application/pdf},
}

@book{dawkins_blind_1986,
	address = {New York, NY, USA},
	title = {The {Blind} {Watchmaker}},
	isbn = {0-393-31570-3},
	url = {https://wwnorton.com/books/The-Blind-Watchmaker/},
	abstract = {One of the most famous arguments of the creationist theory of the universe is the eighteenth-century theologian William Paley's: Just as a watch is too complicated and too functional to have sprung into existence by accident, so too must all living things, with their far greater complexity, be purposefully designed. But as Richard Dawkins, professor of zoology at Oxford University, demonstrates in this brilliant and eloquent riposte to the Argument from Design, the analogy is false. Natural selection, the unconscious, automatic, blind yet essentially non-random process that Darwin discovered, has no purpose in mind. If it can be said to play the role of watchmaker in nature, it is the blind watchmaker. Patiently and lucidly, Dr. Dawkins - in this book which has been acclaimed as perhaps the most influential work on evolution written in this century - identifies those aspects of the theory which people find hard to believe and removes the barrier to credibility one by one.},
	publisher = {W. W. Norton \& Company, Inc.},
	author = {Dawkins, Richard},
	month = {9},
	year = {1986},
	keywords = {evolution, humanfitnessfunction},
}

@article{de_gomensoro_malheiros_leopard_2020,
	title = {The leopard never changes its spots: realistic pigmentation pattern formation by coupling tissue growth with reaction-diffusion},
	volume = {39},
	issn = {0730-0301},
	shorttitle = {The leopard never changes its spots},
	url = {https://doi.org/10.1145/3386569.3392478},
	doi = {10.1145/3386569.3392478},
	abstract = {Previous research in pattern formation using reaction-diffusion mostly focused on static domains, either for computational simplicity or mathematical tractability. In this work, we have explored the expressiveness of combining simple mechanisms as a possible explanation for pigmentation pattern formation, where tissue growth plays a crucial role. Our motivation is not only to realistically reproduce natural patterns but also to get insights into the underlying biological processes. Therefore, we present a novel approach to generate realistic animal skin patterns. First, we describe the approximation of tissue growth by a series of discrete matrix expansion operations. Then, we combine it with an adaptation of Turing's non-linear reaction-diffusion model, which enforces upper and lower bounds to the concentrations of the involved chemical reagents. We also propose the addition of a single-reagent continuous autocatalytic reaction, called reinforcement, to provide a mechanism to maintain an already established pattern during growth. By careful adjustment of the parameters and the sequencing of operations, we closely match the appearance of a few real species. In particular, we reproduce in detail the distinctive features of the leopard skin, also providing a hypothesis for the simultaneous productions of the most common melanin types, eumelanin and pheomelanin.},
	number = {4},
	urldate = {2022-10-23},
	journal = {ACM Transactions on Graphics},
	author = {De Gomensoro Malheiros, Marcelo and Fensterseifer, Henrique and Walter, Marcelo},
	month = jul,
	year = {2020},
	keywords = {natural phenomena, pattern formation, reaction-diffusion, texturing, turing model},
	pages = {63:63:1--63:62:14},
}


@article{endler_natural_1980,
	title = {Natural {Selection} on {Color} {Patterns} in {Poecilia} reticulata},
	volume = {34},
	issn = {0014-3820},
	url = {https://www.jstor.org/stable/2408316},
	doi = {10.2307/2408316},
	abstract = {06},
	number = {1},
	urldate = {2024-02-25},
	journal = {Evolution},
	author = {Endler, John A.},
	year = {1980},
	note = {Publisher: [Society for the Study of Evolution, Wiley]},
	pages = {76--91},
	file = {Endler - 1980 - Natural Selection on Color Patterns in Poecilia re.pdf:/Users/cwr/Zotero/storage/L8CTTJLM/Endler - 1980 - Natural Selection on Color Patterns in Poecilia re.pdf:application/pdf},
}

@article{endler_framework_2012,
	title = {A framework for analysing colour pattern geometry: adjacent colours},
	volume = {107},
	issn = {0024-4066},
	shorttitle = {A framework for analysing colour pattern geometry},
	url = {https://doi.org/10.1111/j.1095-8312.2012.01937.x},
	doi = {10.1111/j.1095-8312.2012.01937.x},
	abstract = {The analysis of colour pattern geometry is not as well advanced as the analysis of colour, although this reflects a lack of an analytical framework. The present study proposes an approach based on a consideration of which colours are adjacent to each other. Both vertebrate and invertebrate eyes do not take static images of the world but move across the field of view. As a consequence, the eye takes transects across the field of view responding to the colours and luminances within patches and to the colour and/or luminance transitions between patches. The framework and methods suggested here are based upon transects across colour patterns and make it possible to estimate colour pattern parameters that capture not only the relative areas of each patch class, but also the relative frequencies of colour/luminance transitions or adjacency. This allows tests of new hypotheses about colour patterns at the same time as including colour, pattern, and texture. Eleven groups of predictions are made with respect to the often conflicting needs of communication with conspecifics, avoiding predation, and finding food. New phenomena may be discovered as a result of these methods and predictions. For example, certain colour transitions may be used for species recognition even though the same colours are used by all species.},
	number = {2},
	urldate = {2024-02-25},
	journal = {Biological Journal of the Linnean Society},
	author = {Endler, John A.},
	month = oct,
	year = {2012},
	keywords = {aposematic, camouflage, coc, cocish, coloration, crypsis, geometry, mimicry, pattern, texture\_analysis},
	pages = {233--253},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/BQK6X4ZX/Endler - 2012 - A framework for analysing colour pattern geometry.pdf:application/pdf;Snapshot:/Users/cwr/Zotero/storage/HH8EU5FI/2701487.html:text/html},
}

@incollection{endler_predators_1978,
	address = {Boston, MA},
	series = {Evolutionary {Biology}},
	title = {A {Predator}’s {View} of {Animal} {Color} {Patterns}},
	isbn = {978-1-4615-6956-5},
	url = {https://doi.org/10.1007/978-1-4615-6956-5_5},
	abstract = {It has long been known that the general colors and tones of animals tend to match their backgrounds (E. Darwin, 1794; Poulton, 1890). The adaptive significance of this has been borne out in numerous experimental studies (DiCesnola, 1904; Sumner, 1934, 1935; Isley, 1938; Popham, 1942; Dice, 1947; Turner, 1961; Kettlewell, 1956, 1973; Kaufman, 1974; Wiklund, 1975; Curio, 1976). There is also a good understanding of warning coloration (Cott, 1940; Wickler, 1968; Edmunds, 1974; Rothschild, 1975). However, the determinants of color pattern are poorly known, although it is known in a general way that the patterns and forms of animals are similar to their backgrounds (Poulton, 1890; Thayer, 1909; Cott, 1940; Wickler, 1968; Robinson, 1969; Edmunds, 1974; Fogden and Fogden, 1974). It is the purpose of this paper to explore the factors that determine color patterns under various specific conditions. The basic assumption is that a color pattern must resemble a random sample of the background seen by predators in order to be cryptic, and must deviate from the background in one or more ways in order to be conspicuous. As a result, the actual pattern evolved in a particular place represents a compromise between factors which favor crypsis and those which favor conspicuous color patterns.},
	language = {en},
	urldate = {2024-02-25},
	booktitle = {Evolutionary {Biology}},
	publisher = {Springer US},
	author = {Endler, John A.},
	editor = {Hecht, Max K. and Steere, William C. and Wallace, Bruce},
	year = {1978},
	doi = {10.1007/978-1-4615-6956-5_5},
	keywords = {camouflage, coc, cocish, Color Patch, Color Pattern, Color Vision, Patch Size, Sexual Selection},
	pages = {319--364},
	file = {Endler - 1978 - A Predator’s View of Animal Color Patterns.pdf:/Users/cwr/Zotero/storage/YBY5WB58/Endler - 1978 - A Predator’s View of Animal Color Patterns.pdf:application/pdf},
}

@misc{gao_nerf_2022,
	title = {{NeRF}: {Neural} {Radiance} {Field} in {3D} {Vision}, {A} {Comprehensive} {Review}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2210.00379},
    howpublished={\url{http://arxiv.org/abs/2210.00379}},
	doi = {10.48550/arXiv.2210.00379},
	abstract = {Neural Radiance Field (NeRF), a new novel view synthesis with implicit scene representation has taken the field of Computer Vision by storm. As a novel view synthesis and 3D reconstruction method, NeRF models find applications in robotics, urban mapping, autonomous navigation, virtual reality/augmented reality, and more. Since the original paper by Mildenhall et al., more than 250 preprints were published, with more than 100 eventually being accepted in tier one Computer Vision Conferences. Given NeRF popularity and the current interest in this research area, we believe it necessary to compile a comprehensive survey of NeRF papers from the past two years, which we organized into both architecture, and application based taxonomies. We also provide an introduction to the theory of NeRF based novel view synthesis, and a benchmark comparison of the performance and speed of key NeRF models. By creating this survey, we hope to introduce new researchers to NeRF, provide a helpful reference for influential works in this field, as well as motivate future research directions with our discussion section.},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Gao, Kyle and Gao, Yina and He, Hongjie and Lu, Denning and Xu, Linlin and Li, Jonathan},
	month = nov,
	year = {2022},
	note = {arXiv:2210.00379 [cs]},
	keywords = {3d, Computer Science - Computer Vision and Pattern Recognition, deep neural networks, image based rendering, nerf, survey, vision},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/9ZL5759C/Gao et al. - 2022 - NeRF Neural Radiance Field in 3D Vision, A Compre.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/EUHBEERH/2210.html:text/html},
}

@book{glassner_deep_2021,
	address = {San Francisco, CA},
	title = {Deep {Learning}: {A} {Visual} {Approach}},
	isbn = {978-1-71850-072-3},
	url = {https://nostarch.com/deep-learning-visual-approach},
	publisher = {No Starch Press},
	author = {Glassner, Andrew},
	year = {2021},
	keywords = {classification, deep\_learning, deeplearning, gan, learning, machinelearning, neural\_nets, optimization, regression, statistics, tutorial, visualization},
}

@misc{goodfellow_gan_2014,
    doi = {10.48550/ARXIV.1406.2661},
    url = {https://arxiv.org/abs/1406.2661},
    howpublished={\url{https://arxiv.org/abs/1406.2661}},
    author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Generative Adversarial Networks},
    publisher = {arXiv},
    year = {2014},
    copyright = {arXiv.org perpetual, non-exclusive license},
    note = {Republished in 2020 in CACM: https://doi.org/10.1145/3422622}
}

@article{Guerrero_MatFormer_2022,
    author = {Guerrero, Paul and Ha\v{s}an, Milo\v{s} and Sunkavalli, Kalyan and M\v{e}ch, Radom\'{\i}r and Boubekeur, Tamy and Mitra, Niloy J.},
    title = {MatFormer: A Generative Model for Procedural Materials},
    year = {2022},
    issue_date = {July 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {41},
    number = {4},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/3528223.3530173},
    doi = {10.1145/3528223.3530173},
    abstract = {Procedural material graphs are a compact, parameteric, and resolution-independent representation that are a popular choice for material authoring. However, designing procedural materials requires significant expertise and publicly accessible libraries contain only a few thousand such graphs. We present MatFormer, a generative model that can produce a diverse set of high-quality procedural materials with complex spatial patterns and appearance. While procedural materials can be modeled as directed (operation) graphs, they contain arbitrary numbers of heterogeneous nodes with unstructured, often long-range node connections, and functional constraints on node parameters and connections. MatFormer addresses these challenges with a multi-stage transformer-based model that sequentially generates nodes, node parameters, and edges, while ensuring the semantic validity of the graph. In addition to generation, MatFormer can be used for the auto-completion and exploration of partial material graphs. We qualitatively and quantitatively demonstrate that our method outperforms alternative approaches, in both generated graph and material quality.},
    journal = {ACM Trans. Graph.},
    month = {7},
    articleno = {46},
    numpages = {12},
    keywords = {transformers, node graphs, procedural materials, generative models}
    }

@misc{guo_ganmouflage_2022,
	title = {{GANmouflage}: {3D} Object Nondetection with Texture Fields},
	shorttitle = {{GANmouflage}},
	url = {http://arxiv.org/abs/2201.07202},
    howpublished={\url{http://arxiv.org/abs/2201.07202}},
	abstract = {We propose a method that learns to camouflage 3D objects within scenes. Given an object's shape and a distribution of viewpoints from which it will be seen, we estimate a texture that will make it difficult to detect. Successfully solving this task requires a model that can accurately reproduce textures from the scene, while simultaneously dealing with the highly conflicting constraints imposed by each viewpoint. We address these challenges with a model based on texture fields and adversarial learning. Our model learns to camouflage a variety of object shapes from randomly sampled locations and viewpoints within the input scene, and is the first to address the problem of hiding complex object shapes. Using a human visual search study, we find that our estimated textures conceal objects significantly better than previous methods. Project site: https://rrrrrguo.github.io/ganmouflage/},
	urldate = {2022-01-23},
	journal = {arXiv:2201.07202 [cs]},
	author = {Guo, Rui and Collins, Jasmine and de Lima, Oscar and Owens, Andrew},
    month = {1},
	year = {2022},
	note = {arXiv: 2201.07202},
	keywords = {camouflage, cocish, 3d, texture\_synthesis, human, detection, Computer Science - Computer Vision and Pattern Recognition, anti\_camouflage, multi\_view},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/VKQU27CT/Guo et al. - 2022 - GANmouflage 3D Object Nondetection with Texture F.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/VPPJU2P2/2201.html:text/html},
    numpages = {16},
    eprinttype = {arXiv},
    eprint={2201.07202},
}

@article{hancock_camoevo_2022,
    author = {Hancock, George R. A. and Troscianko, Jolyon},
    title = {CamoEvo: An open access toolbox for artificial camouflage evolution experiments},
    journal = {Evolution},
    volume = {76},
    number = {5},
    pages = {870-882},
    keywords = {CamoEvo, camouflage, evolution, genetic algorithms, optimization, selection},
    doi = {https://doi.org/10.1111/evo.14476},
    abstract = {Abstract Camouflage research has long shaped our understanding of evolution by natural selection, and elucidating the mechanisms by which camouflage operates remains a key question in visual ecology. However, the vast diversity of color patterns found in animals and their backgrounds, combined with the scope for complex interactions with receiver vision, presents a fundamental challenge for investigating optimal camouflage strategies. Genetic algorithms (GAs) have provided a potential method for accounting for these interactions, but with limited accessibility. Here, we present CamoEvo, an open-access toolbox for investigating camouflage pattern optimization by using tailored GAs, animal and egg maculation theory, and artificial predation experiments. This system allows for camouflage evolution within the span of just 10–30 generations (∼1–2 min per generation), producing patterns that are both significantly harder to detect and that are optimized to their background. CamoEvo was built in ImageJ to allow for integration with an array of existing open access camouflage analysis tools. We provide guides for editing and adjusting the predation experiment and GA as well as an example experiment. The speed and flexibility of this toolbox makes it adaptable for a wide range of computer-based phenotype optimization experiments.},
    year = {2022}
}

@inproceedings{harrington_coevolution_2014,
	author = {Harrington, Kyle I. and Freeman, Jesse and Pollack, Jordan},
    title = "{Coevolution in Hide and Seek: Camouflage and Vision}",
    booktitle = {ALIFE 14: The Fourteenth International Conference on the Synthesis and Simulation of Living Systems},
    address = {New York City, NY, USA},
    pages = {25-32},
    year = {2014},
    month = {07},
    publisher = {MIT Press},
    url = {https://direct.mit.edu/isal/proceedings/alife2014/25/98754},
}

@incollection{holland_genetic_1984,
	address = {Boston, MA},
	title = {Genetic {Algorithms} and {Adaptation}},
	isbn = {978-1-4684-8941-5},
	url = {https://doi.org/10.1007/978-1-4684-8941-5_21},
	abstract = {Genetics provides us with a canonical example of a complex search through a space of ill-defined possibilities. The basic problem is one of manipulating representations — the chromosomes — so as to search out and generate useful organization — the functional properties of the organism.},
	language = {en},
	urldate = {2024-05-30},
	booktitle = {Adaptive {Control} of {Ill}-{Defined} {Systems}},
	publisher = {Springer US},
	author = {Holland, John H.},
	editor = {Selfridge, Oliver G. and Rissland, Edwina L. and Arbib, Michael A.},
	year = {1984},
	doi = {10.1007/978-1-4684-8941-5_21},
	pages = {317--333},
}

@misc{hu_shifting_2024,
	title = {Shifting {Spotlight} for {Co}-supervision: {A} {Simple} yet {Efficient} {Single}-branch {Network} to {See} {Through} {Camouflage}},
	shorttitle = {Shifting {Spotlight} for {Co}-supervision},
	url = {http://arxiv.org/abs/2404.08936},
	doi = {10.48550/arXiv.2404.08936},
	abstract = {Efficient and accurate camouflaged object detection (COD) poses a challenge in the field of computer vision. Recent approaches explored the utility of edge information for network co-supervision, achieving notable advancements. However, these approaches introduce an extra branch for complex edge extraction, complicate the model architecture and increases computational demands. Addressing this issue, our work replicates the effect that animal's camouflage can be easily revealed under a shifting spotlight, and leverages it for network co-supervision to form a compact yet efficient single-branch network, the Co-Supervised Spotlight Shifting Network (CS\${\textasciicircum}3\$Net). The spotlight shifting strategy allows CS\${\textasciicircum}3\$Net to learn additional prior within a single-branch framework, obviating the need for resource demanding multi-branch design. To leverage the prior of spotlight shifting co-supervision, we propose Shadow Refinement Module (SRM) and Projection Aware Attention (PAA) for feature refinement and enhancement. To ensure the continuity of multi-scale features aggregation, we utilize the Extended Neighbor Connection Decoder (ENCD) for generating the final predictions. Empirical evaluations on public datasets confirm that our CS\${\textasciicircum}3\$Net offers an optimal balance between efficiency and performance: it accomplishes a 32.13\% reduction in Multiply-Accumulate (MACs) operations compared to leading efficient COD models, while also delivering superior performance.},
	publisher = {arXiv},
	author = {Hu, Yang and Zhang, Jinxia and Zhang, Kaihua and Yuan, Yin},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08936 [cs]},
	keywords = {camouflage, cocish, image\_processing, Computer Science - Computer Vision and Pattern Recognition, COD, shadow, supervision, supervised\_learning},
}

@article{hughes_imperfect_2019,
	title = {Imperfect camouflage: how to hide in a variable world?},
	volume = {286},
	shorttitle = {Imperfect camouflage},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2019.0646},
	doi = {10.1098/rspb.2019.0646},
	abstract = {Camouflage is an important anti-predator strategy for many animals and is traditionally thought of as being tightly linked to a specific visual background. While much work focuses on optimizing camouflage against one background, this may not be relevant for many species and contexts, as animals may encounter many different habitats throughout their lives due to temporal and spatial variation in their environment. How should camouflage be optimized when an animal or object is seen against multiple visual backgrounds? Various solutions may exist, including colour change to match new environments or use of behaviour to maintain crypsis by choosing appropriate substrates. Here, we focus on a selection of approaches under a third alternative strategy: animals may adopt (over evolution) camouflage appearances that represent an optimal solution against multiple visual scenes. One approach may include a generalist or compromise strategy, where coloration matches several backgrounds to some extent, but none closely. A range of other camouflage types, including disruptive camouflage, may also provide protection in multiple environments. Despite detailed theoretical work determining the plausibility of compromise camouflage and elucidating the conditions under which it might evolve, there is currently mixed experimental evidence supporting its value and little evidence of it in natural systems. In addition, there remain many questions including how camouflage strategies should be defined and optimized, and how they might interact with other types of crypsis and defensive markings. Overall, we provide a critical overview of our current knowledge about how camouflage can enable matching to multiple backgrounds, discuss important challenges of working on this question and make recommendations for future research.},
	number = {1902},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Hughes, Anna and Liggins, Eric and Stevens, Martin},
	month = may,
	year = {2019},
	note = {Publisher: Royal Society},
	keywords = {anti-predator coloration, artificial\_prey, camouflage, cocish, disruption, environment, generalist, simulation, specialist, varying, vision},
	pages = {20190646},
}

@article{ishibuchi_difficulties_2022,
	title = {Difficulties in {Fair} {Performance} {Comparison} of {Multi}-{Objective} {Evolutionary} {Algorithms}},
	volume = {17},
	issn = {1556-6048},
	url = {https://ieeexplore.ieee.org/document/9679762},
	doi = {10.1109/MCI.2021.3129961},
	abstract = {The performance of a newly designed evolutionary algorithm is usually evaluated by computational experiments in comparison with existing algorithms. However, comparison results depend on experimental setting; thus, fair comparison is difficult. Fair comparison of multi-objective evolutionary algorithms is even more difficult since solution sets instead of solutions are evaluated. In this paper, the following four issues are discussed for fair comparison of multi-objective evolutionary algorithms: (i) termination condition, (ii) population size, (iii) performance indicators, and (iv) test problems. Whereas many other issues related to computational experiments such as the choice of a crossover operator and the specification of its probability can be discussed for each algorithm separately, all the above four issues should be addressed for all algorithms simultaneously. For each issue, its strong effects on comparison results are first clearly demonstrated. Then, the handling of each issue for fair comparison is discussed. Finally, future research topics related to each issue are suggested.},
	number = {1},
	urldate = {2024-02-28},
	journal = {IEEE Computational Intelligence Magazine},
	author = {Ishibuchi, Hisao and Pang, Lie Meng and Shang, Ke},
	month = feb,
	year = {2022},
	keywords = {algorithm, comparison, Convergence, evolution, Evolutionary computation, ga, gp, multi\_objective, Optimization, Robustness, Social factors, Statistics},
	pages = {86--101},
	file = {IEEE Xplore Abstract Record:/Users/cwr/Zotero/storage/WYRLTITK/9679762.html:text/html;Ishibuchi et al. - 2022 - Difficulties in fair performance comparison of mul.pdf:/Users/cwr/Zotero/storage/NY8DHEE9/Ishibuchi et al. - 2022 - Difficulties in fair performance comparison of mul.pdf:application/pdf},
}

@inproceedings{jaskowski_fitnessless_2008,
	address = {New York, NY, USA},
	series = {{GECCO} '08},
	title = {Fitnessless coevolution},
	isbn = {978-1-60558-130-9},
	url = {https://doi.org/10.1145/1389095.1389161},
	doi = {10.1145/1389095.1389161},
	abstract = {We introduce fitnessless coevolution (FC), a novel method of comparative one-population coevolution. FC plays games between individuals to settle tournaments in the selection phase and skips the typical phase of evaluation. The selection operator applies a single-elimination tournament to a randomly drawn group of individuals, and the winner of the final round becomes the result of selection. Therefore, FC does not involve explicit fitness measure. We prove that, under a condition of transitivity of the payoff matrix, the dynamics of FC is identical to that of the traditional evolutionary algorithm. The experimental results, obtained on a diversified group of problems, demonstrate that FC is able to produce solutions that are equally good or better than solutions obtained using fitness-based one-population coevolution with different selection methods.},
	urldate = {2023-06-17},
	booktitle = {Proceedings of the 10th annual conference on {Genetic} and evolutionary computation},
	publisher = {Association for Computing Machinery},
	author = {Jaśkowski, Wojciech and Krawiec, Krzysztof and Wieloch, Bartosz},
	month = jul,
	year = {2008},
	keywords = {cocish, evolution, games, ga, coevolution, gp, evolutionary computation, one-population coevolution, selection methods, relative\_fitness},
	pages = {355--362},
	file = {Jaśkowski et al. - 2008 - Fitnessless coevolution.pdf:/Users/cwr/Zotero/storage/FUPD6M9F/Jaśkowski et al. - 2008 - Fitnessless coevolution.pdf:application/pdf},
}

@article{kashtan_varying_2007,
	title = {Varying environments can speed up evolution},
	volume = {104},
	issn = {1091-6490},
	url = {http://dx.doi.org/10.1073/pnas.0611630104},
	doi = {10.1073/pnas.0611630104},
	abstract = {Simulations of biological evolution, in which computers are used to evolve systems toward a goal, often require many generations to achieve even simple goals. It is therefore of interest to look for generic ways, compatible with natural conditions, in which evolution in simulations can be speeded. Here, we study the impact of temporally varying goals on the speed of evolution, defined as the number of generations needed for an initially random population to achieve a given goal. Using computer simulations, we find that evolution toward goals that change over time can, in certain cases, dramatically speed up evolution compared with evolution toward a fixed goal. The highest speedup is found under modularly varying goals, in which goals change over time such that each new goal shares some of the subproblems with the previous goal. The speedup increases with the complexity of the goal: the harder the problem, the larger the speedup. Modularly varying goals seem to push populations away from local fitness maxima, and guide them toward evolvable and modular solutions. This study suggests that varying environments might significantly contribute to the speed of natural evolution. In addition, it suggests a way to accelerate optimization algorithms and improve evolutionary approaches in engineering.},
	number = {34},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kashtan, Nadav and Noor, Elad and Alon, Uri},
	month = aug,
	year = {2007},
	pmid = {17698964},
	keywords = {cocish, environment, evolution, algorithm, optimization, ga, gp, varying, modularity},
	pages = {13711--13716},
}

@article{kimura_evolutionary_1968,
	title = {Evolutionary rate at the molecular level.},
	volume = {217},
	issn = {00280836},
	url = {http://coleoguy.github.io/reading.group/kimura.pdf},
	abstract = {Calculating the rate of evolution in terms of nucleotide substitutions seems to give a value so high that many of the mutations involved must be neutral ones.},
	number = {129},
	journal = {Nature insight : aids.},
	author = {Kimura, Motoo},
	month = jan,
	year = {1968},
	pages = {624--626},
	publisher = {Nature Publishing},
	address = {London.},
}

@article{kelley_role_2023,
	title = {The role of pictorial cues and contrast for camouflage},
	issn = {1573-8477},
	url = {https://doi.org/10.1007/s10682-023-10267-z},
	doi = {10.1007/s10682-023-10267-z},
	abstract = {Shadows that are produced across the surface of an object (self-shadows) are potentially an important source of information for visual systems. Animal patterns may exploit this principle for camouflage, using pictorial cues to produce false depth information that manipulates the viewer’s detection/recognition processes. However, pictorial cues could also facilitate camouflage by matching the contrast (e.g. due to shadows) of 3D backgrounds. Aside from studies of countershading (patterning that may conceal depth information), the role of self-shadows in camouflage patterns remains unclear. Here we investigated whether pictorial cues (self-shadows) increase the survival probability of moth-like prey presented to free-living wild bird predators relative to targets without these cues. We manipulated the presence of self-shadows by adjusting the illumination conditions to produce patterned targets under directional lighting (lit from above or from below; self-shadows present) or diffuse lighting (no self-shadows). We used non-patterned targets (uniform colour) as controls. We manipulated the direction of illumination because it has been linked with depth perception in birds; objects lit from above may appear convex while those lit from below can appear concave. As shadows influence contrast, which also determines detectability, we photographed the targets in situ over the observation period, allowing us to evaluate the effect of visual metrics on survival. We found some evidence that patterned targets without self-shadows had a lower probability of survival than patterned targets with self-shadows and targets with uniform colour. Surprisingly, none of the visual metrics explained variation in survival probability. However, predators increased their foraging efficiency over time, suggesting that predator learning may have overridden the benefits afforded by camouflaging coloration.},
	language = {en},
	urldate = {2023-10-18},
	journal = {Evolutionary Ecology},
	author = {Kelley, Jennifer L. and Jessop, Anna-Lee and Kelley, Laura A. and Troscianko, Jolyon},
	month = oct,
	year = {2023},
	keywords = {camouflage, 3d, artificial\_prey, coc, Background matching, Depth perception, Disruptive coloration, Shape recognition, Shape-from-shading, Visual perception, shadow},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/PP2QLKTX/Kelley et al. - 2023 - The role of pictorial cues and contrast for camouf.pdf:application/pdf},
}


@book{koza_genetic_1992,
	address = {Cambridge, Mass.},
	edition = {1},
	title = {Genetic {Programming}: {On} the {Programming} of {Computers} by {Means} of {Natural} {Selection} ({Complex} {Adaptive} {Systems})},
	isbn = {0-262-11170-5},
	url = {https://mitpress.mit.edu/9780262527910/genetic-programming/},
	publisher = {MIT Press, A Bradford Book},
	author = {Koza, John R.},
	month = dec,
	year = {1992},
	keywords = {evolution, gp},
}

@inbook{latham_form_1989,
    author = {Latham, William},
    title = {Form Synth: The Rule-Based Evolution of Complex Forms from Geometric Primitives},
    year = {1989},
    isbn = {0387968962},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    booktitle = {Computers in Art, Design and Animation},
    pages = {80–108},
    numpages = {29}
    }

@article{livnat_sex_2016,
	title = {Sex {As} an {Algorithm}: {The} {Theory} of {Evolution} {Under} the {Lens} of {Computation}},
	volume = {59},
	issn = {0001-0782},
	url = {http://dx.doi.org/10.1145/2934662},
	doi = {10.1145/2934662},
	abstract = {Looking at the mysteries of evolution from a computer science point of view yields some unexpected insights.},
	number = {11},
	journal = {Commun. ACM},
	author = {Livnat, Adi and Papadimitriou, Christos},
	month = oct,
	year = {2016},
	keywords = {simulation, cocish, evolution, algorithm, optimization, genetics, computation, drift, pac, theory, weak\_selection},
	pages = {84--93},
	file = {Livnat and Papadimitriou - 2016 - Sex As an Algorithm The Theory of Evolution Under.pdf:/Users/cwr/Zotero/storage/F747KDHT/Livnat and Papadimitriou - 2016 - Sex As an Algorithm The Theory of Evolution Under.pdf:application/pdf},
}

@misc{lv_cod_2022,
    doi = {10.48550/ARXIV.2205.11333},
    url = {https://arxiv.org/abs/2205.11333},
    howpublished={\url{https://arxiv.org/abs/2205.11333}},
    author = {Lv, Yunqiu and Zhang, Jing and Dai, Yuchao and Li, Aixuan and Barnes, Nick and Fan, Deng-Ping},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Towards Deeper Understanding of Camouflaged Object Detection},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Mckay_2010,
    author = {Mckay, Robert I. and Hoai, Nguyen Xuan and Whigham, Peter Alexander and Shan, Yin and O'Neill, Michael},
    title = {Grammar-Based Genetic Programming: A Survey},
    year = {2010},
    issue_date = {September 2010},
    publisher = {Kluwer Academic Publishers},
    address = {USA},
    volume = {11},
    number = {3–4},
    issn = {1389-2576},
    url = {https://doi.org/10.1007/s10710-010-9109-y},
    doi = {10.1007/s10710-010-9109-y},
    abstract = {Grammar formalisms are one of the key representation structures in Computer Science. So it is not surprising that they have also become important as a method for formalizing constraints in Genetic Programming (GP). Practical grammar-based GP systems first appeared in the mid 1990s, and have subsequently become an important strand in GP research and applications. We trace their subsequent rise, surveying the various grammar-based formalisms that have been used in GP and discussing the contributions they have made to the progress of GP. We illustrate these contributions with a range of applications of grammar-based GP, showing how grammar formalisms contributed to the solutions of these problems. We briefly discuss the likely future development of grammar-based GP systems, and conclude with a brief summary of the field.},
    journal = {Genetic Programming and Evolvable Machines},
    month = {9},
    pages = {365–396},
    numpages = {32},
    keywords = {Regular, Tree adjoining, Evolutionary computation, Grammar, Genetic programming, Context free}
}

@article{miller_color_2022,
	title = {Color in motion: {Generating} 3-dimensional multispectral models to study dynamic visual signals in animals},
	volume = {10},
	issn = {2296-701X},
	shorttitle = {Color in motion},
	url = {https://www.frontiersin.org/articles/10.3389/fevo.2022.983369},
	abstract = {Analyzing color and pattern in the context of motion is a central and ongoing challenge in the quantification of animal coloration. Many animal signals are spatially and temporally variable, but traditional methods fail to capture this dynamism because they use stationary animals in fixed positions. To investigate dynamic visual displays and to understand the evolutionary forces that shape dynamic colorful signals, we require cross-disciplinary methods that combine measurements of color, pattern, 3-dimensional (3D) shape, and motion. Here, we outline a workflow for producing digital 3D models with objective color information from museum specimens with diffuse colors. The workflow combines multispectral imaging with photogrammetry to produce digital 3D models that contain calibrated ultraviolet (UV) and human-visible (VIS) color information and incorporate pattern and 3D shape. These “3D multispectral models” can subsequently be animated to incorporate both signaler and receiver movement and analyzed in silico using a variety of receiver-specific visual models. This approach—which can be flexibly integrated with other tools and methods—represents a key first step toward analyzing visual signals in motion. We describe several timely applications of this workflow and next steps for multispectral 3D photogrammetry and animation techniques.},
	urldate = {2022-11-19},
	journal = {Frontiers in Ecology and Evolution},
	author = {Miller, Audrey E. and Hogan, Benedict G. and Stoddard, Mary Caswell},
	year = {2022},
	keywords = {3d, animation, camouflage, cocish, coloration, dynamic, in silico, modeling, motion, multispectral, predator, signal, vision},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/IVEHRXRR/Miller et al. - 2022 - Color in motion Generating 3-dimensional multispe.pdf:application/pdf},
    numpages = {19},
}

@article{montana_strongly_1995,
	title = {Strongly {Typed} {Genetic} {Programming}},
	volume = {3},
	url = {http://web.archive.org/web/20070814014654/http://vishnu.bbn.com/papers/stgp.pdf},
	abstract = {Genetic programming is a powerful method for automatically generating computer programs via the process of natural selection (Koza, 1992). However, in its standard form, there is no way to restrict the programs it generates to those where the functions operate on appropriate data types. In the case when the programs manipulate multiple data types and contain functions designed to operate on particular data types, this can lead to unnecessarily large search times and/or unnecessarily poor generalization performance. Strongly typed genetic programming (STGP) is an enhanced version of genetic programming which enforces data type constraints and whose use of generic functions and generic data types makes it more powerful than other approaches to type constraint enforcement. After describing its operation, we illustrate its use on problems in two domains, matrix/vector manipulation and list manipulation, which require its generality. The examples are: (1) the multi-dimensional least-squares regression problem, (2) the multi-dimensional Kalman ﬁlter, (3) the list manipulation function NTH, and (4) the list manipulation function MAPCAR.},
	number = {2},
	journal = {Evolutionary Computation},
	author = {Montana, David J.},
	year = {1995},
	keywords = {self\_organizing, evolution, algorithm, emergence, optimization, gp},
	pages = {199--230},
	annote = {See also July 1993 BBN Technical Report \#7866 of the same name},
}

@article{murray_how_1988,
	title = {How the leopard gets its spots},
	volume = {258},
	abstract = {Mammals exhibit a remarkable variety of coat patterns; the variety has elicited a comparable variety of explanations—many of them at the level of cogency that prevails in Rudyard Kipling's delightful "How the Leopard Got Its Spots." Although genes control the processes involved in coat pattern formation, the actual mechanisms that create the patterns are still not known. It would be attractive from the viewpoint of both evolutionary and developmental biology if a single mechanism were found to produce the enormous assortment of coat patterns found in nature.},
	number = {3},
	journal = {Scientific American},
	author = {Murray, James D.},
	month = mar,
	year = {1988},
	keywords = {camouflage, morphogenesis, texture\_synthesis, reaction-diffusion},
	pages = {80--87},
    doi={10.1038/scientificamerican0388-80}
}

@misc{nguyen_few-shot_2023,
	title = {Few-shot {Camouflaged} {Animal} {Detection} and {Segmentation}},
	url = {http://arxiv.org/abs/2304.07444},
	abstract = {Camouflaged object detection and segmentation is a new and challenging research topic in computer vision. There is a serious issue of lacking data of camouflaged objects such as camouflaged animals in natural scenes. In this paper, we address the problem of few-shot learning for camouflaged object detection and segmentation. To this end, we first collect a new dataset, CAMO-FS, for the benchmark. We then propose a novel method to efficiently detect and segment the camouflaged objects in the images. In particular, we introduce the instance triplet loss and the instance memory storage. The extensive experiments demonstrated that our proposed method achieves state-of-the-art performance on the newly collected dataset.},
	urldate = {2023-05-19},
	publisher = {arXiv},
	author = {Nguyen, Thanh-Danh and Vu, Anh-Khoa Nguyen and Nguyen, Nhat-Duy and Nguyen, Vinh-Tiep and Ngo, Thanh Duc and Do, Thanh-Toan and Tran, Minh-Triet and Nguyen, Tam V.},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07444 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/cwr/Zotero/storage/G4SH3LUT/2304.html:text/html;Full Text PDF:/Users/cwr/Zotero/storage/ZGRE4R5G/Nguyen et al. - 2023 - Few-shot Camouflaged Animal Detection and Segmenta.pdf:application/pdf},
}

@INPROCEEDINGS{owens_camouflaging_2014,
    author={Owens, Andrew and Barnes, Connelly and Flint, Alex and Singh, Hanumant and Freeman, William},
    booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
    title={Camouflaging an Object from Many Viewpoints}, 
    year={2014},
    volume={},
    number={},
    address={Columbus, OH, USA},
    publisher = {IEEE CVPR},
    pages={2782-2789},
    doi={10.1109/CVPR.2014.350}
}

@inproceedings{pang_zoom_2022,
	address = {New Orleans, LA, USA},
	title = {Zoom {In} and {Out}: {A} {Mixed}-scale {Triplet} {Network} for {Camouflaged} {Object} {Detection}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Zoom {In} and {Out}},
	url = {https://ieeexplore.ieee.org/document/9878581/},
	doi = {10.1109/CVPR52688.2022.00220},
	abstract = {The recently proposed camouflaged object detection (COD) attempts to segment objects that are visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from high intrinsic similarity between the camouflaged objects and their background, the objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To deal with these problems, we propose a mixed-scale triplet network, ZoomNet, which mimics the behavior of humans when observing vague images, i.e., zooming in and out. Specifically, our ZoomNet employs the zoom strategy to learn the discriminative mixed-scale semantics by the designed scale integration unit and hierarchical mixed-scale unit, which fully explores imperceptible clues between the candidate objects and background surroundings. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization constraint, uncertainty-aware loss, to promote the model to accurately produce predictions with higher confidence in candidate regions. Without bells and whistles, our proposed highly task-friendly model consistently surpasses the existing 23 state-of-the-art methods on four public datasets. Besides, the superior performance over the recent cuttingedge models on the SOD task also verifies the effectiveness and generality of our model. The code will be available at https://github.com/lartpang/ZoomNet.},
	language = {en},
	urldate = {2022-10-28},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pang, Youwei and Zhao, Xiaoqi and Xiang, Tian-Zhu and Zhang, Lihe and Lu, Huchuan},
	month = jun,
	year = {2022},
	pages = {2150--2160},
	file = {Pang et al. - 2022 - Zoom In and Out A Mixed-scale Triplet Network for.pdf:/Users/cwr/Zotero/storage/99IHVM5Z/Pang et al. - 2022 - Zoom In and Out A Mixed-scale Triplet Network for.pdf:application/pdf},
}

@article{perlin_image_1985,
	title = {An image synthesizer},
	volume = {19},
	issn = {0097-8930},
	url = {http://dx.doi.org/10.1145/325334.325247},
	doi = {10.1145/325334.325247},
	abstract = {We introduce the concept of a Pixel Stream Editor. This forms the basis for an interactive synthesizer for designing highly realistic Computer Generated Imagery. The designer works in an interactive Very High Level programming environment which provides a very fast concept/implement/view iteration cycle. Naturalistic visual complexity is built up by composition of non-linear functions, as opposed to the more conventional texture mapping or growth model algorithms. Powerful primitives are included for creating controlled stochastic effects. We introduce the concept of "solid texture" to the field of CGI.We have used this system to create very convincing representations of clouds, fire, water, stars, marble, wood, rock, soap films and crystal. The algorithms created with this paradigm are generally extremely fast, highly realistic, and asynchronously parallelizable at the pixel level.},
	number = {3},
	journal = {SIGGRAPH '85: Proceedings of the 12th annual conference on Computer graphics and interactive techniques},
	author = {Perlin, Ken},
	month = jul,
	year = {1985},
	keywords = {noise, pattern, texture\_synthesis},
	pages = {287--296},
}


@article{price_background_2019,
	title = {Background matching and disruptive coloration as habitat-specific strategies for camouflage},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-44349-2},
	doi = {10.1038/s41598-019-44349-2},
	abstract = {Camouflage is a key defence across taxa and frequently critical to survival. A common strategy is background matching, resembling the colour and pattern of the environment. This approach, however, may be ineffective in complex habitats where matching one patch may lead to increased visibility in other patches. In contrast, disruptive coloration, which disguises body outlines, may be effective against complex backgrounds. These ideas have rarely been tested and previous work focuses on artificial systems. Here, we test the camouflage strategies of the shore crab (Carcinus maenas) in two habitats, being a species that is highly variable, capable of plastic changes in appearance, and lives in multiple environments. Using predator (bird and fish) vision modelling and image analysis, we quantified background matching and disruption in crabs from rock pools and mudflats, predicting that disruption would dominate in visually complex rock pools but background matching in more uniform mudflats. As expected, rock pool individuals had significantly higher edge disruption than mudflat crabs, whereas mudflat crabs more closely matched the substrate than rock pool crabs for colour, luminance, and pattern. Our study demonstrates facultative expression of camouflage strategies dependent on the visual environment, with implications for the evolution and interrelatedness of defensive strategies.},
	language = {en},
	number = {1},
	journal = {Scientific Reports},
	author = {Price, Natasha and Green, Samuel and Troscianko, Jolyon and Tregenza, Tom and Stevens, Martin},
	month = {5},
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {camouflage, environment, predator, vision, disruptive, crab, coloration, background-matching, Behavioural ecology, Evolutionary ecology, habitat},
	pages = {7840},
}

@article{reynolds_iec_2011,
    author = {Reynolds, Craig},
    title = "{Interactive Evolution of Camouflage}",
    journal = {Artificial Life},
    volume = {17},
    number = {2},
    pages = {123-136},
    year = {2011},
    month = {04},
    abstract = "{This article presents an abstract computation model of the evolution of camouflage in nature. The 2D model uses evolved textures for prey, a background texture representing the environment, and a visual predator. A human observer, acting as the predator, is shown a cohort of 10 evolved textures overlaid on the background texture. The observer clicks on the five most conspicuous prey to remove (“eat”) them. These lower-fitness textures are removed from the population and replaced with newly bred textures. Biological morphogenesis is represented in this model by procedural texture synthesis. Nested expressions of generators and operators form a texture description language. Natural evolution is represented by genetic programming (GP), a variant of the genetic algorithm. GP searches the space of texture description programs for those that appear least conspicuous to the predator.}",
    issn = {1064-5462},
    doi = {10.1162/artl_a_00023},
    url = {https://doi.org/10.1162/artl\_a\_00023},
    eprint = {},
}

@misc{reynolds_texsyn_2019,
    author = {Reynolds, Craig},
    title = {{TexSyn}: Library for evolutionary texture synthesis},
    url = {https://github.com/cwreynolds/TexSyn},
    howpublished={\url{https://github.com/cwreynolds/TexSyn}},
    year = {2019},
    originalyear = {15.12.2019},
}

@misc{reynolds_lazypredator_2020,
    author = {Reynolds, Craig},
    title = {{LazyPredator}: Genetic programming, negative selection, genetic drift},
    url = {https://github.com/cwreynolds/LazyPredator},
    howpublished={\url{https://github.com/cwreynolds/LazyPredator}},
    year = {2022},
    originalyear = {5.8.2020}
}

@misc{reynolds_predatoreye_2021,
    author = {Reynolds, Craig},
    title = {{PredatorEye}: anti-camouflage, camouflage-breaking},
    url = {https://github.com/cwreynolds/PredatorEye},
    howpublished={\url{https://github.com/cwreynolds/PredatorEye}},
    year = {2021},
    originalyear = {6.10.2021}
}

@misc{reynolds_FCD6_2022,
    author = {Reynolds, Craig},
    title = {{FCD6} {({Find} {Conspicuous} {Disk} {CNN} {Model:} {Version} 6 (rc4))}},
    url = {https://drive.google.com/drive/folders/1tJSHUhiqACmXipr-oVvZmCOyDogErVuq?usp=sharing},
    year = {2022},
    month = {3},
    day = {21},      
}

@misc{reynolds_texsyn_blog_2023,
    author = {Reynolds, Craig},
    title = {{TexSyn} {Development} {Blog}},
    url = {https://cwreynolds.github.io/TexSyn/},
    howpublished={\url{https://cwreynolds.github.io/TexSyn/}},
    year = {2023},
    originalyear = {15.12.2019},
}

@inproceedings{reynolds_coevolution_2023,
	title = {Coevolution of {Camouflage}},
	url = {https://dx.doi.org/10.1162/isal_a_00583},
	doi = {10.1162/isal_a_00583},
	abstract = {Abstract. Camouflage in nature seems to arise from competition between predator and prey. To survive, predators must find prey, and prey must avoid being found. This work simulates an abstract model of that adversarial relationship. It looks at crypsis through evolving prey camouflage patterns (as color textures) in competition with evolving predator vision. During their “lifetime” predators learn to better locate camouflaged prey. The environment for this 2D simulation is provided by a set of photographs, typically of natural scenes. This model is based on two evolving populations, one of prey and another of predators. Mutual conflict between these populations can produce both effective prey camouflage and predators skilled at “breaking” camouflage. The result is an open source artificial life model to help study camouflage in nature, and the perceptual phenomenon of camouflage more generally.},
	language = {en},
	urldate = {2023-07-30},
	publisher = {MIT Press},
	author = {Reynolds, Craig},
	month = jul,
	year = {2023},
    booktitle = {ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference},
    keywords = {learning, simulation, biology, camouflage, cocish, texture\_synthesis, coevolution, predator, prey, alife, vision, nature, predator-prey interactions, coc, mitp},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/GM29H4XU/Reynolds - 2023 - Coevolution of Camouflage.pdf:application/pdf},
}

@article{romera-paredes_mathematical_2023,
	title = {Mathematical discoveries from program search with large language models},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06924-6},
	doi = {10.1038/s41586-023-06924-6},
	abstract = {Large Language Models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations) which can result in them making plausible but incorrect statements [1,2]. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pre-trained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best known results in important problems, pushing the boundary of existing LLM-based approaches [3]. Applying FunSearch to a central problem in extremal combinatorics — the cap set problem — we discover new constructions of large cap sets going beyond the best known ones, both in finite dimensional and asymptotic cases. This represents the first discoveries made for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve upon widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications.},
	language = {en},
	urldate = {2023-12-23},
	journal = {Nature},
	author = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan S. and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
	month = dec,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {ai, deep\_learning, evolution, gp, Computer science, LLM, Pure mathematics, island, FunSearch},
	pages = {1--3},
	file = {Full Text:/Users/cwr/Zotero/storage/RPLH6I3F/Romera-Paredes et al. - 2023 - Mathematical discoveries from program search with .pdf:application/pdf},
}

@article{sharp_spelunking_2022,
    author = {Sharp, Nicholas and Jacobson, Alec},
    title = {Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis},
    year = {2022},
    issue_date = {July 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {41},
    number = {4},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/3528223.3530155},
    doi = {10.1145/3528223.3530155},
    abstract = {Neural implicit representations, which encode a surface as the level set of a neural network applied to spatial coordinates, have proven to be remarkably effective for optimizing, compressing, and generating 3D geometry. Although these representations are easy to fit, it is not clear how to best evaluate geometric queries on the shape, such as intersecting against a ray or finding a closest point. The predominant approach is to encourage the network to have a signed distance property. However, this property typically holds only approximately, leading to robustness issues, and holds only at the conclusion of training, inhibiting the use of queries in loss functions. Instead, this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures. Our key tool is the application of range analysis to neural networks, using automatic arithmetic rules to bound the output of a network over a region; we conduct a study of range analysis on neural networks, and identify variants of affine arithmetic which are highly effective. We use the resulting bounds to develop geometric queries including ray casting, intersection testing, constructing spatial hierarchies, fast mesh extraction, closest-point evaluation, evaluating bulk properties, and more. Our queries can be efficiently evaluated on GPUs, and offer concrete accuracy guarantees even on randomly-initialized networks, enabling their use in training objectives and beyond. We also show a preliminary application to inverse rendering.},
    journal = {ACM Trans. Graph.},
    month = {7},
    articleno = {107},
    numpages = {16},
    keywords = {geometry processing, range analysis, implicit surfaces, neural networks}
}

@article{sims_evolving_1994,
    author = {Sims, Karl},
    title = {Evolving 3d Morphology and Behavior by Competition},
    year = {1994},
    issue_date = {Summer 1994},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {1},
    number = {4},
    issn = {1064-5462},
    url = {https://doi.org/10.1162/artl.1994.1.4.353},
    doi = {10.1162/artl.1994.1.4.353},
    abstract = {This article describes a system for the evolution and coevolution of virtual creatures that compete in physically simulated three-dimensional worlds. Pairs of individuals enter one-on-one contests in which they contend to gain control of a common resource. The winners receive higher relative fitness scores allowing them to survive and reproduce. Realistic dynamics simulation including gravity, collisions, and friction, restricts the actions to physically plausible behaviors.The morphology of these creatures and the neural systems for controlling their muscle forces are both genetically determined, and the morphology and behavior can adapt to each other as they evolve simultaneously. The genotypes are structured as directed graphs of nodes and connections, and they can efficiently but flexibly describe instructions for the development of creatures' bodies and control systems with repeating or recursive components. When simulated evolutions are performed with populations of competing creatures, interesting and diverse strategies and counterstrategies emerge.},
    journal = {Artif. Life},
    month = {7},
    pages = {353–372},
    numpages = {20},
    keywords = {artificial evolution, virtual creatures, coevolution, evolutionary programming, artificial life, dynamic simulation}
}

@inproceedings{sims_artificial_1991,
	address = {New York, NY, USA},
	title = {Artificial evolution for computer graphics},
	volume = {25},
	isbn = {0-89791-436-8},
	url = {http://www.genarts.com/karl/papers/siggraph91.html},
	doi = {10.1145/122718.122752},
	abstract = {This paper describes how evolutionary techniques of variation and selection can be used to create complex simulated structures, textures, and motions for use in computer graphics and animation. Interactive selection, based on visual perception of procedurally generated results, allows the user to direct simulated evolutions in preferred directions. Several examples using these methods have been implemented and are described. 3D plant structures are grown using fixed sets of genetic parameters. Images, solid textures, and animations are created using mutating symbolic lisp expressions. Genotypes consisting of symbolic expressions are presented as an attempt to surpass the limitations of fixed-length genotypes with predefined expression rules. It is proposed that artificial evolution has potential as a powerful tool for achieving flexible complexity with a minimum of user input and knowledge of details.},
	booktitle = {{SIGGRAPH} '91: {Proceedings} of the 18th annual conference on {Computer} graphics and interactive techniques},
	publisher = {ACM},
	author = {Sims, Karl},
	month = jul,
	year = {1991},
	keywords = {evolution, color, texture\_synthesis, gp, humanfitnessfunction},
	pages = {319--328},
}

@misc{song_camouflaged_2023,
	title = {Camouflaged {Object} {Detection} with {Feature} {Grafting} and {Distractor} {Aware}},
	url = {http://arxiv.org/abs/2307.03943},
	doi = {10.48550/arXiv.2307.03943},
	abstract = {The task of Camouflaged Object Detection (COD) aims to accurately segment camouflaged objects that integrated into the environment, which is more challenging than ordinary detection as the texture between the target and background is visually indistinguishable. In this paper, we proposed a novel Feature Grafting and Distractor Aware network (FDNet) to handle the COD task. Specifically, we use CNN and Transformer to encode multi-scale images in parallel. In order to better explore the advantages of the two encoders, we design a cross-attention-based Feature Grafting Module to graft features extracted from Transformer branch into CNN branch, after which the features are aggregated in the Feature Fusion Module. A Distractor Aware Module is designed to explicitly model the two possible distractors in the COD task to refine the coarse camouflage map. We also proposed the largest artificial camouflaged object dataset which contains 2000 images with annotations, named ACOD2K. We conducted extensive experiments on four widely used benchmark datasets and the ACOD2K dataset. The results show that our method significantly outperforms other state-of-the-art methods. The code and the ACOD2K will be available at https://github.com/syxvision/FDNet.},
	urldate = {2023-07-13},
	publisher = {arXiv},
	author = {Song, Yuxuan and Li, Xinyue and Qi, Lin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03943 [cs]},
	keywords = {cocish, Computer Science - Computer Vision and Pattern Recognition, COD},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/3IXW9HBH/Song et al. - 2023 - Camouflaged Object Detection with Feature Grafting.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/4CYTGDXI/2307.html:text/html},
}

@misc{stevens_games_2022,
    title={ Sensory {Ecology} and {Evolution}: Games. [{Archived} web page.]},
    author={Stevens, Martin and others},
    year={2022},
	url = {https://web.archive.org/web/20230119122958/https://www.sensoryecology.com/games/},
}

@misc{sun_boundary-guided_2022,
	title = {Boundary-{Guided} {Camouflaged} {Object} {Detection}},
	url = {http://arxiv.org/abs/2207.00794},
    howpublished={\url{http://arxiv.org/abs/2207.00794}},
	abstract = {Camouflaged object detection (COD), segmenting objects that are elegantly blended into their surroundings, is a valuable yet challenging task. Existing deep-learning methods often fall into the difficulty of accurately identifying the camouflaged object with complete and fine object structure. To this end, in this paper, we propose a novel boundary-guided network (BGNet) for camouflaged object detection. Our method explores valuable and extra object-related edge semantics to guide representation learning of COD, which forces the model to generate features that highlight object structure, thereby promoting camouflaged object detection of accurate boundary localization. Extensive experiments on three challenging benchmark datasets demonstrate that our BGNet significantly outperforms the existing 18 state-of-the-art methods under four widely-used evaluation metrics. Our code is publicly available at: https://github.com/thograce/BGNet.},
	number = {arXiv:2207.00794},
	urldate = {2022-07-29},
	institution = {arXiv},
	author = {Sun, Yujia and Wang, Shuo and Chen, Chenglizhao and Xiang, Tian-Zhu},
	month = jul,
	year = {2022},
	doi = {10.48550/arXiv.2207.00794},
	note = {arXiv:2207.00794 [cs] type: article},
	keywords = {camouflage, cocish, predator, prey, vision, detection, Computer Science - Computer Vision and Pattern Recognition, Object detection, breaking},
}

@incollection{syswerda_study_1991,
	title = {A {Study} of {Reproduction} in {Generational} and {Steady}-{State} {Genetic} {Algorithms}},
	volume = {1},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080506845500094},
	abstract = {Two techniques of population control are currently used in the field of serial genetic algorithms: generational and steady state. Although they have been used somewhat interchangeably in the past, it has become apparent that the two techniques are actually quite different. In this paper, I study the behavior of each with regard to reproduction, and show that while each can be made similar with respect to the schema theorem, in practice their behavior is quite different.},
	language = {en},
	booktitle = {Foundations of {Genetic} {Algorithms}},
	publisher = {Elsevier},
    address = {Amsterdam},
	author = {Syswerda, Gilbert},
	editor = {Rawlins, Gregory J. E.},
	month = jan,
	year = {1991},
	doi = {10.1016/B978-0-08-050684-5.50009-4},
	keywords = {generational reproduction, population, reproduction, steady-state reproduction},
	pages = {94--101},
	file = {ScienceDirect Snapshot:/Users/cwr/Zotero/storage/P8IYJRLN/B9780080506845500094.html:text/html;Syswerda - 1991 - A Study of Reproduction in Generational and Steady.pdf:/Users/cwr/Zotero/storage/GIGTKITI/Syswerda - 1991 - A Study of Reproduction in Generational and Steady.pdf:application/pdf},
}

@article{talas_camogan_2020,
    author = {Talas, Laszlo and Fennell, John G. and Kjernsmo, Karin and Cuthill, Innes C. and Scott-Samuel, Nicholas E. and Baddeley, Roland J.},
    title = {CamoGAN: Evolving optimum camouflage with Generative Adversarial Networks},
    journal = {Methods in Ecology and Evolution},
    volume = {11},
    number = {2},
    pages = {240-247},
    keywords = {CamoGAN, camouflage, co-evolution, deep learning, generative adversarial networks, predator-prey interactions, protective colouration, signalling patterns},
    doi = {https://doi.org/10.1111/2041-210X.13334},
    abstract = {Abstract One of the most challenging issues in modelling the evolution of protective colouration is the immense number of potential combinations of colours and textures. We describe CamoGAN, a novel method to exploit Generative Adversarial Networks to simulate an evolutionary arms race between the camouflage of a synthetic prey and its predator. Patterns evolved using our methods are shown to provide progressively more effective concealment and outperform two recognized camouflage techniques, as validated by using humans as visual predators. We believe CamoGAN will be highly useful, particularly for biologists, for rapidly developing and testing optimal camouflage or signalling patterns in multiple environments.},
    year = {2020}
}

@misc{tensorflow_whitepaper_2015,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    howpublished={\url{https://www.tensorflow.org/}},
    author={Mart\'{i}n~Abadi and others},
    year={2015},
    }

@book{thayer_concealing-coloration_1909,
	address = {New York, NY},
	title = {Concealing-coloration in the animal kingdom: an exposition of the laws of disguise through color and pattern: being a summary of {Abbott} {H}. {Thayer}'s discoveries.},
	url = {http://books.google.com/books?id=vtgKAAAAIAAJ},
	abstract = {PROTECTIVE COLORATION with its achievement of the wonderful inconspicuousness of many wild animals in their native haunts has been recognized since the earliest days of Natural History study. But the true character of this phenomenon has been ignored or misinterpreted and the phenomenon itself has been observed only in one small corner of its wide field of action. It has waited for an artist in the last years of the nineteenth century not only to recognize the basic working laws of protective coloration but to perceive that the many animals of supposed conspicuous attire are almost all colored and marked in the way most potent to conceal them...},
	publisher = {Macmillan},
	author = {Thayer, Gerald H.},
	year = {1909},
	keywords = {camouflage, history},
}

@book{todd_evolutionary_1994,
    author = {Todd, Stephen and Latham, William},
    title = {Evolutionary Art and Computers},
    year = {1994},
    isbn = {012437185X},
    publisher = {Academic Press, Inc.},
    address = {USA} 
}

@article{troscianko_quantifying_2017,
	title = {Quantifying camouflage: how to predict detectability from appearance},
	volume = {17},
	issn = {1471-2148},
	shorttitle = {Quantifying camouflage},
	url = {https://doi.org/10.1186/s12862-016-0854-2},
	doi = {10.1186/s12862-016-0854-2},
	abstract = {Quantifying the conspicuousness of objects against particular backgrounds is key to understanding the evolution and adaptive value of animal coloration, and in designing effective camouflage. Quantifying detectability can reveal how colour patterns affect survival, how animals’ appearances influence habitat preferences, and how receiver visual systems work. Advances in calibrated digital imaging are enabling the capture of objective visual information, but it remains unclear which methods are best for measuring detectability. Numerous descriptions and models of appearance have been used to infer the detectability of animals, but these models are rarely empirically validated or directly compared to one another. We compared the performance of human ‘predators’ to a bank of contemporary methods for quantifying the appearance of camouflaged prey. Background matching was assessed using several established methods, including sophisticated feature-based pattern analysis, granularity approaches and a range of luminance and contrast difference measures. Disruptive coloration is a further camouflage strategy where high contrast patterns disrupt they prey’s tell-tale outline, making it more difficult to detect. Disruptive camouflage has been studied intensely over the past decade, yet defining and measuring it have proven far more problematic. We assessed how well existing disruptive coloration measures predicted capture times. Additionally, we developed a new method for measuring edge disruption based on an understanding of sensory processing and the way in which false edges are thought to interfere with animal outlines.},
	number = {1},
	urldate = {2023-11-02},
	journal = {BMC Evolutionary Biology},
	author = {Troscianko, Jolyon and Skelhorn, John and Stevens, Martin},
	month = jan,
	year = {2017},
	keywords = {cocish, metric, predator, humanfitnessfunction, quality, breaking, Camouflage, coc, Background matching, Disruptive coloration, Animal coloration, Crypsis, Image processing, Pattern analysis, Predation, Signalling, Vision},
	pages = {7},
	file = {Full Text PDF:/Users/cwr/Zotero/storage/GQRDH6BR/Troscianko et al. - 2017 - Quantifying camouflage how to predict detectabili.pdf:application/pdf;Snapshot:/Users/cwr/Zotero/storage/8S7SWQXU/s12862-016-0854-2.html:text/html},
}

@article{turing_chemical_1952,
	title = {The {Chemical} {Basis} of {Morphogenesis}},
	volume = {237},
	issn = {00804622},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1952.0012},
	doi = {10.2307/92463},
	abstract = {It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on Hydra and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.},
	number = {641},
	journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
	author = {Turing, A. M.},
	year = {1952},
	keywords = {camouflage, morphogenesis, texture\_synthesis, natureinspired, reaction-diffusion},
	pages = {37--72},
}

@article{turk_generating_1991,
    author = {Turk, Greg},
    title = {Generating Textures on Arbitrary Surfaces Using Reaction-Diffusion},
    year = {1991},
    issue_date = {July 1991},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {25},
    number = {4},
    issn = {0097-8930},
    url = {https://doi.org/10.1145/127719.122749},
    doi = {10.1145/127719.122749},
    abstract = {This paper describes a biologically motivated method of texture synthesis called reaction-diffusion and demonstrates how these textures can be generated in a manner that directly matches the geometry of a given surface. Reaction-diffusion is a process in which two or more chemicals diffuse at unequal rates over a surface and react with one another to form stable patterns such as spots and stripes. Biologists and mathematicians have explored the patterns made by several reaction-diffusion systems. We extend the range of textures that have previously been generated by using a cascade of multiple reaction-diffusion systems in which one system lays down an initial pattern and then one or more later systems refine the pattern. Examples of patterns generated by such a cascade process include the clusters of spots on leopards known as rosettes and the web-like patterns found on giraffes. In addition, this paper introduces a method which reaction-diffusion textures are created to match the geometry of an arbitrary polyhedral surface. This is accomplished by creating a mesh over a given surface and then simulating the reaction-diffusion process directly on this mesh. This avoids the often difficult task of assigning texture coordinates to a complex surface. A mesh is generated by evenly distributing points over the model using relaxation and then determining which points are adjacent by constructing their Voronoi regions. Textures are rendered directly from the mesh by using a weighted sum of mesh values to compute surface color at a given position. Such textures can also be used as bump maps.},
    journal = {SIGGRAPH Comput. Graph.},
    month = {7},
    pages = {289–298},
    numpages = {10},
    keywords = {reaction-diffusion, biological models, texture mapping}
    }

@book{valiant_probably_2013,
	address = {New York},
	title = {Probably {Approximately} {Correct}: {Nature}'s {Algorithms} for {Learning} and {Prospering} in a {Complex} {World}},
	isbn = {0-465-03271-0},
	url = {https://www.basicbooks.com/titles/leslie-valiant/probably-approximately-correct/9780465037902/},
	abstract = {From a leading computer scientist, a grand unifying theory that will revolutionize our understanding of how life evolves and learns, and hence what life is. How does life prosper in a complex and erratic world? While we know that nature follows patterns—such as the law of gravity—our everyday lives are beyond what known science can predict. We nevertheless muddle through even in the absence of theories of how to act. But how do we do it? In Probably Approximately Correct, computer scientist Leslie Valiant presents a masterful synthesis of learning and evolution to show how both individually and collectively we not only survive, but prosper in a world as complex as our own. The key is ” probably approximately correct” algorithms, a concept Valiant developed to explain how effective behavior can be learned. The model shows that pragmatically coping with a problem can provide a satisfactory solution in the absence of any theory of the problem. After all, finding a mate does not require a theory of mating. Valiant's theory reveals the shared computational nature of evolution and learning, and sheds light on perennial questions such as nature versus nurture and the limits of artificial intelligence. Valiant provides a new perspective on human nature as a product of evolution and adaptation. If, as human beings, we are shaped entirely by evolution before conception and learning afterwards, then all our characteristics, whether biological or psychological, will have been determined by adaptive mechanisms. Offering a powerful and elegant model that encompasses life's complexity, Probably Approximately Correct has profound implications for how we think about behavior, cognition, biological evolution, and the possibilities and limits of human and machine intelligence.},
	publisher = {Basic Books},
	author = {Valiant, Leslie},
	month = jun,
	year = {2013},
	keywords = {ai, computation, darwin, evolution, learning, machinelearning, pac, theory, turing},
}

@article{van_valen_new_1973,
	title = {A new evolutionary law},
	volume = {1},
	issn = {0093-4755},
	url = {https://www.mn.uio.no/cees/english/services/van-valen/evolutionary-theory/},
	abstract = {All groups for which data exist go extinct at a rate that is constant for a given group. When this is recast in ecological form (the effective environment o f any homogeneous group of organisms deteriorates at a stochastically constant rate), no definite exceptions exist although a few are possible. Extinction rates are similar within some very broad categories and vary regularly with size of area inhabited. A new unit of rates for discrete phenomena, the macarthur, is introduced. Laws are appropriate in evolutionary biology. Truth needs more than correct predictions. The Law of Extinction is evidence for ecological significance and comparability of taxa. A non-Markovian hypothesis to explain the law invokes mutually incompatible optima within an adaptive zone. A self-perpetuating fluctuation results which can be stated in terms of an unstudied aspect of zero-sum game theory. The hypothesis can be derived from a view that momentary fitness is the amount of control of resources, which remain constant in total amount. The hypothesis implies that long-term fitness has only two components and that events of mutualism are rare. The hypothesis largely explains the observed pattern of molecular evolution.},
	journal = {Evolutionary Theory},
	author = {Van Valen, Leigh},
	year = {1973},
	pages = {1--30},
}

@misc{vu_leveraging_2023,
	title = {Leveraging {Open}-{Vocabulary} {Diffusion} to {Camouflaged} {Instance} {Segmentation}},
	url = {http://arxiv.org/abs/2312.17505},
	doi = {10.48550/arXiv.2312.17505},
	abstract = {Text-to-image diffusion techniques have shown exceptional capability of producing high-quality images from text descriptions. This indicates that there exists a strong correlation between the visual and textual domains. In addition, text-image discriminative models such as CLIP excel in image labelling from text prompts, thanks to the rich and diverse information available from open concepts. In this paper, we leverage these technical advances to solve a challenging problem in computer vision: camouflaged instance segmentation. Specifically, we propose a method built upon a state-of-the-art diffusion model, empowered by open-vocabulary to learn multi-scale textual-visual features for camouflaged object representations. Such cross-domain representations are desirable in segmenting camouflaged objects where visual cues are subtle to distinguish the objects from the background, especially in segmenting novel objects which are not seen in training. We also develop technically supportive components to effectively fuse cross-domain features and engage relevant features towards respective foreground objects. We validate our method and compare it with existing ones on several benchmark datasets of camouflaged instance segmentation and generic open-vocabulary instance segmentation. Experimental results confirm the advances of our method over existing ones. We will publish our code and pre-trained models to support future research.},
	urldate = {2024-01-03},
	publisher = {arXiv},
	author = {Vu, Tuan-Anh and Nguyen, Duc Thanh and Guo, Qing and Hua, Binh-Son and Chung, Nhat Minh and Tsang, Ivor W. and Yeung, Sai-Kit},
	month = dec,
	year = {2023},
	note = {arXiv:2312.17505 [cs]},
	keywords = {camouflage, cocish, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, coc, cod, llm, clip},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/Y9WNEGGF/Vu et al. - 2023 - Leveraging Open-Vocabulary Diffusion to Camouflage.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/34VBEFU9/2312.html:text/html},
}

@article{witkin_reaction_1991,
    author = {Witkin, Andrew and Kass, Michael},
    title = {Reaction-Diffusion Textures},
    year = {1991},
    issue_date = {July 1991},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {25},
    number = {4},
    issn = {0097-8930},
    url = {https://doi.org/10.1145/127719.122750},
    doi = {10.1145/127719.122750},
    abstract = {We present a method for texture synthesis based on the simulation of a process of local nonlinear interaction, called reaction-diffusion, which has been proposed as a model of biological pattern formation. We extend traditional reaction-diffusion systems by allowing anisotropic and spatially non-uniform diffusion, as well as multiple competing directions of diffusion. We adapt reaction-diffusion system to the needs of computer graphics by presenting a method to synthesize patterns which compensate for the effects of non-uniform surface parameterization. Finally, we develop efficient algorithms for simulating reaction-diffusion systems and display a collection of resulting textures using standard texture- and displacement-mapping techniques.},
    journal = {SIGGRAPH Comput. Graph.},
    month = {7},
    pages = {299–308},
    numpages = {10},
    keywords = {texture synthesis, natural phenomena}
}

@misc{visionxiang_cod,
    author = {visionxiang},
    title = {Camouflaged/Concealed Object Detection},
    url = {https://github.com/visionxiang/awesome-camouflaged-object-detection},
    howpublished={\url{https://github.com/visionxiang/awesome-camouflaged-object-detection}},
    year = {2022},
    originalyear = {4.10.2016}
}

@article{volonakis_camouflage_2018,
	title = {Camouflage assessment: {Machine} and human},
	volume = {99},
	issn = {0166-3615},
	shorttitle = {Camouflage assessment},
	url = {https://www.sciencedirect.com/science/article/pii/S0166361517305705},
	doi = {10.1016/j.compind.2018.03.013},
	abstract = {A vision model is designed using low-level vision principles so that it can perform as a human observer model for camouflage assessment. In a camouflaged-object assessment task, using military patterns in an outdoor environment, human performance at detection and recognition is compared with the human observer model. This involved field data acquisition and subsequent image calibration, a human experiment, and the design of the vision model. Human and machine performance, at recognition and detection, of military patterns in two environments was found to correlate highly. Our model offers an inexpensive, automated, and objective method for the assessment of camouflage where it is impractical, or too expensive, to use human observers to evaluate the conspicuity of a large number of candidate patterns. Furthermore, the method should generalize to the assessment of visual conspicuity in non-military contexts.},
	language = {en},
	journal = {Computers in Industry},
	author = {Volonakis, Timothy N. and Matthews, Olivia E. and Liggins, Eric and Baddeley, Roland J. and Scott-Samuel, Nicholas E. and Cuthill, Innes C.},
	month = aug,
	year = {2018},
	keywords = {Camouflage assessment, Observer modelling, Visual search},
	pages = {173--182},
	file = {ScienceDirect Snapshot:/Users/cwr/Zotero/storage/X6ZC2DGM/S0166361517305705.html:text/html;Submitted Version:/Users/cwr/Zotero/storage/DEV5SZLV/Volonakis et al. - 2018 - Camouflage assessment Machine and human.pdf:application/pdf},
}

@article{wu_mimicry_2021,
	title = {Mimicry: {Genetic}-algorithm-based {Real}-time {System} of {Virtual} {Insects} in a {Living} {Environment}-{A} {New} and {Altered} {Nature}},
	volume = {4},
	shorttitle = {Mimicry},
	url = {https://dl.acm.org/doi/10.1145/3465615},
	doi = {10.1145/3465615},
	abstract = {The authors have collaborated on a machine learning multiscreen video installation powered by computer algorithms and inspired by mimicry in the natural world. The artwork explores a pseudo-environment loop system in nature and artificial mechanical organisms combining living flowers with projectors, webcams, and computer monitors. Technically, the software adopts a genetic algorithm to simulate the process of mimicry; conceptually, this real-time art installation is in conversation with Nam June Paik's piece TV Garden. The project explores the possibilities of integrating artificial intelligence and nature in the landscape of the future.},
	number = {2},
	journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
	author = {Wu, Ziwei and Huang, Lingdong},
	month = aug,
	year = {2021},
	keywords = {camouflage, cocish, art, interactive, flower, humanfitnessfunction, mimicry, plant, Interactive Installation, game, Computer systems organization, Computing methodologies, Software and its engineering, iec},
	pages = {28:1--28:8},
}


@article{xiuxia_imitation_2023,
	title = {Imitation camouflage synthesis based on shallow neural network},
	volume = {29},
	issn = {1432-1882},
	url = {https://doi.org/10.1007/s00530-023-01149-z},
	doi = {10.1007/s00530-023-01149-z},
	abstract = {Deep learning technology has been widely used in the military field, which have achieved great success. The traditional method for painting camouflage either using the background information or the artificial pattern. None of the traditional methods can both consider the background information and camouflage rules. In this paper, a new automatic camouflage generation framework is proposed. A method for generating camouflage pattern is designed. The imitation camouflage pattern is synthesized from the features of both background and artificial pattern. In our method, the texture feature of both background and traditional pattern patches are extracted from the feature maps of shallow neural network (SNN). Based on the feature maps, statistic information of second order differential and mean subtracted contrast normalized coefficients for texture and color is extracted. By iterating to optimize the imitation camouflage to be generated, the statistical information of the imitation camouflage can approximate the characteristic statistical information of the background and pattern. The new generated camouflage pattern can contain the color and texture information of background; besides, it can maintain the traditional patch camouflage criteria. Our approach makes camouflage painting more flexible and allows the target to better infuse into the background. And our method is designed for the preparation of painting camouflage.},
	language = {en},
	number = {5},
	urldate = {2025-02-13},
	journal = {Multimedia Systems},
	author = {Xiuxia, Cai and Pin, Zhang and Shuaibin, Du},
	month = oct,
	year = {2023},
	keywords = {Camouflage, cocish, Deep learning, deep\_learning, Feature, features, military, painting, Painting, texture\_analysis, texture\_synthesis},
	pages = {2705--2714},
	file = {Submitted Version:/Users/cwr/Zotero/storage/8BBL95L8/Xiuxia et al. - 2023 - Imitation camouflage synthesis based on shallow neural network.pdf:application/pdf},
}

@misc{yin_camoformer_2022,
    doi = {10.48550/ARXIV.2212.06570},
    url = {https://arxiv.org/abs/2212.06570},
    howpublished={\url{https://arxiv.org/abs/2212.06570}},
    author = {Yin, Bowen and Zhang, Xuying and Hou, Qibin and Sun, Bo-Yuan and Fan, Deng-Ping and Van Gool, Luc},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {CamoFormer: Masked Separable Attention for Camouflaged Object Detection},
    publisher = {arXiv},
    year = {2022},
    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{Zhang_Yin_Nie_Zheng_2020,
    title={Deep Camouflage Images},
    author={Zhang, Qing and Yin, Gelin and Nie, Yongwei and Zheng, Wei-Shi},
    volume={34},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/6981},
    DOI={10.1609/aaai.v34i07.6981},
    number={07},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    year={2020},
    month={4},
    pages={12845-12852},
    abstractNote={&lt;p&gt;This paper addresses the problem of creating &lt;em&gt;camouflage images&lt;/em&gt;. Such images typically contain one or more hidden objects embedded into a background image, so that viewers are required to consciously focus to discover them. Previous methods basically rely on hand-crafted features and texture synthesis to create camouflage images. However, due to lack of reliable understanding of what essentially makes an object recognizable, they typically result in either complete standout or complete invisible hidden objects. Moreover, they may fail to produce seamless and natural images because of the sensitivity to appearance differences. To overcome these limitations, we present a novel neural style transfer approach that adopts the visual perception mechanism to create camouflage images, which allows us to hide objects more effectively while producing natural-looking results. In particular, we design an attention-aware camouflage loss to adaptively mask out information that make the hidden objects visually standout, and also leave subtle yet enough feature clues for viewers to perceive the hidden objects. To remove the appearance discontinuities between the hidden objects and the background, we formulate a naturalness regularization to constrain the hidden objects to maintain the manifold structure of the covered background. Extensive experiments show the advantages of our approach over existing camouflage methods and state-of-the-art neural style transfer algorithms.&lt;/p&gt;}
}

@article{zhang_spatial_2013,
	title = {The {Spatial} {Color} {Mixing} {Model} of {Digital} {Camouflage} {Pattern}},
	volume = {9},
	issn = {2214-9147},
	url = {https://www.sciencedirect.com/science/article/pii/S2214914713000160},
	doi = {10.1016/j.dt.2013.09.015},
	abstract = {A spatial color-mixing model based on tricolor angular frequencies is proposed in consideration that the design theory falls behind the application of digital camouflage pattern. The model is based on Fourier transform and Gaussian low-pass filter (LPF). In the model, the tricolor angular frequencies are introduced to the spatial frequency response function of human color vision, and the effects of atmospheric attenuation and air screen brightness on color mixture are considered. The field test shows that the model can simulate the color-mixing process in the aspects of color-mixing order, and shape and position of color-mixing spot. But the color-mixing spot color is not perfect, which can be improved by optimizing the atmospheric parameters and tricolor cut-off angular frequencies. The model provides a tool for the research on digital camouflage pattern.},
	language = {en},
	number = {3},
	urldate = {2023-02-14},
	journal = {Defence Technology},
	author = {Zhang, Yong and Xue, Shi-qiang and Jiang, Xiao-jun and Mu, Jing-yang and Yi, Yang},
	month = sep,
	year = {2013},
	keywords = {cocish, color, Angular frequency, Camouflage, Digital camouflage pattern, Gauss low-pass filter, Spatial color mixing},
	pages = {157--161},
	file = {ScienceDirect Full Text PDF:/Users/cwr/Zotero/storage/S6YFHVDH/Zhang et al. - 2013 - The Spatial Color Mixing Model of Digital Camoufla.pdf:application/pdf;ScienceDirect Snapshot:/Users/cwr/Zotero/storage/6J9CVRRM/S2214914713000160.html:text/html},
}

@inproceedings{Zhang2022,
    author = {Zhang, Miao and Xu, Shuang and Piao, Yongri and Shi, Dongxiang and Lin, Shusen and Lu, Huchuan},
    title = {{PreyNet}: Preying on Camouflaged Objects},
    year = {2022},
    isbn = {9781450392037},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA}, url = {https://doi.org/10.1145/3503161.3548178},
    doi = {10.1145/3503161.3548178},
    abstract = {Species often adopt various camouflage strategies to be seamlessly blended into the surroundings for self-protection. To figure out the concealment, predators have evolved excellent hunting skills. Exploring the intrinsic mechanisms of the predation behavior can offer more insightful glimpse into the task of camouflaged object detection (COD). In this work, we strive to seek answers for accurate COD and propose a PreyNet, which mimics the two processes of predation, namely, initial detection (sensory mechanism) and predator learning (cognitive mechanism). To exploit the sensory process, a bidirectional bridging interaction module (BBIM) is designed for selecting and aggregating initial features in an attentive manner. The predator learning process is formulated as a policy-and-calibration paradigm, with the goal of deciding on uncertain regions and encouraging targeted feature calibration. Besides, we obtain adaptive weight for multi-layer supervision during training via computing on the uncertainty estimation. Extensive experiments demonstrate that our model produces state-of-the-art results on several benchmarks. We further verify the scalability of the predator learning paradigm through applications on top-ranking salient object detection models. Our code is publicly available at urlhttps://github.com/OIPLab-DUT/PreyNet.},
    booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
    pages = {5323–5332},
    numpages = {10},
    keywords = {camouflaged object detection, sensory mechanism, predator learning},
    location = {Lisboa, Portugal},
    series = {MM '22}
}

@misc{zhang_camouflaged_2023,
	title = {Camouflaged {Image} {Synthesis} {Is} {All} {You} {Need} to {Boost} {Camouflaged} {Detection}},
	url = {http://arxiv.org/abs/2308.06701},
	doi = {10.48550/arXiv.2308.06701},
	abstract = {Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.},
	urldate = {2023-08-17},
	publisher = {arXiv},
	author = {Zhang, Haichao and Qin, Can and Yin, Yu and Fu, Yun},
	month = aug,
	year = {2023},
	note = {arXiv:2308.06701 [cs]},
	keywords = {learning, camouflage, cocish, generative, adversary, vision, segmentation, perception, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, breaking, background-matching, Visual search, COD},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/4DA4YI3I/Zhang et al. - 2023 - Camouflaged Image Synthesis Is All You Need to Boo.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/H3MGR4GK/2308.html:text/html},
}