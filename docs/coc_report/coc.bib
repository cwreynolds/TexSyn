@article{chen_boundary-guided_2022,
	title = {Boundary-guided network for camouflaged object detection},
	volume = {248},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705122004294},
	doi = {https://doi.org/10.1016/j.knosys.2022.108901},
	abstract = {Compared with the traditional object segmentation/detection, camouflaged object detection is much more difficult due to the indefinable boundaries and high intrinsic similarities between the camouflaged regions and the background. Although various algorithms have been proposed to solve the issue, these methods still suffer from coarse boundaries and are not competent to identify the camouflaged objects from the background in complex scenarios. In this paper, we propose a novel boundary-guided network to address this challenging problem in a coarse-to-fine manner. Specifically, we design a locating module to infer the initial location of the camouflaged objects by exploiting local detailed cues and global contextual information. Moreover, a boundary-guided fusion module is proposed to explore the complementary relationship between the camouflaged regions and their boundaries. By leveraging the boundary feature, we can not only generate prediction maps with sharper boundaries but also effectively eliminate background noises. Equipped with the two key modules, our BgNet is capable of segmenting camouflaged regions accurately and quickly. Extensive experimental results on four widely used benchmark datasets demonstrate that the proposed BgNet runs at a real-time speed (36 FPS) on a single NVIDIA Titan XP GPU and outperforms 17 state-of-the-art competing algorithms in terms of six standard evaluation metrics. Source code will be publicly available at https://github.com/clelouch/BgNet upon paper acceptance.},
	journal = {Knowledge-Based Systems},
	author = {Chen, Tianyou and Xiao, Jin and Hu, Xiaoguang and Zhang, Guofeng and Wang, Shaojie},
	year = {2022},
	keywords = {Boundary guidance, Camouflage object detection, Coarse-to-fine refinement, Convolutional neural network},
	pages = {108901},
}

@article{chu_camo_image_2010,
    author = {Chu, Hung-Kuo and Hsu, Wei-Hsin and Mitra, Niloy J. and Cohen-Or, Daniel and Wong, Tien-Tsin and Lee, Tong-Yee},
    title = {Camouflage Images},
    year = {2010},
    issue_date = {July 2010},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {29},
    number = {4},
    issn = {0730-0301},
    url = {https://doi.org/10.1145/1778765.1778788},
    doi = {10.1145/1778765.1778788},
    abstract = {Camouflage images contain one or more hidden figures that remain imperceptible or unnoticed for a while. In one possible explanation, the ability to delay the perception of the hidden figures is attributed to the theory that human perception works in two main phases: feature search and conjunction search. Effective camouflage images make feature based recognition difficult, and thus force the recognition process to employ conjunction search, which takes considerable effort and time. In this paper, we present a technique for creating camouflage images. To foil the feature search, we remove the original subtle texture details of the hidden figures and replace them by that of the surrounding apparent image. To leave an appropriate degree of clues for the conjunction search, we compute and assign new tones to regions in the embedded figures by performing an optimization between two conflicting terms, which we call immersion and standout, corresponding to hiding and leaving clues, respectively. We show a large number of camouflage images generated by our technique, with or without user guidance. We have tested the quality of the images in an extensive user study, showing a good control of the difficulty levels.},
    journal = {ACM Trans. Graph.},
    month = {jul},
    articleno = {51},
    numpages = {8}
}

@inproceedings{cramer_representation_1985,
	address = {Carnegie-Mellon University, Pittsburgh, PA, USA},
	title = {A {Representation} for the {Adaptive} {Generation} of {Simple} {Sequential} {Programs}},
	shorttitle = {{ICGA} 1985},
	abstract = {An adaptive system for generating short sequential computer functions is described. The created functions are written in the simple "number-string" language JB, and in TB, a modified version of JB with a tree-like structure. These languages have the feature that they can be used to represent well-formed, useful computer programs while still being amenable to suitably defined genetic operators. The system is used to produce two-input, single-output multiplication functions that are concise and well-defined. Future work, dealing with extensions to more complicated functions and generalizations of the techniques, is also discussed.},
	booktitle = {Proceedings of an {International} {Conference} {On} {Genetic} {Algorithms} {And} {Their} {Applications}},
	publisher = {Lawrence Erlbaum Associates},
	author = {Cramer, Nichael Lynn},
	month = jul,
	year = {1985},
	keywords = {algorithm, evolution, ga, Genetic programming, gp, history, induction, optimization, programming},
	pages = {183--187},
	file = {Cramer - 1985 - A Representation for the Adaptive Generation of Si.pdf:/Users/cwr/Zotero/storage/DNLKQ4TB/Cramer - 1985 - A Representation for the Adaptive Generation of Si.pdf:application/pdf},
}

@book{dawkins_blind_1986,
	address = {New York, NY, USA},
	title = {The {Blind} {Watchmaker}},
	isbn = {0-393-31570-3},
	url = {https://wwnorton.com/books/The-Blind-Watchmaker/},
	abstract = {One of the most famous arguments of the creationist theory of the universe is the eighteenth-century theologian William Paley's: Just as a watch is too complicated and too functional to have sprung into existence by accident, so too must all living things, with their far greater complexity, be purposefully designed. But as Richard Dawkins, professor of zoology at Oxford University, demonstrates in this brilliant and eloquent riposte to the Argument from Design, the analogy is false. Natural selection, the unconscious, automatic, blind yet essentially non-random process that Darwin discovered, has no purpose in mind. If it can be said to play the role of watchmaker in nature, it is the blind watchmaker. Patiently and lucidly, Dr. Dawkins - in this book which has been acclaimed as perhaps the most influential work on evolution written in this century - identifies those aspects of the theory which people find hard to believe and removes the barrier to credibility one by one.},
	publisher = {W. W. Norton \& Company, Inc.},
	author = {Dawkins, Richard},
	month = sep,
	year = {1986},
	keywords = {evolution, humanfitnessfunction},
}

@article{de_gomensoro_malheiros_leopard_2020,
	title = {The leopard never changes its spots: realistic pigmentation pattern formation by coupling tissue growth with reaction-diffusion},
	volume = {39},
	issn = {0730-0301},
	shorttitle = {The leopard never changes its spots},
	url = {https://doi.org/10.1145/3386569.3392478},
	doi = {10.1145/3386569.3392478},
	abstract = {Previous research in pattern formation using reaction-diffusion mostly focused on static domains, either for computational simplicity or mathematical tractability. In this work, we have explored the expressiveness of combining simple mechanisms as a possible explanation for pigmentation pattern formation, where tissue growth plays a crucial role. Our motivation is not only to realistically reproduce natural patterns but also to get insights into the underlying biological processes. Therefore, we present a novel approach to generate realistic animal skin patterns. First, we describe the approximation of tissue growth by a series of discrete matrix expansion operations. Then, we combine it with an adaptation of Turing's non-linear reaction-diffusion model, which enforces upper and lower bounds to the concentrations of the involved chemical reagents. We also propose the addition of a single-reagent continuous autocatalytic reaction, called reinforcement, to provide a mechanism to maintain an already established pattern during growth. By careful adjustment of the parameters and the sequencing of operations, we closely match the appearance of a few real species. In particular, we reproduce in detail the distinctive features of the leopard skin, also providing a hypothesis for the simultaneous productions of the most common melanin types, eumelanin and pheomelanin.},
	number = {4},
	urldate = {2022-10-23},
	journal = {ACM Transactions on Graphics},
	author = {De Gomensoro Malheiros, Marcelo and Fensterseifer, Henrique and Walter, Marcelo},
	month = jul,
	year = {2020},
	keywords = {natural phenomena, pattern formation, reaction-diffusion, texturing, turing model},
	pages = {63:63:1--63:62:14},
}

@book{glassner_deep_2021,
	address = {San Francisco, CA},
	title = {Deep {Learning}: {A} {Visual} {Approach}},
	isbn = {978-1-71850-072-3},
	url = {https://nostarch.com/deep-learning-visual-approach},
	publisher = {No Starch Press},
	author = {Glassner, Andrew},
	year = {2021},
	keywords = {classification, deep\_learning, deeplearning, gan, learning, machinelearning, neural\_nets, optimization, regression, statistics, tutorial, visualization},
}

@misc{guo_ganmouflage_2022,
	title = {GANmouflage: 3D Object Nondetection with Texture Fields},
	shorttitle = {{GANmouflage}},
	url = {http://arxiv.org/abs/2201.07202},
	abstract = {We propose a method that learns to camouflage 3D objects within scenes. Given an object's shape and a distribution of viewpoints from which it will be seen, we estimate a texture that will make it difficult to detect. Successfully solving this task requires a model that can accurately reproduce textures from the scene, while simultaneously dealing with the highly conflicting constraints imposed by each viewpoint. We address these challenges with a model based on texture fields and adversarial learning. Our model learns to camouflage a variety of object shapes from randomly sampled locations and viewpoints within the input scene, and is the first to address the problem of hiding complex object shapes. Using a human visual search study, we find that our estimated textures conceal objects significantly better than previous methods. Project site: https://rrrrrguo.github.io/ganmouflage/},
	urldate = {2022-01-23},
	journal = {arXiv:2201.07202 [cs]},
	author = {Guo, Rui and Collins, Jasmine and de Lima, Oscar and Owens, Andrew},
    month = {jan},
	year = {2022},
	note = {arXiv: 2201.07202},
	keywords = {camouflage, cocish, 3d, texture\_synthesis, human, detection, Computer Science - Computer Vision and Pattern Recognition, anti\_camouflage, multi\_view},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/VKQU27CT/Guo et al. - 2022 - GANmouflage 3D Object Nondetection with Texture F.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/VPPJU2P2/2201.html:text/html},
    numpages = {16},
    eprinttype = {arXiv},
    eprint={2201.07202},
}

@inproceedings{harrington_coevolution_2014,
	author = {Harrington, Kyle I. and Freeman, Jesse and Pollack, Jordan},
    title = "{Coevolution in Hide and Seek: Camouflage and Vision}",
    booktitle = {ALIFE 14: The Fourteenth International Conference on the Synthesis and Simulation of Living Systems},
    address = {New York City, NY, USA},
    pages = {25-32},
    year = {2014},
    month = {07},
    publisher = {MIT Press},
    url = {https://direct.mit.edu/isal/proceedings/alife2014/25/98754},
}

@book{koza_genetic_1992,
	address = {Cambridge, Mass.},
	edition = {1},
	title = {Genetic {Programming}: {On} the {Programming} of {Computers} by {Means} of {Natural} {Selection} ({Complex} {Adaptive} {Systems})},
	isbn = {0-262-11170-5},
	url = {https://mitpress.mit.edu/9780262527910/genetic-programming/},
	publisher = {MIT Press, A Bradford Book},
	author = {Koza, John R.},
	month = dec,
	year = {1992},
	keywords = {evolution, gp},
}

@misc{lv_cod_2022,
    doi = {10.48550/ARXIV.2205.11333},
    url = {https://arxiv.org/abs/2205.11333},
    author = {Lv, Yunqiu and Zhang, Jing and Dai, Yuchao and Li, Aixuan and Barnes, Nick and Fan, Deng-Ping},
    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {Towards Deeper Understanding of Camouflaged Object Detection},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Mckay_2010,
    author = {Mckay, Robert I. and Hoai, Nguyen Xuan and Whigham, Peter Alexander and Shan, Yin and O'Neill, Michael},
    title = {Grammar-Based Genetic Programming: A Survey},
    year = {2010},
    issue_date = {September 2010},
    publisher = {Kluwer Academic Publishers},
    address = {USA},
    volume = {11},
    number = {3–4},
    issn = {1389-2576},
    url = {https://doi.org/10.1007/s10710-010-9109-y},
    doi = {10.1007/s10710-010-9109-y},
    abstract = {Grammar formalisms are one of the key representation structures in Computer Science. So it is not surprising that they have also become important as a method for formalizing constraints in Genetic Programming (GP). Practical grammar-based GP systems first appeared in the mid 1990s, and have subsequently become an important strand in GP research and applications. We trace their subsequent rise, surveying the various grammar-based formalisms that have been used in GP and discussing the contributions they have made to the progress of GP. We illustrate these contributions with a range of applications of grammar-based GP, showing how grammar formalisms contributed to the solutions of these problems. We briefly discuss the likely future development of grammar-based GP systems, and conclude with a brief summary of the field.},
    journal = {Genetic Programming and Evolvable Machines},
    month = {sep},
    pages = {365–396},
    numpages = {32},
    keywords = {Regular, Tree adjoining, Evolutionary computation, Grammar, Genetic programming, Context free}
}

@article{montana_strongly_1995,
	title = {Strongly {Typed} {Genetic} {Programming}},
	volume = {3},
	url = {http://web.archive.org/web/20070814014654/http://vishnu.bbn.com/papers/stgp.pdf},
	abstract = {Genetic programming is a powerful method for automatically generating computer programs via the process of natural selection (Koza, 1992). However, in its standard form, there is no way to restrict the programs it generates to those where the functions operate on appropriate data types. In the case when the programs manipulate multiple data types and contain functions designed to operate on particular data types, this can lead to unnecessarily large search times and/or unnecessarily poor generalization performance. Strongly typed genetic programming (STGP) is an enhanced version of genetic programming which enforces data type constraints and whose use of generic functions and generic data types makes it more powerful than other approaches to type constraint enforcement. After describing its operation, we illustrate its use on problems in two domains, matrix/vector manipulation and list manipulation, which require its generality. The examples are: (1) the multi-dimensional least-squares regression problem, (2) the multi-dimensional Kalman ﬁlter, (3) the list manipulation function NTH, and (4) the list manipulation function MAPCAR.},
	number = {2},
	journal = {Evolutionary Computation},
	author = {Montana, David J.},
	year = {1995},
	keywords = {self\_organizing, evolution, algorithm, emergence, optimization, gp},
	pages = {199--230},
	annote = {See also July 1993 BBN Technical Report \#7866 of the same name – http://www.citeulike.org/user/numata/article/219660},
}

@INPROCEEDINGS{owens_camouflaging_2014,
    author={Owens, Andrew and Barnes, Connelly and Flint, Alex and Singh, Hanumant and Freeman, William},
    booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
    title={Camouflaging an Object from Many Viewpoints}, 
    year={2014},
    volume={},
    number={},
    address={Columbus, OH, USA},
    publisher = {IEEE CVPR},
    pages={2782-2789},
    doi={10.1109/CVPR.2014.350}
}



@inproceedings{pang_zoom_2022,
	address = {New Orleans, LA, USA},
	title = {Zoom {In} and {Out}: {A} {Mixed}-scale {Triplet} {Network} for {Camouflaged} {Object} {Detection}},
	isbn = {978-1-66546-946-3},
	shorttitle = {Zoom {In} and {Out}},
	url = {https://ieeexplore.ieee.org/document/9878581/},
	doi = {10.1109/CVPR52688.2022.00220},
	abstract = {The recently proposed camouflaged object detection (COD) attempts to segment objects that are visually blended into their surroundings, which is extremely complex and difficult in real-world scenarios. Apart from high intrinsic similarity between the camouflaged objects and their background, the objects are usually diverse in scale, fuzzy in appearance, and even severely occluded. To deal with these problems, we propose a mixed-scale triplet network, ZoomNet, which mimics the behavior of humans when observing vague images, i.e., zooming in and out. Specifically, our ZoomNet employs the zoom strategy to learn the discriminative mixed-scale semantics by the designed scale integration unit and hierarchical mixed-scale unit, which fully explores imperceptible clues between the candidate objects and background surroundings. Moreover, considering the uncertainty and ambiguity derived from indistinguishable textures, we construct a simple yet effective regularization constraint, uncertainty-aware loss, to promote the model to accurately produce predictions with higher confidence in candidate regions. Without bells and whistles, our proposed highly task-friendly model consistently surpasses the existing 23 state-of-the-art methods on four public datasets. Besides, the superior performance over the recent cuttingedge models on the SOD task also verifies the effectiveness and generality of our model. The code will be available at https://github.com/lartpang/ZoomNet.},
	language = {en},
	urldate = {2022-10-28},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pang, Youwei and Zhao, Xiaoqi and Xiang, Tian-Zhu and Zhang, Lihe and Lu, Huchuan},
	month = jun,
	year = {2022},
	pages = {2150--2160},
	file = {Pang et al. - 2022 - Zoom In and Out A Mixed-scale Triplet Network for.pdf:/Users/cwr/Zotero/storage/99IHVM5Z/Pang et al. - 2022 - Zoom In and Out A Mixed-scale Triplet Network for.pdf:application/pdf},
}




@article{perlin_image_1985,
	title = {An image synthesizer},
	volume = {19},
	issn = {0097-8930},
	url = {http://dx.doi.org/10.1145/325334.325247},
	doi = {10.1145/325334.325247},
	abstract = {We introduce the concept of a Pixel Stream Editor. This forms the basis for an interactive synthesizer for designing highly realistic Computer Generated Imagery. The designer works in an interactive Very High Level programming environment which provides a very fast concept/implement/view iteration cycle. Naturalistic visual complexity is built up by composition of non-linear functions, as opposed to the more conventional texture mapping or growth model algorithms. Powerful primitives are included for creating controlled stochastic effects. We introduce the concept of "solid texture" to the field of CGI.We have used this system to create very convincing representations of clouds, fire, water, stars, marble, wood, rock, soap films and crystal. The algorithms created with this paradigm are generally extremely fast, highly realistic, and asynchronously parallelizable at the pixel level.},
	number = {3},
	journal = {SIGGRAPH '85: Proceedings of the 12th annual conference on Computer graphics and interactive techniques},
	author = {Perlin, Ken},
	month = jul,
	year = {1985},
	keywords = {noise, pattern, texture\_synthesis},
	pages = {287--296},
}

@article{Reynolds2011,
    author = {Reynolds, Craig},
    title = "{Interactive Evolution of Camouflage}",
    journal = {Artificial Life},
    volume = {17},
    number = {2},
    pages = {123-136},
    year = {2011},
    month = {04},
    abstract = "{This article presents an abstract computation model of the evolution of camouflage in nature. The 2D model uses evolved textures for prey, a background texture representing the environment, and a visual predator. A human observer, acting as the predator, is shown a cohort of 10 evolved textures overlaid on the background texture. The observer clicks on the five most conspicuous prey to remove (“eat”) them. These lower-fitness textures are removed from the population and replaced with newly bred textures. Biological morphogenesis is represented in this model by procedural texture synthesis. Nested expressions of generators and operators form a texture description language. Natural evolution is represented by genetic programming (GP), a variant of the genetic algorithm. GP searches the space of texture description programs for those that appear least conspicuous to the predator.}",
    issn = {1064-5462},
    doi = {10.1162/artl_a_00023},
    url = {https://doi.org/10.1162/artl\_a\_00023},
    eprint = {},
}

@inproceedings{sims_artificial_1991,
	address = {New York, NY, USA},
	title = {Artificial evolution for computer graphics},
	volume = {25},
	isbn = {0-89791-436-8},
	url = {http://www.genarts.com/karl/papers/siggraph91.html},
	doi = {10.1145/122718.122752},
	abstract = {This paper describes how evolutionary techniques of variation and selection can be used to create complex simulated structures, textures, and motions for use in computer graphics and animation. Interactive selection, based on visual perception of procedurally generated results, allows the user to direct simulated evolutions in preferred directions. Several examples using these methods have been implemented and are described. 3D plant structures are grown using fixed sets of genetic parameters. Images, solid textures, and animations are created using mutating symbolic lisp expressions. Genotypes consisting of symbolic expressions are presented as an attempt to surpass the limitations of fixed-length genotypes with predefined expression rules. It is proposed that artificial evolution has potential as a powerful tool for achieving flexible complexity with a minimum of user input and knowledge of details.},
	booktitle = {{SIGGRAPH} '91: {Proceedings} of the 18th annual conference on {Computer} graphics and interactive techniques},
	publisher = {ACM},
	author = {Sims, Karl},
	month = jul,
	year = {1991},
	keywords = {evolution, color, texture\_synthesis, gp, humanfitnessfunction},
	pages = {319--328},
}

@techreport{sun_boundary-guided_2022,
	title = {Boundary-{Guided} {Camouflaged} {Object} {Detection}},
	url = {http://arxiv.org/abs/2207.00794},
	abstract = {Camouflaged object detection (COD), segmenting objects that are elegantly blended into their surroundings, is a valuable yet challenging task. Existing deep-learning methods often fall into the difficulty of accurately identifying the camouflaged object with complete and fine object structure. To this end, in this paper, we propose a novel boundary-guided network (BGNet) for camouflaged object detection. Our method explores valuable and extra object-related edge semantics to guide representation learning of COD, which forces the model to generate features that highlight object structure, thereby promoting camouflaged object detection of accurate boundary localization. Extensive experiments on three challenging benchmark datasets demonstrate that our BGNet significantly outperforms the existing 18 state-of-the-art methods under four widely-used evaluation metrics. Our code is publicly available at: https://github.com/thograce/BGNet.},
	number = {arXiv:2207.00794},
	urldate = {2022-07-29},
	institution = {arXiv},
	author = {Sun, Yujia and Wang, Shuo and Chen, Chenglizhao and Xiang, Tian-Zhu},
	month = jul,
	year = {2022},
	doi = {10.48550/arXiv.2207.00794},
	note = {arXiv:2207.00794 [cs] type: article},
	keywords = {camouflage, cocish, predator, prey, vision, detection, Computer Science - Computer Vision and Pattern Recognition, Object detection, breaking},
	annote = {Comment: Accepted by IJCAI2022},
	file = {arXiv Fulltext PDF:/Users/cwr/Zotero/storage/Q98WB5DY/Sun et al. - 2022 - Boundary-Guided Camouflaged Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/cwr/Zotero/storage/VTK9VHKN/2207.html:text/html},
}

@misc{visionxiang_cod,
    author = {visionxiang},
    title = {Camouflaged/Concealed Object Detection},
    url = {https://github.com/visionxiang/awesome-camouflaged-object-detection},
    year = {2022},
    urldate = {27.10.2022},
    originalyear = {4.10.2016}
}

@article{Zhang_Yin_Nie_Zheng_2020,
    title={Deep Camouflage Images},
    author={Zhang, Qing and Yin, Gelin and Nie, Yongwei and Zheng, Wei-Shi},
    volume={34},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/6981},
    DOI={10.1609/aaai.v34i07.6981},
    number={07},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    year={2020},
    month={Apr.},
    pages={12845-12852},
    abstractNote={&lt;p&gt;This paper addresses the problem of creating &lt;em&gt;camouflage images&lt;/em&gt;. Such images typically contain one or more hidden objects embedded into a background image, so that viewers are required to consciously focus to discover them. Previous methods basically rely on hand-crafted features and texture synthesis to create camouflage images. However, due to lack of reliable understanding of what essentially makes an object recognizable, they typically result in either complete standout or complete invisible hidden objects. Moreover, they may fail to produce seamless and natural images because of the sensitivity to appearance differences. To overcome these limitations, we present a novel neural style transfer approach that adopts the visual perception mechanism to create camouflage images, which allows us to hide objects more effectively while producing natural-looking results. In particular, we design an attention-aware camouflage loss to adaptively mask out information that make the hidden objects visually standout, and also leave subtle yet enough feature clues for viewers to perceive the hidden objects. To remove the appearance discontinuities between the hidden objects and the background, we formulate a naturalness regularization to constrain the hidden objects to maintain the manifold structure of the covered background. Extensive experiments show the advantages of our approach over existing camouflage methods and state-of-the-art neural style transfer algorithms.&lt;/p&gt;}
}

@inproceedings{Zhang2022,
    author = {Zhang, Miao and Xu, Shuang and Piao, Yongri and Shi, Dongxiang and Lin, Shusen and Lu, Huchuan},
    title = {PreyNet: Preying on Camouflaged Objects},
    year = {2022},
    isbn = {9781450392037},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA}, url = {https://doi.org/10.1145/3503161.3548178},
    doi = {10.1145/3503161.3548178},
    abstract = {Species often adopt various camouflage strategies to be seamlessly blended into the surroundings for self-protection. To figure out the concealment, predators have evolved excellent hunting skills. Exploring the intrinsic mechanisms of the predation behavior can offer more insightful glimpse into the task of camouflaged object detection (COD). In this work, we strive to seek answers for accurate COD and propose a PreyNet, which mimics the two processes of predation, namely, initial detection (sensory mechanism) and predator learning (cognitive mechanism). To exploit the sensory process, a bidirectional bridging interaction module (BBIM) is designed for selecting and aggregating initial features in an attentive manner. The predator learning process is formulated as a policy-and-calibration paradigm, with the goal of deciding on uncertain regions and encouraging targeted feature calibration. Besides, we obtain adaptive weight for multi-layer supervision during training via computing on the uncertainty estimation. Extensive experiments demonstrate that our model produces state-of-the-art results on several benchmarks. We further verify the scalability of the predator learning paradigm through applications on top-ranking salient object detection models. Our code is publicly available at urlhttps://github.com/OIPLab-DUT/PreyNet.},
    booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
    pages = {5323–5332},
    numpages = {10},
    keywords = {camouflaged object detection, sensory mechanism, predator learning},
    location = {Lisboa, Portugal},
    series = {MM '22}
}